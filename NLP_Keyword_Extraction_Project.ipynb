{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "3W3KsGmynBvj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CQpaL6LyAdFH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "collapsed": true,
        "id": "mXIHF8DGo__4"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/sample_data/papers.csv',on_bad_lines='warn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "collapsed": true,
        "id": "nOGiQ90uqtq4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "outputId": "ac31e282-43cb-40de-9840-7179880a9f7b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id  year                                              title  \\\n",
              "0        1  1987  Self-Organization of Associative Database and ...   \n",
              "1       10  1987  A Mean Field Theory of Layer IV of Visual Cort...   \n",
              "2      100  1988  Storing Covariance by the Associative Long-Ter...   \n",
              "3     1000  1994  Bayesian Query Construction for Neural Network...   \n",
              "4     1001  1994  Neural Network Ensembles, Cross Validation, an...   \n",
              "...    ...   ...                                                ...   \n",
              "7236   994  1994                Single Transistor Learning Synapses   \n",
              "7237   996  1994  Bias, Variance and the Combination of Least Sq...   \n",
              "7238   997  1994          A Real Time Clustering CMOS Neural Engine   \n",
              "7239   998  1994  Learning direction in global motion: two class...   \n",
              "7240   999  1994  Correlation and Interpolation Networks for Rea...   \n",
              "\n",
              "     event_type                                           pdf_name  \\\n",
              "0           NaN  1-self-organization-of-associative-database-an...   \n",
              "1           NaN  10-a-mean-field-theory-of-layer-iv-of-visual-c...   \n",
              "2           NaN  100-storing-covariance-by-the-associative-long...   \n",
              "3           NaN  1000-bayesian-query-construction-for-neural-ne...   \n",
              "4           NaN  1001-neural-network-ensembles-cross-validation...   \n",
              "...         ...                                                ...   \n",
              "7236        NaN        994-single-transistor-learning-synapses.pdf   \n",
              "7237        NaN  996-bias-variance-and-the-combination-of-least...   \n",
              "7238        NaN  997-a-real-time-clustering-cmos-neural-engine.pdf   \n",
              "7239        NaN  998-learning-direction-in-global-motion-two-cl...   \n",
              "7240        NaN  999-correlation-and-interpolation-networks-for...   \n",
              "\n",
              "              abstract                                         paper_text  \n",
              "0     Abstract Missing  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
              "1     Abstract Missing  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
              "2     Abstract Missing  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
              "3     Abstract Missing  Bayesian Query Construction for Neural\\nNetwor...  \n",
              "4     Abstract Missing  Neural Network Ensembles, Cross\\nValidation, a...  \n",
              "...                ...                                                ...  \n",
              "7236  Abstract Missing  Single Transistor Learning Synapses\\n\\nPaul Ha...  \n",
              "7237  Abstract Missing  Bias, Variance and the Combination of\\nLeast S...  \n",
              "7238  Abstract Missing  A Real Time Clustering CMOS\\nNeural Engine\\nT....  \n",
              "7239  Abstract Missing  Learning direction in global motion: two\\nclas...  \n",
              "7240  Abstract Missing  Correlation and Interpolation Networks for\\nRe...  \n",
              "\n",
              "[7241 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aa005f07-7ae4-488e-8471-be0e9f6b822b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1987</td>\n",
              "      <td>Self-Organization of Associative Database and ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1-self-organization-of-associative-database-an...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>1987</td>\n",
              "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>1988</td>\n",
              "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000</td>\n",
              "      <td>1994</td>\n",
              "      <td>Bayesian Query Construction for Neural Network...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1001</td>\n",
              "      <td>1994</td>\n",
              "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7236</th>\n",
              "      <td>994</td>\n",
              "      <td>1994</td>\n",
              "      <td>Single Transistor Learning Synapses</td>\n",
              "      <td>NaN</td>\n",
              "      <td>994-single-transistor-learning-synapses.pdf</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Single Transistor Learning Synapses\\n\\nPaul Ha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7237</th>\n",
              "      <td>996</td>\n",
              "      <td>1994</td>\n",
              "      <td>Bias, Variance and the Combination of Least Sq...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>996-bias-variance-and-the-combination-of-least...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Bias, Variance and the Combination of\\nLeast S...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7238</th>\n",
              "      <td>997</td>\n",
              "      <td>1994</td>\n",
              "      <td>A Real Time Clustering CMOS Neural Engine</td>\n",
              "      <td>NaN</td>\n",
              "      <td>997-a-real-time-clustering-cmos-neural-engine.pdf</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>A Real Time Clustering CMOS\\nNeural Engine\\nT....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7239</th>\n",
              "      <td>998</td>\n",
              "      <td>1994</td>\n",
              "      <td>Learning direction in global motion: two class...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>998-learning-direction-in-global-motion-two-cl...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Learning direction in global motion: two\\nclas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7240</th>\n",
              "      <td>999</td>\n",
              "      <td>1994</td>\n",
              "      <td>Correlation and Interpolation Networks for Rea...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>999-correlation-and-interpolation-networks-for...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Correlation and Interpolation Networks for\\nRe...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7241 rows × 7 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aa005f07-7ae4-488e-8471-be0e9f6b822b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-aa005f07-7ae4-488e-8471-be0e9f6b822b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-aa005f07-7ae4-488e-8471-be0e9f6b822b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fc334e40-e024-40e6-8189-4905f2fbb275\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fc334e40-e024-40e6-8189-4905f2fbb275')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fc334e40-e024-40e6-8189-4905f2fbb275 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_0abbcd0e-746e-4e09-ad75-7beda3a3d908\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_0abbcd0e-746e-4e09-ad75-7beda3a3d908 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 7241,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2098,\n        \"min\": 1,\n        \"max\": 7284,\n        \"num_unique_values\": 7241,\n        \"samples\": [\n          1466,\n          3336,\n          6755\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 1987,\n        \"max\": 2017,\n        \"num_unique_values\": 31,\n        \"samples\": [\n          1992,\n          1990,\n          2012\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7241,\n        \"samples\": [\n          \"Independent Component Analysis for Identification of Artifacts in Magnetoencephalographic Recordings\",\n          \"Near-Maximum Entropy Models for Binary Neural Representations of Natural Images\",\n          \"Nearest-Neighbor Sample Compression: Efficiency, Consistency, Infinite Dimensions\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"event_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Oral\",\n          \"Spotlight\",\n          \"Poster\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pdf_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7241,\n        \"samples\": [\n          \"1466-independent-component-analysis-for-identification-of-artifacts-in-magnetoencephalographic-recordings.pdf\",\n          \"3336-near-maximum-entropy-models-for-binary-neural-representations-of-natural-images.pdf\",\n          \"6755-nearest-neighbor-sample-compression-efficiency-consistency-infinite-dimensions.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3923,\n        \"samples\": [\n          \"Recommendation for e-commerce with a mix of durable and nondurable goods has characteristics that distinguish it from the well-studied media recommendation problem. The demand for items is a combined effect of form utility and time utility, i.e., a product must both be intrinsically appealing to a consumer and the time must be right for purchase. In particular for durable goods, time utility is a function of inter-purchase duration within product category because consumers are unlikely to purchase two items in the same category in close temporal succession. Moreover, purchase data, in contrast to ratings data, is implicit with non-purchases not necessarily indicating dislike. Together, these issues give rise to the positive-unlabeled demand-aware recommendation problem that we pose via joint low-rank tensor completion and product category inter-purchase duration vector estimation. We further relax this problem and propose a highly scalable alternating minimization approach with which we can solve problems with millions of users and millions of items in a single thread. We also show superior prediction accuracies on multiple real-world data sets.\",\n          \"Stochastic Neighbor Embedding (SNE) has shown to be quite promising for data visualization.  Currently, the most popular implementation, t-SNE, is restricted to a particular Student t-distribution as its embedding distribution. Moreover, it uses a gradient descent algorithm that may require users to tune parameters such as the learning step size, momentum, etc., in finding its optimum. In this paper, we propose the Heavy-tailed Symmetric Stochastic Neighbor Embedding (HSSNE) method, which is a generalization of the t-SNE to accommodate various heavy-tailed embedding similarity functions. With this generalization, we are presented with two difficulties.  The first is how to select the best embedding similarity among all heavy-tailed functions and the second is how to optimize the objective function once the heave-tailed function has been selected. Our contributions then are: (1) we point out that various heavy-tailed embedding similarities can be characterized by their negative score functions. Based on this finding, we present a parameterized subset of similarity functions for choosing the best tail-heaviness for HSSNE; (2) we present a fixed-point optimization algorithm that can be applied to all heavy-tailed functions and does not require the user to set any parameters; and (3) we present two empirical studies, one for unsupervised visualization showing that our optimization algorithm runs as fast and as good as the best known t-SNE implementation and the other for semi-supervised visualization showing quantitative superiority using the homogeneity measure as well as qualitative advantage in cluster separation over t-SNE.\",\n          \"Adaptive schemes, where tasks are assigned based on the data collected thus far, are widely used in practical crowdsourcing systems to efficiently allocate the budget. However, existing theoretical analyses of crowdsourcing systems suggest that the gain of adaptive task assignments is minimal. To bridge this gap, we investigate this question under a strictly more general probabilistic model, which has been recently introduced to model practical crowdsourcing data sets. Under this generalized Dawid-Skene model, we characterize the fundamental trade-off between budget and accuracy, and introduce a novel adaptive scheme that matches this fundamental limit. We further quantify the gain of adaptivity, by comparing the trade-off with the one for non-adaptive schemes, and confirm that the gain is significant and can be made arbitrarily large depending on the distribution of the difficulty level of the tasks at hand.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7237,\n        \"samples\": [\n          \"Statistical Performance of Convex Tensor\\nDecomposition\\nRyota Tomioka?\\nTaiji Suzuki?\\nDepartment of Mathematical Informatics,\\nThe University of Tokyo\\nTokyo 113-8656, Japan\\ntomioka@mist.i.u-tokyo.ac.jp\\ns-taiji@stat.t.u-tokyo.ac.jp\\n\\nKohei Hayashi?\\nGraduate School of Information Science,\\nNara Institute of Science and Technology\\nNara 630-0192, Japan\\nkohei-h@is.naist.jp\\n\\n?\\n\\n?\\n\\nHisashi Kashima?,?\\nBasic Research Programs PRESTO,\\nSynthesis of Knowledge for Information Oriented Society, JST\\nTokyo 102-8666, Japan\\nkashima@mist.i.u-tokyo.ac.jp\\n?\\n\\nAbstract\\nWe analyze the statistical performance of a recently proposed convex tensor decomposition algorithm. Conventionally tensor decomposition has been formulated as non-convex optimization problems, which hindered the analysis of their\\nperformance. We show under some conditions that the mean squared error of\\nthe convex method scales linearly with the quantity we call the normalized rank\\nof the true tensor. The current analysis naturally extends the analysis of convex\\nlow-rank matrix estimation to tensors. Furthermore, we show through numerical\\nexperiments that our theory can precisely predict the scaling behaviour in practice.\\n\\n1 Introduction\\nTensors (multi-way arrays) generalize matrices and naturally represent data having more than two\\nmodalities. For example, multi-variate time-series, for instance, electroencephalography (EEG),\\nrecorded from multiple subjects under various conditions naturally form a tensor. Moreover, in\\ncollaborative ?ltering, users? preferences on products, conventionally represented as a matrix, can\\nbe represented as a tensor when the preferences change over time or context.\\nFor the analysis of tensor data, various models and methods for the low-rank decomposition of\\ntensors have been proposed (see Kolda & Bader [12] for a recent survey). These techniques have\\nrecently become increasingly popular in data-mining [1, 14] and computer vision [25, 26]. Besides\\nthey have proven useful in chemometrics [4], psychometrics [24], and signal processing [20, 7, 8].\\nDespite empirical success, the statistical performance of tensor decomposition algorithms has not\\nbeen fully elucidated. The dif?culty lies in the non-convexity of the conventional tensor decomposition algorithms (e.g., alternating least squares [6]). In addition, studies have revealed many\\ndiscrepancies (see [12]) between matrix rank and tensor rank, which make extension of studies on\\nthe performance of low-rank matrix models (e.g., [9]) challenging.\\nRecently, several authors [21, 10, 13, 23] have focused on the notion of tensor mode-k rank (instead\\nof tensor rank), which is related to the Tucker decomposition [24]. They discovered that regularized\\nestimation based on the Schatten 1-norm, which is a popular technique for recovering low-rank\\nmatrices via convex optimization, can also be applied to tensor decomposition. In particular, the\\n1\\n\\n\\fConvex\\nTucker (exact)\\nOptimization tolerance\\n\\n0\\n\\n10\\n\\n?3\\n\\n10\\n\\n0\\n\\n0.2\\n0.4\\n0.6\\n0.8\\nFraction of observed elements\\n\\n1\\n\\nFigure 1: Result of estimation of rank-(7, 8, 9) tensor of dimensions\\n50???? 50 ? 20 from partial\\n???\\n? ? W ? ??? is plotted against the\\nmeasurements; see [23] for the details. The estimation error ???W\\nF\\nfraction of observed elements m = M/N . Error bars over 10 repetitions are also shown. Convex\\nrefers to the convex tensor decomposition based on the minimization problem (7). Tucker (exact)\\nrefers to the conventional (non-convex) Tucker decomposition [24] at the correct rank. Gray dashed\\nline shows the optimization tolerance 10?3 . The question is how we can predict the point where the\\ngeneralization begins (roughly m = 0.35 in this plot).\\n\\nstudy in [23] showed that there is a clear transition at certain number of samples where the error\\ndrops dramatically from no generalization to perfect generalization (see Figure 1).\\nIn this paper, motivated by the above recent work, we mathematically analyze the performance of\\nconvex tensor decomposition. The new convex formulation for tensor decomposition allows us to\\ngeneralize recent results on Schatten 1-norm-regularized estimation of matrices (see [17, 18, 5, 19]).\\nUnder a general setting we show how the estimation error scales with the mode-k ranks of the true\\ntensor. Furthermore, we analyze the speci?c settings of (i) noisy tensor decomposition and (ii)\\nrandom Gaussian design. In the ?rst setting, we assume that all the elements of a low-rank tensor\\nis observed with noise and the goal is to recover the underlying low-rank structure. This is the most\\ncommon setting a tensor decomposition algorithm is used. In the second setting, we assume that\\nthe unknown tensor is a coef?cient of a tensor-input scalar-output regression problem and the input\\ntensors (design) are randomly given from independent Gaussian distributions. Surprisingly, it turns\\nout that the random Gaussian setting can precisely predict the phase-transition-like behaviour in\\nFigure 1. To the best of our knowledge, this is the ?rst paper that rigorously studies the performance\\nof a tensor decomposition algorithm.\\n\\n2\\n\\nNotation\\n\\nIn this section, we introduce the notations we use in this paper. Moreover, we introduce a H?olderlike inequality (3) and the notion of mode-k decomposability (5), which play central roles in our\\nanalysis.\\nQK\\nLet X ? Rn1 ????nK be a K-way tensor. We denote the number of elements in X by N = k=1 nk .\\n?\\nThe inner product between two tensors ?W, X ? is de?ned as ?W, X ? = vec(W)\\np ), where\\n??? ??? vec(X\\nvec is a vectorization. In addition, we de?ne the Frobenius norm of a tensor ???X ???F = ?X , X ?.\\nQ\\nThe mode-k unfolding X (k) is the nk ? n\\n? \\\\k (?\\nn\\\\k := k? ?=k nk? ) matrix obtained by concatenating\\nthe mode-k ?bers (the vectors obtained by ?xing every index of X but the kth index) of X as column\\nvectors. The mode-k rank of a tensor X , denoted by rankk (X ), is the rank of the mode-k unfolding\\nX (k) (as a matrix). Note that when K = 2 and X is actually a matrix, and X (2) = X (1) ? . We say\\na tensor X is rank (r1 , . . . , rK ) when rk = rankk (X ) for k = 1, . . . , K. Note that the mode-k rank\\ncan be computed in a polynomial time, because it boils down to computing a matrix rank, whereas\\ncomputing tensor rank is NP complete [11]. See [12] for more details.\\nSince for each k, the convex envelope of the mode-k rank is given as the Schatten 1-norm [18]\\n(known as the trace norm [22] or the nuclear norm [3]), it is natural to consider the following\\n2\\n\\n\\f??? ???\\noverlapped Schatten 1-norm ???W ???S of a tensor W ? Rn1 ?????nK (see also [21]):\\n1\\n\\n??? ???\\n???W ???\\n\\nS1\\n\\n=\\n\\nK\\n?\\n1 X?\\n?W (k) ? ,\\nS1\\nK\\n\\n(1)\\n\\nk=1\\n\\nwhere W (k) is the mode-k unfolding of W. Here ? ? ?S1 is the Schatten 1-norm for a matrix\\nXr\\n?W ?S1 =\\n?j (W ),\\nj=1\\n\\nwhere ?j (W ) is the jth largest singular-value of W . The dual norm of the Schatten 1-norm is the\\nSchatten ?-norm (known as the spectral norm) as follows:\\n?X?S? = max ?j (X).\\nj=1,...,r\\n\\nSince the two norms ? ? ?S1 and ? ? ?S? are dual to each other, we have the following inequality:\\n|?W , X?| ? ?W ?S1 ?X?S? ,\\n(2)\\nwhere ?W , X? is the inner product of W and X.\\nThe same inequality holds for the overlapped Schatten 1-norm (1) and its dual norm. The dual norm\\nof the overlapped Schatten 1-norm can be characterized by the following lemma.\\n??? ???\\nLemma 1. The dual norm of the overlapped Schatten 1-norm denoted as ???????S ? is de?ned as the\\n1\\nin?mum of the maximum mode-k spectral norm over the tensors whose average equals the given\\ntensor X as follows:\\n??? ???\\n(k)\\n???X ??? ? =\\nmax ?Y (k) ?S? ,\\ninf\\nS1\\n1\\n(1) +Y (2) +???+Y (K) =X\\nk=1,...,K\\nY\\n(\\n)\\nK\\n(k)\\n\\nwhere Y (k) is the mode-k unfolding of Y (k) . Moreover, the following upper bound on the dual norm\\n??? ???\\n??????? ? is valid:\\nS1\\n\\n??? ???\\n???X ???\\n\\nS1?\\n\\n??? ???\\n1 XK\\n?X (k) ?S? .\\n? ???X ???mean :=\\nk=1\\nK\\n\\n??? ???\\nProof. The ?rst part can be shown by solving the dual of the maximization problem ???X ???S ? :=\\n1\\n??? ???\\nsup ?W, X ? s.t. ???W ???S1 ? 1. The second part is obtained by setting Y (k) = PK K1/c ? X /ck ,\\nwhere ck = ?X (k) ?S? , and using Jensen?s inequality.\\n\\nk? =1\\n\\nk\\n\\nAccording to Lemma 1, we have the ??following\\n? ??? ??? H?\\n?o??lder-like\\n??? inequality\\n??? ??? ???\\n|?W, X ?| ? ???W ???S1 ???X ???S ? ? ???W ???S1 ???X ???mean .\\n\\n(3)\\n??? ??? ??? ???\\nNote that the above bound is tighter than the more intuitive relation | ?W, X ? | ? ???W ???S ???X ???S\\n1\\n?\\n??? ???\\n(???X ???S? := max1,...,K ?X (k) ?S? ), which one might come up as an analogy to the matrix case (2).\\n1\\n\\nFinally, let W ? ? Rn1 ?????nK be the low-rank tensor that we wish to recover. We assume that W ?\\nis rank (r1 , . . . , rK ). Thus, for each k we have\\nW ?(k) = U k S k V k\\n(k = 1, . . . , K),\\nwhere U k ? Rnk ?rk and V k ? Rn? \\\\k ?rk are orthogonal, and S k ? Rrk ?rk is diagonal. Let\\n? ? Rn1 ?????nK be an arbitrary tensor. We de?ne the mode-k orthogonal complement ???k of an\\nunfolding ?(k) ? Rnk ??n\\\\k of ? with respect to the true low-rank tensor W ? as follows:\\n??k\\n\\n???k = (I nk ? U k U k ? )?(k) (I n? \\\\k ? V k V k ? ).\\n\\n(4)\\n\\n:= ?(k) ? ???k is\\nthe true tensor W ?(k) .\\n\\nIn addition\\nthe component having overlapped row/column space with the\\nunfolding of\\nNote that the decomposition ?(k) = ??k + ???k is de?ned for\\neach mode; thus we use subscript k instead of (k).\\nUsing the decomposition de?ned above we have the following equality, which we call mode-k decomposability of the Schatten 1-norm:\\n?W ?(k) + ???k ?S1 = ?W ?(k) ?S1 + ????k ?S1 (k = 1, . . . , K).\\n(5)\\nThe above decomposition is de?ned for each mode and thus it is weaker than the notion of decomposability discussed by Negahban et al. [15].\\n3\\n\\n\\f3\\n\\nTheory\\n\\nIn this section, we ?rst present a deterministic result that holds under a certain choice of regularization constant ?M and an assumption called the restricted strong convexity. Then, we focus on\\nspecial cases to justify the choice of regularization constant and the restricted strong convexity assumption. We analyze the setting of (i) noisy tensor decomposition and (ii) random Gaussian design\\nin Section 3.2 and Section 3.3, respectively.\\n3.1\\n\\nMain result\\n\\nOur goal is to estimate an unknown rank (r1 , . . . , rK ) tensor W ? ? Rn1 ????nK from observations\\nyi = ?Xi , W ? ? + ?i (i = 1, . . . , M ).\\n(6)\\nHere the noise ?i follows the independent zero-mean Gaussian distribution with variance ? 2 .\\nWe employ the regularized empirical risk minimization problem proposed in [21, 10, 13, 23] for the\\nestimation of W as follows:\\n??? ???\\n1\\nminimize\\n?y ? X(W)?22 + ?M ???W ???S1 ,\\n(7)\\nn\\n?????n\\n1\\nK\\n2M\\nW?R\\nwhere y = (y1 , . . . , yM )? is the collection of observations; X : Rn1 ?????nK ? RM is a linear\\noperator that maps W to the M dimensional output vector X(W) = (?X1 , W? , . . . , ?XM , W?) ? ?\\nRM . The Schatten 1-norm term penalizes every mode of W to be jointly low-rank (see Equation (1));\\n?M > 0 is the regularization constant. Accordingly, the solution of the minimization problem (7) is\\ntypically a low-rank tensor when ?M is suf?ciently large. In addition, we denote the adjoint operator\\nPM\\nof X as X? : RM ? Rn1 ?????nK ; that is X? (?) = i=1 ?i Xi ? Rn1 ?????nK .\\n? ? W?\\nThe ?rst step in our analysis is to characterize the particularity of the residual tensor ? := W\\nas in the following lemma.\\n???\\n???\\n? be the solution of the minimization problem (7) with ?M ? 2???X? (?)???\\nLemma 2. Let W\\n/M ,\\nmean\\n? ? W ? , where W ? is the true low-rank tensor. Let ?(k) = ?? + ??? be the\\nand let ? := W\\nk\\nk\\ndecomposition de?ned in Equation (4). Then we have the following inequalities:\\n1. rank(??k ) ? 2rk for each k = 1, . . . , K.\\nPK\\nPK\\n?\\n??\\n2.\\nk=1 ??k ?S1 .\\nk=1 ??k ?S1 ? 3\\nProof. The proof uses the mode-k decomposability (5) and is analogous to that of Lemma 1 in\\n[17].\\nThe second ingredient of our analysis is the restricted strong convexity. Although, ?strong? may\\nsound like a strong assumption, the point is that we require this assumption to hold only for the\\nparticular residual tensor we characterized in Lemma 2. The assumption can be stated as follows.\\nAssumption 1 (Restricted strong convexity). We suppose that there is a positive constant ?(X) such\\nthat the operator X satis?es the inequality\\n??? ???2\\n1\\n?X(?)?22 ??(X)???????F ,\\n(8)\\nM\\nPK\\nfor all ? ? Rn1 ?????nK such that for each k = 1, . . . , K, rank(??k ) ? 2rk and k=1 ????k ?S1 ?\\nPK\\n3 k=1 ???k ?S1 , where ??k and ???k are de?ned through the decomposition (4).\\nNow using the above two ingredients, we are ready to prove the following deterministic guarantee\\non the performance of the estimation procedure (7).\\n???\\n???\\n? be the solution of the minimization problem (7) with ?M ? 2???X? (?)???\\nTheorem 1. Let W\\n/M .\\nmean\\nSuppose that the operator X satis?es the restricted strong convexity condition. Then the following\\nbound is true:\\nPK ?\\n???\\n???\\n? ? W ? ??? ? 32?M k=1 rk .\\n???W\\n(9)\\nF\\n?(X)K\\n4\\n\\n\\f? ? W ? . Combining the fact that the objective value for W\\n?\\nProof. Let ? = W\\n??? ? ???\\n??? is??smaller\\n?\\n??than\\n? ??? that for\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\nW , the H?older-like inequality (3), the triangular inequality W S ? W S ? ?????S , and\\n1\\n1\\n1\\n???\\n???\\nthe assumption ???X? (?)/M ???\\n? ?M /2, we obtain\\nmean\\n\\n??? ???\\n???\\n???\\n??? ???\\n??? ???\\n1\\n(10)\\n?X(?)?22 ? ???X? (?)/M ???mean ???????S1 + ?M ???????S1 ? 2?M ???????S1 .\\n2M\\nNow the left-hand side can be lower-bounded using the restricted strong convexity (8). On the other\\nhand, using Lemma 2, the right-hand side can be upper-bounded as follows:\\n??? ???\\n??? ???\\n??? ???\\n?\\n??????? ? 1 PK (???k ?S1 + ????k ?S1 ) ? 4 PK ???k ?S1 ? 4 ? F PK\\n2rk , (11)\\nk=1\\nk=1\\nk=1\\nK\\nK\\nK\\nS1\\n??? ???\\nwhere the last inequality follows because ???????F = ??(k) ?F for k = 1, . . . , K. Combining inequalities (8), (10), and (11), we obtain our claim (9).\\nNegahban et al. [15] (see also [17]) pointed out that the key properties for establishing a sharp convergence result for a regularized M -estimator is the decomposability of the regularizer and the restricted strong convexity. What we have shown suggests that the weaker mode-k decomposability (5)\\nsuf?ce to obtain the above convergence result for the overlapped Schatten 1-norm (1) regularization.\\n3.2 Noisy Tensor Decomposition\\nIn this subsection, we consider the setting where all the elements are observed (with noise) and the\\ngoal is to recover the underlying low-rank tensor without noise.\\nSince all the elements are observed only once, X is simply a vectorization\\n(M =\\n???\\n??? N ), and the left2\\n? ? W ? ??? . Therefore, the\\n?\\n?\\n?\\n=\\nW\\nhand side of inequality (10)???gives the\\nquantity\\nof\\ninterest\\n?X(?)?\\n2\\nF\\n???\\nremaining task is to bound ???X? (?)???mean as in the following lemma.\\nLemma 3. Suppose\\n???\\n?that\\n?? X : n1 ?? ? ??nK ? N is a vectorization of a tensor. With high probability\\nthe quantity ???X? (?)???mean is concentrated around its mean, which can be bounded as follows:\\nK\\n???\\n???\\n?\\np\\n? X ??\\nE???X? (?)???mean ?\\nnk + n\\n? \\\\k .\\nK\\n\\n(12)\\n\\nk=1\\n\\n???\\n???\\nSetting the regularization constant as ?M = c0 E???X? (?)???mean /N , we obtain the following theorem.\\nTheorem 2. Suppose that X : n1 ?? ? ??nK ? N is a vectorization of a tensor. There are universal\\nconstants c0 and c1 , such that, with high probability, any solution of the minimization problem (7)\\nPK ?\\np\\nwith regularization constant ?M = c0 ? k=1 ( nk + n\\n? \\\\k )/(KN ) satis?es the following bound:\\n?\\n!2 ?\\n!2\\nK\\nK\\nX\\nX\\n???\\n???2\\n??\\n?\\np\\n?\\n1\\n1\\n?\\n2\\n? ? W ??? ? c1 ?\\n???W\\nnk + n\\n? \\\\k\\nrk .\\nF\\nK\\nK\\nk=1\\n\\nk=1\\n\\nProof. Combining Equations (10)?(11) with the fact that X is simply a vectorization and M = N ,\\nwe have\\n?\\n1\\n? ? W ? ?F ? 16 2?M PK ?rk .\\n?W\\nN\\n\\nK\\n\\nk=1\\n\\nSubstituting the choice of regularization constant ?M and squaring both sides, we obtain our claim.?\\nWe can simplify the result of Theorem 2 by noting that n\\n? \\\\k = N/nk ? nk , when the dimenPK ? 2\\n1\\nsions are of the same order. Introducing the notation ?r?1/2 = ( K\\nrk ) and n?1 :=\\nk=1\\n(1/n1 , . . . , 1/nK ), we have\\n???\\n???\\n? ? W ? ???2\\n???W\\n?\\n?\\nF\\n? Op ? 2 ?n?1 ?1/2 ?r?1/2 .\\n(13)\\nN\\nWe call the quantity r? = ?n?1 ?1/2 ?r?1/2 the normalized rank, because r? = r/n when the dimensions are balanced (nk = n and rk = r for all k = 1, . . . , K).\\n5\\n\\n\\f3.3\\n\\nRandom Gaussian Design\\n\\nIn this subsection, we consider the case the elements of the input tensors Xi (i = 1, . . . , M ) in the\\nobservation model (6) are distributed according to independent identical standard Gaussian distributions. We call this setting random Gaussian design.\\n???\\n???\\nFirst we show an upper bound on the norm ???X? (?)???mean , which we use to specify the scaling of\\nthe regularization constant ?M in Theorem 1.\\nLemma 4. Let X : Rn1 ?????nK ? RM be a random Gaussian design. In addition, we assume\\nthat\\n?i is sampled independently from N (0, ? 2 ). Then with high probability the quantity\\n?\\n??? ? the ??noise\\n???X (?)???\\nis concentrated around its mean, which can be bounded as follows:\\nmean\\n???\\n???\\nE???X? (?)???\\n\\nmean\\n\\n?\\nK\\n?\\np\\n? M X ??\\n?\\nnk + n\\n? \\\\k .\\nK\\nk=1\\n\\nNext the following lemma, which is a generalization of a result presented in Negahban and Wainwright [17, Proposition 1], provides a ground for the restricted strong convexity assumption (8).\\nLemma 5. Let X : Rn1 ?????nK ? RM be a random Gaussian design. Then it satis?es\\n?r\\n!\\nr\\nK\\nn\\n? \\\\k ?????? ??????\\n1 X\\n?X(?)?2\\n1 ?????? ??????\\nnk\\n?\\n? F?\\n+\\n? S1 ,\\n?\\n4\\nK\\nM\\nM\\nM\\nk=1\\n\\nwith probability at least 1 ? 2 exp(?N/32).\\nProof. The proof is analogous to that of Proposition 1 in [17] except that we use H?older-like inequality (3) for tensors instead of inequality (2) for matrices.\\nFinally, we obtain the following convergence bound.\\nTheorem 3. Under the random Gaussian design setup, there are universal constants c0 , c1 , and c2\\nPK ?\\nPK ? 2\\np\\n1\\n1\\nsuch that for a sample size M ? c1 ( K\\nn\\n? \\\\k ))2 ( K\\nrk ) , any solution of the\\nk=1 ( nk +\\n?\\nPk=1\\np\\n?\\nK\\nminimization problem (7) with regularization constant ?M = c0 ? k=1 ( nk + n\\n? \\\\k )/(K M )\\nsatis?es the following bound:\\nPK ?\\nPK ? 2\\np\\n1\\n1\\n???\\n???\\n?2 ( K\\nn\\n? \\\\k ))2 ( K\\nk=1 ( nk +\\nk=1 rk )\\n? ? W ? ???2 ? c2\\n???W\\n,\\nF\\nM\\nwith high probability.\\nAgain we can simplify the result of Theorem 3 as follows: for sample size M ? c1 N r? we have\\n?\\n?\\n?1\\n???\\n???\\n? ? W ? ???2 ? Op ? 2 N ?n ?1/2 ?r?1/2 ,\\n???W\\n(14)\\nF\\nM\\nwhere r? = ?n?1 ?1/2 ?r?1/2 is the normalized rank. Note that the condition on the number of\\nsamples M does not depend on the noise variance ? 2 . Therefore in the limit ? 2 ? 0, the bound (14)\\nis suf?ciently small but only valid for sample size M that exceeds c1 N r?, which implies a threshold\\nbehavior as in Figure 1.\\nNote also that in the matrix case (K = 2), r1 = r2 = r and N ?n?1 ?1/2 = O(n1 + n2 ). Therefore\\n? ? W ? ?2 ?\\nwe can restate the above result as for sample size M ? c1 r(n1 + n2 ), we have ?W\\nF\\nOp (r(n1 + n2 )/M ), which is compatible with the result in [17, 18].\\n\\n4\\n\\nExperiments\\n\\nIn this section, we conduct two numerical experiments to con?rm our analysis in Section 3.2 and\\nSection 3.3.\\n6\\n\\n\\f?4\\n\\n3\\n\\nx 10\\n\\n0.03\\n\\nsize=[50 50 20] ?M=0.03/N\\nsize=[50 50 20] ?M=0.33/N\\n\\nsize=[50 50 20] ?M=2.34/N\\n\\nsize=[50 50 20] ? =0.54/N\\n\\n0.025\\n\\nM\\n\\nsize=[100 100 50] ?M=0.66/N\\n\\nsize=[100 100 50] ?M=0.69/N\\n\\nMean squared error\\n\\nMean squared error\\n\\nsize=[50 50 20] ? =6/N\\nM\\n\\nsize=[100 100 50] ?M=0.06/N\\n2\\n\\nsize=[50 50 20] ?M=0.33/N\\n\\nsize=[100 100 50] ? =1.11/N\\nM\\n\\n1\\n\\n0.02\\n\\nsize=[100 100 50] ? =4.5/N\\nM\\n\\nsize=[100 100 50] ?M=12/N\\n0.015\\n\\n0.01\\n\\n0.005\\n\\n0\\n0\\n\\n0.2\\n\\n0.4\\n0.6\\nNormalized rank\\n\\n0.8\\n\\n0\\n0\\n\\n1\\n\\n(a) Small noise (? = 0.01).\\n\\n0.2\\n\\n0.4\\n0.6\\nNormalized rank\\n\\n0.8\\n\\n1\\n\\n(b) Large noise (? = 0.1).\\n\\nFigure 2: Result of noisy tensor decomposition for tensors of size 50 ? 50 ? 20 and 100 ? 100 ? 50.\\n\\n4.1\\n\\nNoisy Tensor Decomposition\\n\\nWe randomly generated low-rank tensors of dimensions n(1) = (50, 50, 20) and n(2) =\\n(100, 100, 50) for various ranks (r1 , . . . , rK ). For a speci?c rank, we generated the true tensor\\nby drawing elements of the r1 ? ? ? ? ? rK ?core tensor? from the standard normal distribution and\\nmultiplying its each mode by an orthonormal factor randomly drawn from the Haar measure. As\\ndescribed in Section 3.2, the observation y consists of all the elements of the original tensor once\\n(M = N ) with additive independent Gaussian noise with variance ? 2 . We used the alternating\\ndirection method of multipliers (ADMM) for ?constraint? approaches described in [23, 10] to solve\\nthe minimization problem (7). The whole experiment was repeated 10 times and averaged.\\n???\\n???\\n? ? W ? ???2 /N is plotted against\\nThe results are shown in Figure 2. The mean squared error ???W\\nF\\nthe normalized rank r? = ?n?1 ?1/2 ?r?1/2 (of the true tensor) de?ned in Equation (13). Since the\\nchoice of the regularization constant ?M only depends on the size of the tensor and not on the ranks\\nof the underlying tensor in Theorem 2, we ?x the regularization constant to some different values\\nand report the dependency of the estimation error on the normalized rank r? of the true tensor.\\nFigure 2(a) shows the result for small noise (? = 0.01) and Figure 2(b) shows the result for large\\n???\\n???\\n? ? W ? ???2 grows linearly\\nnoise (? = 0.1). As predicted by Theorem 2, the squared error ???W\\nF\\nagainst the normalized rank r?. This behaviour is consistently observed not only around the preferred\\nregularization constant value (triangles) but also in the over-?tting case (circles) and the under?tting case (crosses). Moreover, as predicted by Theorem 2, the preferred regularization constant\\nvalue scales linearly and the squared error scales quadratically to the noise standard deviation ?.\\nAs predicted by Lemma 3, the curves for the smaller 50 ? 50 ? 20 tensor and those for the larger\\n100 ? 100 ? 50 tensor seem to agree when the regularization constant\\nis scaled by the factor two.\\np\\nNote that the dominant term in inequality (12) is the second term n\\n? \\\\k , which is roughly scaled by\\nthe factor two from 50 ? 50 ? 20 to 100 ? 100 ? 50.\\n4.2\\n\\nTensor completion from partial observations\\n\\nIn this subsection, we repeat the simulation originally done by Tomioka et al. [23] and demonstrate\\nthat our results in Section 3.3 can precisely predict the empirical scaling behaviour with respect to\\nboth the size and rank of a tensor.\\nWe present results for both matrix completion (K = 2) and tensor completion (K = 3). For\\nthe matrix case, we randomly generated low-rank matrices of dimensions 50 ? 20, 100 ? 40, and\\n250 ? 200. For the tensor case, we randomly generated low-rank tensors of dimensions 50 ? 50 ? 20\\nand 100 ? 100 ? 50. We generated the matrices or tensors as in the previous subsection for various\\nranks. We randomly selected some elements of the true matrix/tensor for training and kept the\\n7\\n\\n\\f1\\n\\n0.8\\n\\n0.8\\n\\n0.6\\n0.4\\nsize=[50 20]\\nsize=[100 40]\\nsize=[250 200]\\n\\n0.2\\n0\\n0\\n\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nNormalized rank ||n?1|| ||r||\\n1/2\\n\\nFraction at Error<=0.01\\n\\nFraction at err<=0.01\\n\\n1\\n\\n0.6\\n0.4\\n0.2\\n0\\n0\\n\\n0.6\\n\\n1/2\\n\\n(a) Matrix completion (K = 2).\\n\\nsize=[50 50 20]\\nsize=[100 100 50]\\n0.2\\n0.4\\n0.6\\nNormalized rank ||n?1||1/2||r||1/2\\n\\n0.8\\n\\n(b) Tensor completion (K = 3).\\n\\nFigure 3: Scaling behaviour of matrix/tensor completion with respect to the size n and the rank r.\\n\\nremaining elements for testing. No observation noise is added. We used the ADMM for ?as a\\nmatrix? and ?constraint? approaches described in [23] to solve the minimization problem (7) for\\nmatrix completion and tensor completion, respectively. Since there is no observation noise, we\\nchose the regularization constant ? ? 0. A single experiment for a speci?c size and rank can be\\nvisualized as in Figure 1.\\n?In\\n?? Figure ?3,\\n??? we plot the minimum fraction of observations m = M/N that achieved error\\n? ? W ??? smaller than 0.01 against the normalized rank r? = ?n?1 ?1/2 ?r?1/2 (of the true ten???W\\nF\\nsor) de?ned in Equation (13). The matrix case is plotted in Figure 3(a) and the tensor case is plotted\\nin Figure 3(b). Each series (blue crosses or red circles) corresponds to different matrix/tensor size\\nand each data-point corresponds to a different core size (rank). We can see that the fraction of observations m = M/N scales linearly against the normalized rank r?, which agrees with the condition\\nM/N ? c1 ?n?1 ?1/2 ?r?1/2 = c1 r? in Theorem 3 (see Equation (14)). The agreement is especially\\ngood for tensor completion (Figure 3(b)), where the two series almost overlap. Interestingly, we\\ncan see that when compared at the same normalized rank, tensor completion is easier than matrix\\ncompletion. For example, when nk = 50 and rk = 10 for each k = 1, . . . , K, the normalized rank\\nis 0.2. From Figure 3, we can see that we only need to see 30% of the entries in the tensor case to\\nachieve error smaller than 0.01, whereas we need about 60% of the entries in the matrix case.\\n\\n5\\n\\nConclusion\\n\\nWe have analyzed the statistical performance of a tensor decomposition algorithm based on the\\noverlapped Schatten 1-norm regularization (7). Numerical experiments show that our theory can\\npredict the empirical scaling behaviour well. The fraction of observation m = M/N at the threshold\\npredicted by our theory is proportional to the quantity we call the normalized rank, which re?nes\\nconjecture (sum of the mode-k ranks) in [23].\\nThere are numerous directions that the current study can be extended. In this paper, we have focused\\non the convergence of the estimation error; it would be meaningful to also analyze the condition for\\nthe consistency of the estimated rank as in [2]. Second, although we have succeeded in predicting\\nthe empirical scaling behaviour, the setting of random Gaussian design does not match the tensor\\ncompletion setting in Section 4.2. In order to analyze the latter setting, the notion of incoherence in\\n[5] or spikiness in [16] might be useful. This might also explain why tensor completion is easier than\\nmatrix completion at the same normalized rank. Moreover, when the target tensor is only low-rank\\nin a certain mode, Schatten 1-norm regularization fails badly (as predicted by the high normalized\\nrank). It would be desirable to analyze the ?Mixture? approach that aims at this case [23]. In\\na broader context, we believe that the current paper could serve as a basis for re-examining the\\nconcept of tensor rank and low-rank approximation of tensors based on convex optimization.\\nAcknowledgments. We would like to thank Franz Kir?aly and Hiroshi Kajino for their valuable\\ncomments and discussions. This work was supported in part by MEXT KAKENHI 22700138,\\n23240019, 23120004, 22700289, and NTT Communication Science Laboratories.\\n8\\n\\n\\fReferences\\n[1] E. Acar and B. Yener. Unsupervised multiway data analysis: A literature survey. IEEE T. Knowl. Data.\\nEn., 21(1):6?20, 2009.\\n[2] F.R. Bach. Consistency of trace norm minimization. J. Mach. Learn. Res., 9:1019?1048, 2008.\\n[3] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.\\n[4] R. Bro. PARAFAC. Tutorial and applications. Chemometr. Intell. Lab., 38(2):149?171, 1997.\\n[5] E. J. Candes and B. Recht. Exact matrix completion via convex optimization. Found. Comput. Math.,\\n9(6):717?772, 2009.\\n[6] J.D. Carroll and J.J. Chang. Analysis of individual differences in multidimensional scaling via an n-way\\ngeneralization of ?Eckart-Young? decomposition. Psychometrika, 35(3):283?319, 1970.\\n[7] P. Comon. Tensor decompositions. In J. G. McWhirter and I. K. Proudler, editors, Mathematics in signal\\nprocessing V. Oxford University Press, 2002.\\n[8] L. De Lathauwer and J. Vandewalle. Dimensionality reduction in higher-order signal processing and\\nrank-(r1 , r2 , . . . , rn ) reduction in multilinear algebra. Linear Algebra Appl., 391:31?55, 2004.\\n[9] K. Fukumizu. Generalization error of linear neural networks in unidenti?able cases. In Algorithmic\\nLearning Theory, pages 51?62. Springer, 1999.\\n[10] S. Gandy, B. Recht, and I. Yamada. Tensor completion and low-n-rank tensor recovery via convex optimization. Inverse Problems, 27:025010, 2011.\\n[11] J. H?astad. Tensor rank is NP-complete. Journal of Algorithms, 11(4):644?654, 1990.\\n[12] T. G. Kolda and B. W. Bader. Tensor decompositions and applications. SIAM Review, 51(3):455?500,\\n2009.\\n[13] J. Liu, P. Musialski, P. Wonka, and J. Ye. Tensor completion for estimating missing values in visual data.\\nIn Prof. ICCV, 2009.\\n[14] M. M?rup. Applications of tensor (multiway array) factorizations and decompositions in data mining.\\nWiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 1(1):24?40, 2011.\\n[15] S. Negahban, P. Ravikumar, M. Wainwright, and B. Yu. A uni?ed framework for high-dimensional\\nanalysis of m-estimators with decomposable regularizers. In Y. Bengio, D. Schuurmans, J. Lafferty,\\nC. K. I. Williams, and A. Culotta, editors, Advances in NIPS 22, pages 1348?1356. 2009.\\n[16] S. Negahban and M.J. Wainwright. Restricted strong convexity and weighted matrix completion: Optimal\\nbounds with noise. Technical report, arXiv:1009.2118, 2010.\\n[17] S. Negahban and M.J. Wainwright. Estimation of (near) low-rank matrices with noise and highdimensional scaling. Ann. Statist., 39(2), 2011.\\n[18] B. Recht, M. Fazel, and P.A. Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via\\nnuclear norm minimization. SIAM Review, 52(3):471?501, 2010.\\n[19] A. Rohde and A.B. Tsybakov.\\n39(2):887?930, 2011.\\n\\nEstimation of high-dimensional low-rank matrices.\\n\\nAnn. Statist.,\\n\\n[20] N.D. Sidiropoulos, R. Bro, and G.B. Giannakis. Parallel factor analysis in sensor array processing. IEEE\\nT. Signal Proces., 48(8):2377?2388, 2000.\\n[21] M. Signoretto, L. De Lathauwer, and J.A.K. Suykens. Nuclear norms for tensors and their use for convex\\nmultilinear estimation. Technical Report 10-186, ESAT-SISTA, K.U.Leuven, 2010.\\n[22] N. Srebro, J. D. M. Rennie, and T. S. Jaakkola. Maximum-margin matrix factorization. In Lawrence K.\\nSaul, Yair Weiss, and L?eon Bottou, editors, Advances in NIPS 17, pages 1329?1336. MIT Press, Cambridge, MA, 2005.\\n[23] R. Tomioka, K. Hayashi, and H. Kashima. Estimation of low-rank tensors via convex optimization.\\nTechnical report, arXiv:1010.0789, 2011.\\n[24] L. R. Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika, 31(3):279?311,\\n1966.\\n[25] M. Vasilescu and D. Terzopoulos. Multilinear analysis of image ensembles: Tensorfaces. Computer\\nVision?ECCV 2002, pages 447?460, 2002.\\n[26] H. Wang and N. Ahuja. Facial expression decomposition. In Proc. 9th ICCV, pages 958 ? 965, 2003.\\n\\n9\\n\\n\\f\",\n          \"Effects of Spatial and Temporal Contiguity on\\nthe Acquisition of Spatial Information\\n\\nThea B. Ghiselli-Crippa and Paul W. Munro\\nDepartment of Information Science and Telecommunications\\nUniversity of Pittsburgh\\nPittsburgh, PA 15260\\ntbgst@sis.pitt.edu, munro@sis.pitt.edu\\n\\nAbstract\\nSpatial information comes in two forms: direct spatial information (for\\nexample, retinal position) and indirect temporal contiguity information,\\nsince objects encountered sequentially are in general spatially close. The\\nacquisition of spatial information by a neural network is investigated\\nhere. Given a spatial layout of several objects, networks are trained on a\\nprediction task. Networks using temporal sequences with no direct spatial information are found to develop internal representations that show\\ndistances correlated with distances in the external layout. The influence\\nof spatial information is analyzed by providing direct spatial information\\nto the system during training that is either consistent with the layout or\\ninconsistent with it. This approach allows examination of the relative\\ncontributions of spatial and temporal contiguity.\\n\\n1 Introduction\\nSpatial information is acquired by a process of exploration that is fundamentally temporal, whether it be on a small scale, such as scanning a picture, or on a larger one, such as\\nphysically navigating through a building, a neighborhood, or a city. Continuous scanning\\nof an environment causes locations that are spatially close to have a tendency to occur in\\ntemporal proximity to one another. Thus, a temporal associative mechanism (such as a\\nHebb rule) can be used in conjunction with continuous exploration to capture the spatial\\nstructure of the environment [1]. However, the actual process of building a cognitive map\\nneed not rely solely on temporal associations, since some spatial information is encoded in\\nthe sensory array (position on the retina and proprioceptive feedback). Laboratory studies\\nshow different types of interaction between the relative contributions of temporal and spatial contiguities to the formation of an internal representation of space. While Clayton and\\nHabibi's [2] series of recognition priming experiments indicates that priming is controlled\\nonly by temporal associations, in the work of McNamara et al. [3] priming in recognition is observed only when space and time are both contiguous. In addition, Curiel and\\nRadvansky's [4] work shows that the effects of spatial and temporal contiguity depend on\\nwhether location or identity information is emphasized during learning. Moreover, other\\nexperiments ([3]) also show how the effects clearly depend on the task and can be quite\\ndifferent if an explicitly spatial task is used (e.g., additive effects in location judgments).\\n\\n\\fT. B. Ghiselli-Crippa and P W. Munro\\n\\n18\\n\\nlabels\\n\\nlabels\\n\\nlabels\\n(A coeff.)\\n\\nlabels\\n\\nlabels\\n\\ncoordinates\\n\\ncoordinates\\n(B coeff.)\\n\\nlabels\\n\\nFigure 1: Network architectures: temporal-only network (left); spatio-temporal network\\nwith spatial units part of the input representation (center); spatio-temporal network with\\nspatial units part of the output representation (right) .\\n\\n2 Network architectures\\nThe goal of the work presented in this paper is to study the structure of the internal representations that emerge from the integration of temporal and spatial associations. An\\nencoder-like network architecture is used (see Figure 1), with a set of N input units and a\\nset of N output units representing N nodes on a 2-dimensional graph. A set of H units is\\nused for the hidden layer. To include space in the learning process, additional spatial units\\nare included in the network architecture. These units provide a representation of the spatial\\ninformation directly available during the learning/scanning process. In the simulations described in this paper, two units are used and are chosen to represent the (x, y) coordinates of\\nthe nodes in the graph . The spatial units can be included as part of the input representation\\nor as part of the output representation (see Figure 1, center and right panels): both choices\\nare used in the experiments, to investigate whether the spatial information could better benefit training as an input or as an output [5]. In the second case, the relative contribution of\\nthe spatial information can be directly manipulated by introducing weighting factors in the\\ncost function being minimized. A two-term cost function is used, with a cross-entropy term\\nfor the N label units and a squared error term for the 2 coordinate units,\\n\\nri indicates the actual output of unit i and ti its desired output. The relative influence of\\n\\nthe spatial information is controlled by the coefficients A and B.\\n\\n3\\n\\nLearning tasks\\n\\nThe left panel of Figure 2 shows an example of the type of layout used; the effective\\nlayout used in the study consists of N = 28 nodes. For each node, a set of neighboring\\nnodes is defined, chosen on the basis of how an observer might scan the layout to learn the\\nnode labels and their (spatial) relationships; in Figure 2, the neighborhood relationships are\\nrepresented by lines connecting neighboring nodes. From any node in the layout, the only\\nallowed transitions are those to a neighbor, thus defining the set of node pairs used to train\\nthe network (66 pairs out of C(28, 2) = 378 possible pairs). In addition, the probability\\nof occurrence of a particular transition is computed as a function of the distance to the\\ncorresponding neighbor. It is then possible to generate a sequence of visits to the network\\nnodes, aimed at replicating the scanning process of a human observer studying the layout.\\n\\n\\f19\\n\\nSpatiotemporal Contiguity Effects on Spatial Information Acquisition\\n\\neraser\\n\\nknife\\n\\ncup\\n\\ncoin\\n\\neraser\\n\\nbutton\\n\\nFigure 2: Example of a layout (left) and its permuted version (right). Links represent\\nallowed transitions. A larger layout of 28 units was used in the simulations.\\n\\nThe basic learning task is similar to the grammar learning task of Servan-Schreiber et al.\\n[6] and to the neighborhood mapping task described in [1] and is used to associate each of\\nthe N nodes on the graph and its (x, y) coordinates with the probability distribution of the\\ntransitions to its neighboring nodes. The mapping can be learned directly, by associating\\neach node with the probability distribution of the transitions to all its neighbors: in this\\ncase, batch learning is used as the method of choice for learning the mapping. On the\\nother hand, the mapping can be learned indirectly, by associating each node with itself\\nand one of its neighbors, with online learning being the method of choice in this case;\\nthe neighbor chosen at each iteration is defined by the sequence of visits generated on\\nthe basis of the transition probabilities. Batch learning was chosen because it generally\\nconverges more smoothly and more quickly than online learning and gives qualitatively\\nsimilar results. While the task and network architecture described in [1] allowed only\\nfor temporal association learning, in this study both temporal and spatial associations are\\nlearned simultaneously, thanks to the presence of the spatial units. However, the temporalonly (T-only) case, which has no spatial units, is included in the simulations performed\\nfor this study, to provide a benchmark for the evaluation of the results obtained with the\\nspatio-temporal (S- T) networks.\\nThe task described above allows the network to learn neighborhood relationships for which\\nspatial and temporal associations provide consistent information, that is, nodes experienced\\ncontiguously in time (as defined by the sequence) are also contiguous in space (being spatial neighbors). To tease apart the relative contributions of space and time, the task is kept\\nthe same, but the data employed for training the network is modified: the same layout is\\nused to generate the temporal sequence, but the x , y coordinates of the nodes are randomly\\npermuted (see right panel of Figure 2). If the permuted layout is then scanned following the\\nsame sequence of node visits used in the original version, the net effect is that the temporal\\nassociations remain the same, but the spatial associations change so that temporally neighboring nodes can now be spatially close or distant: the spatial associations are no longer\\nconsistent with the temporal associations. As Figure 4 illustrates, the training pairs (filled\\ncircles) all correspond to short distances in the original layout, but can have a distance\\nanywhere in the allowable range in the permuted layout. Since the temporal and spatial\\ndistances were consistent in the original layout, the original spatial distance can be used\\nas an indicator of temporal distance and Figure 4 can be interpreted as a plot of temporal\\ndistance vs. spatial distance for the permuted layout.\\nThe simulations described in the following include three experimental conditions: temporal\\nonly (no direct spatial information available); space and time consistent (the spatial coordinates and the temporal sequence are from the same layout); space and time inconsistent\\n(the spatial coordinates and the temporal sequence are from different layouts).\\n\\n\\fT. B. Ghise/li-Crippa and P. W. Munro\\n\\n20\\n\\nHidden unit representations are compared using Euclidean distance (cosine and inner product measures give consistent results); the internal representation distances are also used to\\ncompute their correlation with Euclidean distances between nodes in the layout (original\\nand permuted). The correlations increase with the number of hidden units for values of\\nH between 5 and 10 and then gradually taper off for values greater than 10. The results\\npresented in the remainder of the paper all pertain to networks trained with H = 20 and\\nwith hidden units using a tanh transfer function; all the results pertaining to S-T networks\\nrefer to networks with 2 spatial output units and cost function coefficients A = 0.625 and\\nB = 6.25.\\n\\n4 Results\\nFigure 3 provides a combined view of the results from all three experiments. The left panel\\nillustrates the evolution of the correlation between internal representation distances and\\nlayout (original and permuted) distances. The right panel shows the distributions of the\\ncorrelations at the end of training (1000 epochs). The first general result is that, when spatial information is available and consistent with the temporal information (original layout),\\nthe correlation between hidden unit distances and layout distances is consistently better\\nthan the correlation obtained in the case of temporal associations alone. The second general result is that, when spatial information is available but not consistent with the temporal\\ninformation (permuted layout), the correlation between hidden unit distances and original\\nlayout distances (which represent temporal distances) is similar to that obtained in the case\\nof temporal associations alone, except for the initial transient. When the correlation is computed with respect to the permuted layout distances, its value peaks early during training\\nand then decreases rapidly, to reach an asymptotic value well below the other three cases.\\nThis behavior is illustrated in the box plots in the right panel of Figure 3, which report the\\ndistribution of correlation values at the end of training.\\n\\n4.1\\n\\nTemporal-only vs. spatio-temporal\\n\\nAs a first step in this study, the effects of adding spatial information to the basic temporal\\nassociations used to train the network can be examined. Since the learning task is the same\\nfor both the T-only and the S-T networks except for the absence or presence of spatial\\ninformation during training, the differences observed can be attributed to the additional\\nspatial information available to the S-T networks. The higher correlation between internal\\nrepresentation distances and original layout distances obtained when spatial information is\\n\\n0\\n\\n~\\n\\n.,\\n\\n8\\n\\n.,\\n\\nS and T CO\\\"Isistent\\n\\n0\\n\\n.\\n\\n0\\n\\n.\\n\\nT-o\\\"\\nSand T InCOnsistent\\n\\n0\\n\\ni:i\\n\\n-==~\\n\\n0\\n\\n(corr with T distance)\\n\\nii\\n\\n...\\n\\n?8 \\\"\\n\\n\\\"0\\n\\n0\\n\\n=s:\\n...........\\nE:2\\n\\nS and T Ir'ICOOSlStent\\n(corr. Wflh S distance)\\n\\n'\\\"ci\\n\\n~\\n\\n--'----'\\n\\nN\\n\\n0\\n\\n0\\n0\\n\\n0\\n0\\n\\n200\\n\\n400\\n600\\nOllnber 01 epochs\\n\\n800\\n\\n1000\\n\\nSandT\\n\\ncon_atent\\n\\nT-only\\n\\nSandT\\nSandT\\nInconsistent\\nineon.stant\\n(corr \\\" th T ast ) (corr wth 5 dst )\\n\\nFigure 3: Evolution of correlation during training (0 - 1000 epochs) (left). Distributions of\\ncorrelations at the end of training (1000 epochs) (right).\\n\\n\\fSpatiotemporal Contiguity Effects on Spatial Information Acquisition\\n\\n-\\n\\n21\\n\\nN\\n\\ndHU = 0.6 + 3.4d T + 0.3ds - 2.1( dT)2 + 0.4( d S )2 - 0.4d T ds\\n\\n0\\n\\n.,\\n\\n25\\n\\n0\\n\\n\\\",\\nE\\n~\\n\\n'\\\"\\n0\\n\\n15\\n\\n...\\n0\\n\\n05\\nN\\n\\n0\\n\\n15\\n0\\n0\\n\\n00\\n\\n02\\n\\n04\\n\\n08\\n\\n10\\n\\n12\\n\\nFigure 4: Distances in the original layout\\n(x) vs_ distances in the permuted layout\\n(y)_ The 66 training pairs are identified by\\nfilled circles_\\n\\n\\\"\\nFigure 5: Similarities (Euclidean distances)\\nbetween internal representations developed\\nby a S-T network (after 300 epochs)_ Figure\\n4 projects the data points onto the x, y plane_\\n\\navailable (see Figure 3) is apparent also when the evolution of the internal representations\\nis examined_ As Figure 6 illustrates, the presence of spatial information results in better\\ngeneralization for the pattern pairs outside the training set While the distances between\\ntraining pairs are mapped to similar distances in hidden unit space for both the T-only and\\nthe S-T networks, the T-only network tends to cluster the non-training pairs into a narrow\\nband of distances in hidden unit space. In the case of the S-T network instead, the hidden\\nunit distances between non-training pairs are spread out over a wider range and tend to\\nreflect the original layout distances.\\n4.2\\n\\nPermuted layout\\n\\nAs described above, with the permuted layout it is possible to decouple the spatial and\\ntemporal contributions and therefore study the effects of each. A comprehensive view of\\nthe results at a particular point during training (300 epochs) is presented in Figure 5, where\\nthe x, y plane represents temporal distance vs. spatial distance (see also Figure 4) and the z\\naxis represents the similarity between hidden unit representations. The figure also includes\\na quadratic regression surface fitted to the data points. The coefficients in the equation of\\nthe surface provide a quantitative measure of the relative contributions of spatial (ds) and\\ntemporal distances (dT ) to the similarity between hidden unit representations (d HU ):\\n(2)\\n\\nIn general, after the transient observed in early training (see Figure 3), the largest and most\\nsignificant coefficients are found for dT and (dT?, indicating a stronger dependence of\\ndHU on temporal distance than on spatial distance.\\nThe results illustrated in Figure 5 represent the situation at a particular point during training\\n(300 epochs). Similar plots can be generated for different points during training, to study\\nthe evolution of the internal representations. A different view of the evolution process is\\nprovided by Figure 7, in which the data points are projected onto the x,Z plane (top panel)\\nand the y,z plane (bottom panel) at four different times during training. In the top panel,\\n\\n14\\n\\n\\fT. B. Ghiselli-Crippa and P W Munro\\n\\n22\\n\\n,.. ,..\\n.. roo\\n:::\\n\\n~\\n_\\n\\n0\\n\\n?\\n\\nN\\n\\n~ ~\\n~ ~\\n\\n~\\n\\n-. .. -\\n\\n:::\\n~\\n\\n~\\n\\n02\\n\\n\\\"\\n\\n06\\n\\nO.\\n\\n\\\"_d\\n\\n\\\"\\n\\n12\\n\\n.\\n\\n,,\\n\\n~'\\n\\n~ :\\n~\\n\\n~\\n~,\\n\\n02\\n\\nos\\n\\n\\\"-'\\n\\n..\\n\\n'\\n\\n02\\n\\n.. .\\n06\\n\\n-\\n\\n,\\n\\ntP\\n\\n.\\n\\nDO\\n\\n0\\n\\n, ,\\n.I'\\n\\n~\\n\\n12\\n\\n.',\\n\\n00\\n\\n02\\n\\n\\\" \\\"-'\\n06\\n\\n.\\\"\\n\\n02\\n\\n~\\n\\n~\\n\\n.~.\\n','\\n\\n~\\n\\n.. .. \\\" \\\"\\n\\n~\\n\\n06\\n\\n00\\n\\n02\\n\\n\\\"_d\\n\\n_\\n\\n0\\n\\n?\\n\\nN\\n\\n~\\n\\n:\\n\\n~,\\n\\n~ ~\\n\\n..\\n\\nf/Po\\n\\n<P\\n\\n\\\"\\n\\ne,\\n\\n.\\n\\nDO\\n\\n.:.\\n\\n~\\n\\n00\\n\\n02\\n\\n\\\"\\n\\n.. ..\\n\\n\\\"-'\\n\\n10\\n\\n12\\n\\n.. .. .. \\\"\\n\\n12\\n\\n\\\" _d\\n\\n:::\\n\\n~\\n,\\n\\n~\\n12\\n\\n0\\nN\\n\\n~ ~\\n\\n',~-,\\n\\n00\\n\\n~\\n\\n~\\n\\n~\\n_\\n?\\n\\n~\\n\\n~\\n\\n12\\n\\n\\\"\\n\\n0\\n\\n00\\n\\n~\\n\\n:::\\n\\ng\\n10\\n\\n~\\n\\n~\\n\\n~ ~\\n\\nO.\\n\\n,\\n\\n:::\\n\\n;; ~\\n\\n.~\\n00\\n\\n00\\n\\n~.\\n\\n~\\n\\n,\\n\\n\\\"_d\\n\\n,\\n\\n:; ~\\n\\n~\\n\\n~\\n\\n~\\n\\n00\\n\\n~\\n\\n~\\n\\n~\\n\\n~\\n\\ni\\n\\n~\\n\\n~\\n\\n~,\\n\\n:::\\n\\n~\\n\\n~\\n\\n~\\n_\\n\\n0\\n\\n?\\n\\nN\\n\\n~\\n\\n~\\n\\n~\\n\\n~\\n\\no\\n\\n,.~,o\\n\\n: s\\n\\nrIP 0\\n\\n00\\n\\n0\\n\\n?\\n\\n','\\n\\n00\\n\\n02\\n\\n\\\"\\n\\nO.\\n\\n\\\"-'\\n\\no.\\n\\n\\\"\\n\\n12\\n\\nFigure 6: Internal representation distances vs. original layout distances: S-T network (top)\\nvs. T-only network (bottom). The training pairs are identified by filled circles. The presence\\nof spatial information results in better generalization for the pairs outside the training set.\\nthe internal representation distances are plotted as a function of temporal distance (i.e., the\\nspatial distance from the original layout), while in the bottom panel they are plotted as a\\nfunction of spatial distance (from the permuted layout). The higher asymptotic correlation\\nbetween internal representation distances and temporal distances, as opposed to spatial\\ndistances (see Figure 3), is apparent also from the examination of the evolutionary plots,\\nwhich show an asymptotic behavior with respect to temporal distances (see Figure 7, top\\npanel) very similar to the T-only case (see Figure 6, bottom panel).\\n\\n5 Discussion\\nThe first general conclusion that can be drawn from the examination of the results described\\nin the previous section is that, when the spatial information is available and consistent with\\nthe temporal information (original layout), the similarity structure of the hidden unit representations is closer to the structure of the original layout than that obtained by using\\ntemporal associations alone. The second general conclusion is that, when the spatial information is available but not consistent with the temporal information (permuted layout),\\nthe similarity structure of the hidden unit representations seems to correspond to temporal\\nmore than spatial proximity. Figures 5 and 7 both indicate that temporal associations take\\nprecedence over spatial associations. This result is in agreement with the results described\\nin [1], showing how temporal associations (plus some high-level constraints) significantly\\ncontribute to the internal representation of global spatial information. However, spatial information certainly is very beneficial to the (temporal) acquisition of a layout, as proven by\\nthe results obtained with the S-T network vs. the T-only network.\\nIn terms of the model presented in this paper, the results illustrated in Figures 5 and 7 can\\nbe compared with the experimental data reported for recognition priming ([2], [3], [4]),\\nwith distance between internal representations corresponding to reaction time. The results\\nof our model indicate that distances in both the spatially far and spatially close condition\\nappear to be consistently shorter for the training pairs (temporally close) than for the nontraining pairs (temporally distant), highlighting a strong temporal effect consistent with the\\ndata reported in [2] and [4] (for spatially far pairs) and in [3] (only for the spatially close\\n\\n\\fSpatiotemporal Contiguity Effects on Spatial Information Acquisition\\n\\n~~\\n\\n0_\\n\\nri\\n\\n; -~-'\\n~. ~~.. .\\nSl\\n...........\\n0\\n\\n\\\" ...... .\\nj!I!A\\n\\n..\\n,.\\n\\n0\\n\\n,.\\n\\n~\\n\\n~\\n\\n~\\n\\n23\\n\\n\\\\\\n\\n0\\n\\n?\\n\\nlfIiiIo\\n\\n'0'\\n\\n110\\n\\n0\\n\\n~'--_ _ _ _ _-.J\\n\\n00\\n\\n02\\n\\nO.\\n\\n01\\n\\n01\\n\\n10\\n\\n12\\n\\n00\\n\\n02\\n\\nO.\\n\\n~\\n\\n01\\n\\n01\\n\\n10\\n\\n12\\n\\n00\\n\\n02\\n\\n0.4\\n\\n01\\n\\n01\\n\\n10\\n\\n12\\n\\n02\\n\\n0\\\"\\n\\n01\\nIn_d (S)\\n\\n01\\n\\n'0\\n\\n12\\n\\n~l.-\\n\\n00\\n\\n0.2\\n\\no.\\n\\not\\n.._d(S)\\n\\n02\\n\\nO.\\n\\n01\\n\\n10\\n\\n12\\n\\n0.0\\n\\n02\\n\\n04\\n\\n08\\n...u:I (S)\\n\\n01\\n\\noa\\n\\n10\\n\\n12\\n\\nIn_den\\n\\nL..-_ _ _ _- . l\\n00\\n\\n0 0\\n\\nl'I_d (T)\\n\\nIn_d(TI\\n\\nIn _d (T}\\n\\nall\\n\\n10\\n\\n12\\n\\n00\\n\\n_ _ _ _ _-.J\\n02\\n\\nO.\\n\\n06\\n\\noa\\n\\n10\\n\\n12\\n\\n!rUi (S)\\n\\nFigure 7: Internal representation distances vs. temporal distances (top) and vs. spatial\\ndistances (bottom) for a S-T network (permuted layout). The training pairs are identified\\nby filled circles. The asymptotic behavior with respect to temporal distances (top panel) is\\nsimilar to the T-only condition. The bottom panel indicates a weak dependence on spatial\\ndistances.\\ncase). For the training pairs (temporally close), slightly shorter distances are obtained for\\nspatially close pairs vs. spatially far pairs; this result does not provide support for the\\nexperimental data reported in either [3] (strong spatial effect) or [2] (no spatial effect).\\nFor the non-training pairs (temporally distant), long distances are found throughout, with\\nno strong dependence on spatial distance; this effect is consistent with all the reported\\nexperimental data. Further simulations and statistical analyses are necessary for a more\\nconclusive comparison with the experimental data.\\nReferences\\n[1] Ghiselli-Crippa, TB. & Munro, P.w. (1994). Emergence of global structure from local associations. In J.D. Cowan, G. Tesauro, & J. Alspector (Eds.), Advances in Neural Information Processing\\nSystems 6, pp. 1101-1108. San Francisco, CA: Morgan Kaufmann.\\n[2] Clayton, K.N. & Habibi, A. (1991). The contribution of temporal contiguity to the spatial priming\\neffect. Journal of Experimental Psychology: Learning. Memory. and Cognition 17:263-271.\\n[3] McNamara, TP., Halpin. J.A. & Hardy, J.K. (1992). Spatial and temporal contributions to the\\nstructure of spatial memory. Journal of Experimental Psychology: Learning. Memory. and Cognition\\n18:555-564.\\n[4] Curiel, J.M. & Radvansky, G.A. (1998). Mental organization of maps. Journal of Experimental\\nPsychology: Learning. Memory. and Cognition 24:202-214.\\n[5] Caruana, R. & de Sa, VR. (1997). Promoting poor features to supervisors: Some inputs work\\nbetter as outputs . In M.e. Mozer, M.I. Jordan, & T Petsche (Eds.), Advances in Neural Information\\nProcessing Systems 9, pp. 389-395. Cambridge, MA: MIT Press.\\n[6] Servan-Schreiber, D., Cleeremans, A. & McClelland, J.L. (1989). Learning sequential structure\\nin simple recurrent networks. In D.S. Touretzky (Ed.), Advances in Neural Information Processing\\nSystems 1, pp. 643-652. San Mateo, CA: Morgan Kaufmann.\\n\\n\\fNeural Representation of Multi-Dimensional\\nStimuli\\n\\nChristian W. Eurich, Stefan D. Wilke and Helmut Schwegler\\nInstitut fUr Theoretische Physik\\nUniversitat Bremen, Germany\\n(eurich,swilke,schwegler)@physik.uni-bremen.de\\n\\nAbstract\\nThe encoding accuracy of a population of stochastically spiking neurons\\nis studied for different distributions of their tuning widths. The situation\\nof identical radially symmetric receptive fields for all neurons, which\\nis usually considered in the literature, turns out to be disadvantageous\\nfrom an information-theoretic point of view. Both a variability of tuning widths and a fragmentation of the neural population into specialized\\nsubpopulations improve the encoding accuracy.\\n\\n1 Introduction\\nThe topic of neuronal tuning properties and their functional significance has focused much\\nattention in the last decades. However, neither empirical findings nor theoretical considerations have yielded a unified picture of optimal neural encoding strategies given a sensory\\nor motor task. More specifically, the question as to whether narrow tuning or broad tuning\\nis advantageous for the representation of a set of stimulus features is still being discussed.\\nEmpirically, both situations are encountered: small receptive fields whose diameter is less\\nthan one degree can, for example, be found in the human retina [7] , and large receptive\\nfields up to 180 0 in diameter occur in the visual system of tongue-projecting salamanders\\n[10]. On the theoretical side, arguments have been put forward for small [8] as well as for\\nlarge [5, 1,9, 3, 13] receptive fields.\\nIn the last years, several approaches have been made to calculate the encoding accuracy\\nof a neural population as a function of receptive field size [5, 1,9,3, 13]. It has turned\\nout that for a firing rate coding, large receptive fields are advantageous provided that D 2:\\n3 stimulus features are encoded [9, 13]. For binary neurons, large receptive fields are\\nadvantageous also for D = 2 [5,3].\\nHowever, so far only radially symmetric tuning curves have been considered. For neural\\npopulations which lack this symmetry, the situation may be very different. Here we study\\nthe encoding accuracy of a popUlation of stochastically spiking neurons. A Fisher information analysis performed on different distributions of tunings widths will indeed reveal a\\nmuch more detailed picture of neural encoding strategies.\\n\\n\\fC. W. Eurich. S. D. Wilke and H. Schwegler\\n\\nJ J6\\n\\n2 Model\\nConsider a D-dimensional stimulus space, X. A stimulus is characterized by a position\\nx\\n(Xl, ... , XD) E X, where the value of feature i, Xi (i\\n1, ... , D), is measured\\nrelative to the total range of values in the i-th dimension such that it is dimensionless.\\nInformation about the stimulus is encoded by a popUlation of N stochastically spiking\\nneurons. They are assumed to have independent spike generation mechanisms such that the\\njoint probability distribution for observing n = (n(l), ... ,n(k), ... ,n(N?) spikes within a\\ntime interval T, Ps(n; x), can be written in the form\\n\\n=\\n\\n=\\n\\nN\\n\\nPs(n;x) =\\n\\nII\\n\\nps(k) (n(k);\\n\\nx),\\n\\n(1)\\n\\nk=l\\nwhere Ps(k) (n(k); x) is the single-neuron probability distribution of the number of observed\\nspikes given the stimulus at position x. Note that (1) does not exclude a correlation of the\\nneural firing rates, i.e., the neurons may have common input or even share the same tuning\\nfunction.\\nThe firing rates depend on the stimulus via the local values of the tuning functions, such that\\nx) can be written in the form Ps(k) (n(k); x) = S (n(k), j(k) (x), T), where the\\ntuning function of neuron k, j(k) (x), gives its mean firing rate in response to the stimulus\\nat position x. We assume here a form of the tuning function that is not necessarily radially\\nsymmetric,\\nPs(k) (n(k);\\n\\nf(') (x)\\n\\n= F4>\\n\\n(t\\n\\n(Xi\\n\\n~~r) )2) =, F? ( e( ')2) ,\\n\\n(2)\\n\\nwhere e(k) = (c~k), ... , c};?) is the center of the tuning curve of neuron k, O'~k) is its\\ntuning width in the i-th dimension, k )2 := (Xi - c~k?)2/O'ik)2 for i = 1, ... ,D, and\\n~(k)2 := ~~k)2 + ... + ~~)2. F > 0 denotes the maximal firing rate of the neurons, which\\nrequires that maxz~o fj>(z) = 1.\\n\\nd\\n\\nWe assume that the tuning widths O't), . .. ,O'~) of each neuron k are drawn from a distribution PO' (0'1, ... ,O'D). For a population oftuning functions with centers e(l), ... , e(N), a\\ndensity 1}(x) is introduced according to 1}(x) := L:~=l 8(x - e(k?).\\nThe encoding accuracy can be quantified by the Fisher information matrix, J, which is\\ndefined as\\n(3)\\n\\nwhere E[ . ..J denotes the expectation value over the probability distribution P(n; x) [2].\\nThe Fisher information yields a lower bound on the expected error of an unbiased estimator\\nthat retrieves the stimulus x from the noisy neural activity (Cramer-Rao inequality) [2]. The\\nminimal estimation error for the i-th feature Xi, ti,min, is given by t;,min = (J - 1 )ii which\\nreduces to t;,min = 1/ Jii(X) if J is diagonal.\\nWe shall now derive a general expression for the popUlation Fisher information. In the\\nnext chapter, several cases and their consequences for neural encoding strategies will be\\ndiscussed.\\nFor model neuron (k), the Fisher information (3) reduces to\\n(k)\\n\\nJ ij\\n\\n.\\n\\n(k)\\n\\n(X'O'I\\n\\n(k) _\\n\\\"\\\"'O'D) -\\n\\n1\\n\\n(k)\\nO'i\\n\\n(k)Aq..\\nO'j\\n\\n(\\n\\n~\\n\\n(k)2\\n\\n,F,T\\n\\n)\\n\\n(k) (k)\\n\\n~i ~j\\n\\n,\\n\\n(4)\\n\\n\\f117\\n\\nNeural Representation of Multi-Dimensional Stimuli\\n\\nwhere the dependence on the tuning widths is indicated by the list of arguments. The\\nfunction A.p depends on the shape of the tuning function and is given in [13]. The independence assumption (1) implies that the population Fisher information is the sum of\\n. d??d\\nI\\n\\\",N J(k)(\\n(k)\\n(k)) . U7\\nt he contn?b?\\nutlOns 0 f the III\\nIVI ua neurons, L.Jk=1 ij x; 0\\\"1 , ... ,0\\\"D\\nne now define\\na population Fisher information which is averaged over the distribution of tuning widths\\nPt:T(0\\\"1, . .. ,O\\\"D):\\nN\\n\\n(Jij (x)) 17 =\\n\\nL / d0\\\"1 . .. dO\\\"D Pt:T(0\\\"1,? .. , O\\\"D) Ji~k) (x; 0\\\"1, ? .. , O\\\"D) .\\n\\n(5)\\n\\nk= 1\\n\\nIntroducing the density of tuning curves, 1J(x), into (5) and assuming a constant distribution, 1J(x) == 1J == const., one obtains the result that the population Fisher information\\nbecomes independentofx and that the off-diagonal elements of J vanish [13]. The average\\npopulation Fisher information then becomes\\n(Jij)t:T =\\n\\n1JD K.p (F, r, D ) \\\\/\\n\\nflt:l\\n0\\\"1) ~\\n0\\\";\\nVij,\\n17\\n\\n(6)\\n\\nwhere K.p depends on the geometry of the tuning curves and is defined in [13].\\n\\n3 Results\\nIn this section, we consider different distributions of tuning widths in (6) and discuss advantageous and disadvantageous strategies for obtaining a high representational accuracy\\nin the neural population.\\nRadially symmetric tuning curves.\\nthe tuning-width distribution reads\\n\\nFor radially symmetric tuning curves of width a,\\nD\\n\\nPt:T(O\\\"l, .. . ,O\\\"D)\\n\\n= II O(O\\\"i -a);\\ni=l\\n\\nsee Fig. 1a for a schematic visualization of the arrangement of the tuning widths for the\\ncase D = 2. The average population Fisher information (6) for i = j becomes\\n(Jii)t:T =\\n\\n1JDK.p(F, r, D) aD -\\n\\n2,\\n\\n(7)\\n\\na result already obtained by Zhang and Sejnowski [13]. Equation (7) basically shows that\\nthe minimal estimation error increases with a for D = 1, that it does not depend on a for\\nD = 2, and that it decreases as a increases for D 2: 3. We shall discuss the relevance of\\nthis case below.\\nIdentical tuning curves without radial symmetry. Next we discuss tuning curves which\\nare identical but not radially symmetric; the tuning-width distribution for this case is\\nD\\n\\nPt:T(0\\\"1, . .. ,O\\\"D)\\n\\n=\\n\\nII\\n\\nO(O\\\"i -\\n\\nad,\\n\\ni=l\\n\\nwhere ai denotes the fixed width in dimension i. For i = j, the average population Fisher\\ninformation (6) reduces to [11,4]\\n(Jii)t:T = 1JDK.p ( F,\\n\\nr,\\n\\nD)\\n\\nDfl 1=1\\n0\\\"1\\n-2\\n\\nO\\\"i\\n\\n.\\n\\n(8)\\n\\n\\fc.\\n\\n118\\n\\n(a)\\n\\nW. Eurich, S. D. Wilke and H. Schwegler\\n\\n(b)\\n\\n/\\n\\nFigure 1: Visualization of different distributions of\\ntuning widths for D = 2. (a) Radially symmetric tuning curves. The dot indicates a fixed (j, while the diagonalline symbolizes a variation in (j discussed in [13].\\n(b) Identical tuning curves which are not radially symmetric. (c) Tuning widths uniformly distributed within\\na small rectangle. (d) Two sUbpopulations each of\\nwhich is narrowly tuned in one dimension and broadly\\ntuned in the other direction.\\n\\n.\\n\\n(c)\\n\\n(d)\\n\\n.\\n\\nb _ b\\n2\\n\\n.\\n\\nEquation (8) contains (7) as a special case. From (8) it becomes immediately clear that the\\nexpected minimal square encoding error for the i-th stimulus feature, ?~ min = 1/ (Jii(X))u,\\ndepends on i, i. e., the population specializes in certain features. The error obtained in\\ndimension i thereby depends on the tuning widths in all dimensions.\\nWhich encoding strategy is optimal for a population whose task it is to encode a single\\nfeature, say feature i, with high accuracy while not caring about the other dimensions? In\\norder to answer this question, we re-write (8) in terms of receptive field overlap.\\nFor the tuning functions f(k) (x) encountered empirically, large values ofthe single-neuron\\nFisher information (4) are typically restricted to a region around the center of the tuning\\nfunction, c(k). The fraction p({3) of the Fisher information that falls into a region ED\\nJ~(k)2 ~ (3 aroundc(k) is given by\\n\\nf\\np({3)\\n\\n:=\\n\\nd\\n\\nE; d\\nX\\n\\nD\\n\\nD\\n\\n\\\"\\\",D\\nX L....i=l\\n\\nX\\n\\n(k) ( )\\nJ ii X\\n\\n2:~t=l J~~)\\n( )\\nu\\nX\\n\\nj3\\n\\nf\\n\\nd~ ~D+l At/>(e, F, T)\\n\\no\\n\\n(9)\\n\\n00\\n\\nf\\n\\nd~ ~D+l At/>(~2, F, T)\\n\\no\\n\\nwhere the index (k) was dropped because the tuning curves are assumed to have identical shapes. Equation (9) allows the definition of an effective receptive field, RF~~,\\ninside of which neuron k conveys a major fraction Po of Fisher information, RF~~ :=\\n\\n{xl~ ~ {3o} , where (3o is chosen such that p({3o)\\n\\n= Po. The Fisher information a\\n\\nneuron k carries is small unless x E RF~~. This has the consequence that a fixed stimulus\\nx is actually encoded only by a subpopulation of neurons. The point x in stimulus space is\\ncovered by\\n27r D/ 2({30)D D _\\n(10)\\nNcode:= 1] Dr(D/2)\\n(Jj\\n\\n}1\\n\\nreceptive fields. With the help of (10), the average population Fisher information (8) can\\nbe re-written as\\n(11)\\n\\nEquation (11) can be interpreted as follows: We assume that the population of neurons\\nencodes stimulus dimension i accurately, while all other dimensions are of secondary importance. The average population Fisher information for dimension i, (Jii ) u, is determined\\nby the tuning width in dimension i, (ji, and by the size of the active subpopulation, N code '\\nThere is a tradeoff between these quantities. On the one hand, the encoding error can be\\ndecreased by decreasing (ji, which enhances the Fisher information carried by each single\\n\\n\\fNeural Representation ofMulti-Dimensional Stimuli\\n\\n119\\n\\nneuron. Decreasing ai, on the other hand, will also shrink the active subpopulation via\\n(10). This impairs the encoding accuracy, because the stimulus position is evaluated from\\nthe activity of fewer neurons. If (11) is valid due to a sufficient receptive field overlap,\\nNcode can be increased by increasing the tuning widths, aj, in all other dimensions j i- i.\\nThis effect is illustrated in Fig. 2 for D = 2.\\n\\nX2\\nc=:>\\n\\nx2, s\\n\\nX2\\n,II\\\"\\\\..\\\\\\n\\nU\\nx2,s\\n\\nFigure 2: Encoding strategy for a stimulus characterized by parameters Xl,s and X2,s' Feature Xl is to be encoded accurately. Effective receptive field shapes are indicated for both\\npopulations. If neurons are narrowly tuned in X2 (left), the active population (solid) is\\nsmall (here: Ncode = 3). Broadly tuned receptive fields for X2 (right) yield a much larger\\npopulation (here: Ncode = 27) thus increasing the encoding accuracy.\\nIt shall be noted that although a narrow tuning width ai is advantageous, the limit ai ---t 0\\nyields a bad representation. For narrowly tuned cells, gaps appear between the receptive\\nfields: The condition 17(X) == const. breaks down, and (6) is no longer valid. A more\\ndetailed calculation shows that the encoding error diverges as ai --* 0 [4]. The fact that\\nthe encoding error decreases for both narrow tuning and broad tuning - due to (11) - proves\\nthe existence of an optimal tuning width, An example is given in Fig. 3a.\\n3\\n\\nrTI~--~------~----~------~\\n\\n1\\\\\\n\\n(b)\\n\\nIi\\n\\n1\\\\\\n\\nIi\\n\\n0.8\\n\\nII\\nII\\nI;\\n\\n2\\n\\n1\\\\\\n\\nI ,\\n\\n;to.6\\n~\\n\\n~~~~;::~-:.~~;:\\n\\nA\\n\\nN~O.4\\nw\\n\\n----- ---- ----- -- ---\\n\\nv\\n\\n0.2\\n\\nO'----~--~--~-----'-------'\\n\\no\\n\\n0.5\\n\\n1\\nA\\n\\n1.5\\n\\n2\\n\\nFigure 3: (a) Example for the encoding behavior with narrow tuning curves arranged on\\na regular lattice of dimension D = 1 (grid spacing ~). Tuning curves are Gaussian, and\\nneural firing is modeled as a Poisson process, Dots indicate the minimal square encoding\\nerror averaged over a uniform distribution of stimuli, (E~in)' as a function ofa. The minimum is clearly visible. The dotted line shows the corresponding approximation according\\nto (8). The inset shows Gaussian tuning curves of optimal width, ao pt ~ 0.4~. (b) 9D()..)\\nas a function of ).. for different values of D.\\n\\n\\fc. W.\\n\\n120\\n\\nEurich, S. D. Wilke and H. Schwegler\\n\\nNarrow distribution of tuning curves. In order to study the effects of encoding the\\nstimulus with distributed tuning widths instead of identical tuning widths as in the previous\\ncases, we now consider the distribution\\n\\ng:i e\\nD\\n\\nPu(lT1,'\\\" ,lTD)\\n\\n=\\n\\n[lTi - (O'i -\\n\\ni)] e [(O'i + i) -lTi] ,\\n\\n(12)\\n\\ne\\n\\ndenotes the Heaviside step function. Equation (12) describes a uniform distriwhere\\nbution in a D-dimensional cuboid of size b1 , ... , bD around (0'1, .. . 0'D); cf. Fig. 1c. A\\nstraightforward calculation shows that in this case, the average population Fisher information (6) for i = j becomes\\n\\n(Jii)u\\n\\n= f/DKtj) (F, T, D) n~l\\nO'~ 0'1\\n\\n{\\n\\n1\\n1 + 12\\n\\n(bO'i 2+ 0 [( O'ib 4] }.\\ni )\\n\\ni )\\n\\n(13)\\n\\nA comparison with (8) yields the astonishing result that an increase in bi results in an\\nincrease in the i-th diagonal element of the average population Fisher information matrix\\nand thus in an improvement in the encoding of the i-th stimulus feature, while the encoding\\nin dimensions j :f. i is not affected. Correspondingly, the total encoding error can be\\ndecreased by increasing an arbitrary number of edge lengths of the cube. The encoding by\\na population with a variability in the tuning curve geometries as described is more precise\\nthan that by a uniform population. This is true/or arbitrary D. Zhang and Sejnowski [13]\\nconsider the more artificial situation of a correlated variability ofthe tuning widths: tuning\\ncurves are always assumed to be radially symmetric. This is indicated by the diagonal\\nline in Fig. 1a. A distribution of tuning widths restricted to this subset yields an average\\npopulation Fisher information ex: (O'D-2) and does not improve the encoding for D = 2 or\\n\\nD=3.\\nFragmentation into D subpopulations. Finally, we study a family of distributions of\\ntuning widths which also yields a lower minimal encoding error than the uniform population. Let the density of tuning curves be given by\\n1 D\\n\\nPu(lT1,'\\\" ,lTD) = D\\n\\nL 6( lTi i=l\\n\\nAO')\\n\\nII 6(lTj - 0'),\\n\\n(14)\\n\\nj?-i\\n\\nwhere A > O. For A = 1, the population is uniform as in (7). For A :f. 1, the population\\nis split up into D subpopulations; in subpopulation i, lTi is modified while lTj == 0' for\\nj :f. i. See Fig. Id for an example. The diagonal elements ofthe average population Fisher\\ninformation are\\n\\n(Jii)u\\n\\n{1 + (D = f/DKtj)(F, T, D) -D-2\\nIT\\nDA\\n\\nI)A 2 }\\n\\n'\\n\\n(15)\\n\\nwhere the term in brackets will be abbreviated as 9D(A). (Jii)u does not depend on i in\\nthis case because of the symmetry in the sUbpopulations. Equation (15) and the uniform\\ncase (7) differ by 9D(A) which will now be discussed. Figure 3b shows 9D(A) for different\\nvalues of D. For A = 1, 9D(A) = 1 and (7) is recovered as expected. 9D(A) = 1\\nalso holds for A = 1/ (D - 1) < 1: narrowing one tuning width in each subpopulation\\nwill at first decrease the resolution provided D 2: 3; this is due to the fact that Ncode is\\ndecreased. For A < 1/(D - 1), however, 9D(A) > 1, and the resolution exceeds (Jii)u in\\n(7) because each neuron in the i-th subpopulation carries a high Fisher information in the\\ni-th dimension. D = 2 is a special case where no impairment of encoding occurs because\\nthe effect of a decrease of Ncode is less pronounced. Interestingly, an increase in A also\\nyields an improvement in the encoding accuracy. This is a combined effect resulting from\\nan increase in Ncode on the one hand and the existence of D subpopulations, D - 1 of\\n\\n\\fNeural Representation of Multi-Dimensional Stimuli\\n\\n121\\n\\nwhich maintain their tuning widths in each dimension on the other hand. The discussion\\nof 9D(>\\\") leads to the following encoding strategy. For small >.., (Jii)u increases rapidly,\\nwhich suggests a fragmentation of the population into D subpopulations each of which\\nencodes one feature with high accuracy, i.e., one tuning width in each subpopulation is\\nsmall whereas the remaining tuning widths are broad. Like in the case discussed above, the\\ntheoretical limit of this method is a breakdown of the approximation of TJ == const. and the\\nvalidity of (6) due to insufficient receptive field overlap.\\n\\n4 Discussion and Outlook\\nWe have discussed the effects of a variation of the tuning widths on the encoding accuracy\\nobtained by a population of stochastically spiking neurons. The question of an optimal\\ntuning strategy has turned out to be more complicated than previously assumed. More\\nspecifically, the case which focused most attention in the literature - radially symmetric\\nreceptive fields [5, 1,9, 3, 13] - yields a worse encoding accuracy than most other cases we\\nhave studied: uniform populations with tuning curves which are not radially symmetric;\\ndistributions of tuning curves around some symmetric or non-symmetric tuning curve; and\\nthe fragmentation of the population into D subpopulations each of which is specialized in\\none stimulus feature.\\nIn a next step, the theoretical results will be compared to empirical data on encoding properties of neural popUlations. One aspect is the existence of sensory maps which consist\\nof neural subpopulations with characteristic tuning properties for the features which are\\nrepresented. For example, receptive fields of auditory neurons in the midbrain of the barn\\nowl have elongated shapes [6]. A second aspect concerns the short-term dynamics of receptive fields. Using single-unit recordings in anaesthetized cats, Worgotter et al. [12]\\nobserved changes in receptive field size taking place in 50-lOOms. Our findings suggest\\nthat these dynamics alter the resolution obtained for the corresponding stimulus features.\\nThe observed effect may therefore realize a mechanism of an adaptable selective signal\\nprocessing.\\n\\nReferences\\n[1] Baldi, P. & HeiJigenberg, W. (1988) BioI. Cybern. 59:313-318.\\n[2] Deco, G. & Obradovic, D. (1997) An Information-Theoretic Approach to Neural Computing.\\nNew York: Springer.\\n[3] Eurich, C. W. & Schwegler, H. (1997) BioI. Cybern. 76: 357-363.\\n\\n[4] Eurich, C. W. & Wilke, S. D. (2000) NeuraL Compo (in press).\\n[5] Hinton, G. E., McClelland, J. L. & Rumelhart, D. E (1986) In Rumelhart, D. E. & McClelland,\\nJ. L. (eds.), ParaLLeL Distributed Processing, Vol. 1, pp. 77-109. Cambridge MA: MIT Press.\\n[6] Knudsen, E. I. & Konishi, M. (1978) Science 200:795-797.\\n[7] Kuffter, S. W. (1953) 1. Neurophysiol. 16:37-68.\\n[8] Lettvin, J. Y., Maturana, H. R., McCulloch, W. S. & Pitts, W. H. (1959) Proc. Inst. Radio Eng.\\nNY 47:1940-1951.\\n[9] Snippe, H. P. & Koenderink, J. J. (1992) BioI. Cybern. 66:543-551.\\n[10] Wiggers, W., Roth, G., Eurich, C. W. & Straub, A. (1995) J. Camp. Physiol. A 176:365-377.\\n[11] Wilke, S. D. & Eurich, C. W. (1999) In Verleysen, M. (ed.), ESANN 99, European Symposium\\non Artificial Neural Networks, pp. 435-440. Brussels: D-Facto.\\n[12] Worgotter, F., Suder, K., Zhao, Y., Kerscher, N., Eysel, U. T. & Funke, K. (1998) Nature\\n396:165-168.\\n[13] Zhang, K. & Sejnowski, T. J. (1999) NeuraL Compo 11:75-84.\\n\\n\\f\",\n          \"Searching for Character Models\\n\\nJaety Edwards\\nDepartment of Computer Science\\nUC Berkeley\\nBerkeley, CA 94720\\njaety@cs.berkeley.edu\\n\\nDavid Forsyth\\nDepartment of Computer Science\\nUC Berkeley\\nBerkeley, CA 94720\\ndaf@cs.berkeley.edu\\n\\nAbstract\\nWe introduce a method to automatically improve character models for a\\nhandwritten script without the use of transcriptions and using a minimum\\nof document specific training data. We show that we can use searches for\\nthe words in a dictionary to identify portions of the document whose\\ntranscriptions are unambiguous. Using templates extracted from those\\nregions, we retrain our character prediction model to drastically improve\\nour search retrieval performance for words in the document.\\n\\n1 Introduction\\nAn active area of research in machine transcription of handwritten documents is reducing\\nthe amount and expense of supervised data required to train prediction models. Traditional\\nOCR techniques require a large sample of hand segmented letter glyphs for training. This\\nper character segmentation is expensive and often impractical to acquire, particularly if the\\ncorpora in question contain documents in many different scripts.\\nNumerous authors have presented methods for reducing the expense of training data by\\nremoving the need to segment individual characters. Both Kopec et al [3] and LeCun et al\\n[5] have presented models that take as input images of lines of text with their ASCII transcriptions. Training with these datasets is made possible by explicitly modelling possible\\nsegmentations in addition to having a model for character templates.\\nIn their research on ?wordspotting?, Lavrenko et al [4] demonstrate that images of entire\\nwords can be highly discriminative, even when the individual characters composing the\\nword are locally ambiguous. This implies that images of many sufficiently long words\\nshould have unambiguous transcriptions, even when the character models are poorly tuned.\\nIn our previous work, [2], the discriminatory power of whole words allowed us to achieve\\nstrong search results with a model trained on a single example per character.\\nThe above results have shown that A) one can learn new template models given images of\\ntext lines and their associated transcriptions, [3, 5] without needing an explicit segmentation\\nand that B) entire words can often be identified unambiguously, even when the models for\\nindividual characters are poorly tuned. [2, 4]. The first of these two points implies that\\ngiven a transcription, we can learn new character models. The second implies that for at\\nleast some parts of a document, we should be able to provide that transcription ?for free?,\\nby matching against a dictionary of known words.\\n\\n\\fs1\\n\\ns2\\n\\ns3\\n\\ns4\\n\\ns5\\n\\ns6\\n\\ns7\\n\\ns8\\n\\n?d\\n\\ndi\\n\\nix\\n\\nxe\\n\\ner\\n\\nri\\n\\nis\\n\\ns?\\n\\nFigure 1: A line, and the states that generate it. Each state st is defined by its left and\\nright characters ctl and ctr (eg ?x? and ?e? for s4 ). In the image, a state spans half of each\\nof these two characters, starting just past the center of the left character and extending to\\nthe center of the right character, i.e. the right half of the ?x? and the left half of the ?e?\\nin s4 . The relative positions of the two characters is given by a displacement vector dt\\n(superimposed on the image as white lines). Associating states with intracharacter spaces\\ninstead of with individual characters allows for the bounding boxes of characters to overlap\\nwhile maintaining the independence properties of the Markov chain.\\nIn this work we combine these two observations in order to improve character models\\nwithout the need for a document specific transcription. We provide a generic dictionary of\\nwords in the target language. We then identify ?high confidence? regions of a document.\\nThese are image regions for which exactly one word from our dictionary scores highly\\nunder our model. Given a set of high confidence regions, we effectively have a training\\ncorpus of text images with associated transcriptions. In these regions, we infer a segmentation and extract new character examples. Finally, we use these new exemplars to learn\\nan improved character prediction model. As in [2], our document in this work is a 12th\\ncentury manuscript of Terence?s Comedies obtained from Oxford?s Bodleian library [1].\\n\\n2 The Model\\nHidden Markov Models are a natural and widely used method for modeling images of text.\\nIn their simplest incarnation, a hidden state represents a character and the evidence variable\\nis some feature vector calculated at points along the line. If all characters were known to\\nbe of a single fixed width, this model would suffice. The probability of a line under this\\nmodel is given as\\nY\\np(line) = p(c1 |?)\\np(ct |ct?1 )p(im[w?(t?1):w?t]|ct )\\n(1)\\nt>1\\n\\nwhere ct represents the tth character on the line, ? represents the start state, w is the width\\nof a character, and im[w(t?1)+1:wt] represents the column of pixels beginning at column\\nw ? (t ? 1) + 1 of the image and ending at column w ? t, (i.e. the set of pixels spanned by\\nc)\\nUnfortunately, character?s widths do vary quite substantially and so we must extend the\\nmodel to accommodate different possible segmentations. A generalized HMM allows us to\\ndo this. In this model a hidden state is allowed to emit a variable length series of evidence\\nvariables. We introduce an explicit distribution over the possible widths of a character.\\nLetting dt be the displacement vector associated with the tth character, and ctx refer to the\\nx location of the left edge of a character on the line, the probability of a line under this\\nrevised model is\\nY\\np(line) = p(c1 |?)\\np(ct |ct?1 )p(dt |ct )p(im[ctx +1:ctx +d] |dt , ct )\\n(2)\\nt>1\\n\\nThis is the model we used in [2]. It performs far better than using an assumption of fixed\\nwidths, but it still imposes unrealistic constraints on the relative positions of characters. In\\n\\n\\fparticular, the portion of the ink generated by the current character is assumed to be independent of the preceding character. In other words, the model assumes that the bounding\\nboxes of characters do not overlap. This constraint is obviously unrealistic. Characters\\nroutinely overlap in our documents. ?f?s, for instance, form ligatures with most following characters. In previous work, we treated this overlap as noise, hurting our ability to\\ncorrectly localize templates. Under this model, local errors of alignment would also often propagate globally, adversely affecting the segmentation of the whole line. For search,\\nthis noisy segmentation still provides acceptable results. In this work, however, we need\\nto extract new templates, and thus correct localization and segmentation of templates is\\ncrucial.\\nIn our current work, we have relaxed this constraint, allowing characters to partially overlap. We achieve this by changing hidden states to represent character bigrams instead of\\nsingle characters (Figure 1). In the image, a state now spans the pixels from just past the\\ncenter of the left character to the pixel containing the center of the right character. We\\nadjust our notation somewhat to reflect this change, letting st now represent the tth hidden state and ctl and ctr be the left and right characters associated with s. dt is now the\\ndisplacement vector between the centers of ctl and ctr .\\nThe probability of a line under this, our actual, model is\\nY\\np(line) = p(s1 |?)\\np(st |st?1 )p(dt |ctl , ctr )p(im[stx +1:stx +dt ]|ctl , ctr , dt )\\n\\n(3)\\n\\nt>1\\n\\nThis model allows overlap of bounding boxes, but it does still make the assumption that\\nthe bounding box of the current character does not extend past the center of the previous\\ncharacter. This assumption does not fully reflect reality either. In Figure 1, for example,\\nthe left descender of the x extends back further than the center of the preceding character.\\nIt does, however, accurately reflect the constraints within the heart of the line (excluding\\nascenders and descenders). In practice, it has proven to generate very accurate segmentations. Moreover, the errors we do encounter no longer tend to affect the entire line, since\\nthe model has more flexibility with which to readjust back to the correct segmentation.\\n2.1 Model Parameters\\nOur transition distribution between states is simply a 3-gram character model. We train this\\nmodel using a collection of ASCII Latin documents collected from the web. This set does\\nnot include the transcriptions of our documents.\\nConditioned on displacement vector, the emission model for generating an image chunk\\ngiven a state is a mixture of gaussians. We associate with each character a set of image\\nwindows extracted from various locations in the document. We initialize these sets with\\none example a piece from our hand cut set (Figure 2). We adjust the probability of an image\\ngiven the state to include the distribution over blocks by expanding the last term of Equation\\n3 to reflect this mixture. Letting bck represent the k th exemplar in the set associated with\\ncharacter c, the conditional probability of an image region spanning the columns from x to\\nx? is given as\\nX\\np(imx:x? |ctl , ctr , dt ) =\\np(imx:x? |bctl i , bctr j , dt )\\n(4)\\ni,j\\n\\nIn principle, the displacement vectors should now be associated with an individual block,\\nnot a character. This is especially true when we have both upper and lower case letters.\\nHowever, our model does not seem particularly sensitive to this displacement distribution\\nand so in practice, we have a single, fairly loose, displacement distribution per character.\\nGiven a displacement vector, we can generate the maximum likelihood template image\\nunder our model by compositing the correct halves of the left and right blocks. Reshaping\\n\\n\\fthe image window into a vector, the likelihood of an image window is then modeled as\\na gaussian, using the corresponding pixels in the template as the means, and assuming\\na diagonal covariance matrix. The covariance matrix largely serves to mask out empty\\nregions of a character?s bounding box, so that we do not pay a penalty when the overlap of\\ntwo characters? bounding boxes contains only whitespace.\\n2.2 Efficiency Considerations\\nThe number of possible different templates for a state is O(|B| ? |B| ? |D|), where |B| is\\nthe number of different possible blocks and |D| is the number of candidate displacement\\nvectors. To make inference in this model computationally feasible, we first restrict the\\ndomain of d. For a given pair of blocks bl and br , we consider only displacement vectors\\nwithin some small x distance from a mean displacement mbl ,br , and we have a uniform\\ndistribution within this region. m is initialized from the known size of our single hand cut\\ntemplate. In the current work, we do not relearn the m. These are held fixed and assumed\\nto be the same for all blocks associated with the same letter.\\nEven when restricting the number of d?s under consideration as discussed above, it is computationally infeasible to consider every possible location and pair of blocks. We therefore\\nprune our candidate locations by looking at the likelihood of blocks in isolation and only\\nconsidering locations where there is a local optimum in the response function and whose\\nvalue is better than a given threshold. In this case our threshold for a given location is that\\nL(block) < .7L(background) (where L(x) represents the negative log likelihood of x).\\nIn other words, a location has to look at least marginally more like a given block than it\\nlooks like the background.\\nAfter pruning locations in this manner, we are left with a discrete set of ?sites,? where we\\ndefine a site as the tuple (block type, x location, y location). We can enumerate the set of\\npossible states by looking at every pair of sites whose displacement vector has a non-zero\\nprobability.\\n2.3 Inference In The Model\\nThe statespace defined above is a directed acyclic graph, anchored at the left edge and\\nright edges of a line of text. A path through this lattice defines both a transcription and\\na segmentation of the line into individual characters. Inference in this model is relatively\\nstraightforward because of our constraint that each character may overlap only one preceding and one following character, and our restriction of displacement vectors to a small\\ndiscrete range. The first restriction means that we need only consider binary relations between templates. The second preserves the independence relationships of an HMM. A\\ngiven state st is independent of the rest of the line given the values of all other states within\\ndmax of either edge of st (where dmax is the legal displacement vector with the longest\\nx component.) We can therefore easily calculate the best path or explicitly calculate the\\nposterior of a node by traversing the state graph in topological order, sorted from left to\\nright. The literature on Weighted Finite State Transducers ([6], [5]) is a good resource for\\nefficient algorithms on these types of statespace graph.\\n\\n3 Learning Better Character Templates\\nWe initialize our algorithm with a set of handcut templates, exactly 1 per character, (Figure\\n2), and our goal is to construct more accurate character models automatically from unsupervised data. As noted above, we can easily calculate the posterior of a given site under\\nour model. (Recall that a site is a particular character template at a given (x,y) location in\\nthe line.) The traditional EM approach to estimating new templates would be to use these\\n\\n\\fFigure 2: Original Training Data These 22 glyphs are our only document specific training\\ndata. We use the model based on these characters to extract the new examples shown below\\n\\nFigure 3: Examples of extracted templates We extract new templates from high confidence\\nregions. From these, we choose a subset to incorporate into the model as new exemplars.\\nTemplates are chosen iteratively to best cover the space of training examples. Notice that\\nfor ?q? and ?a?, we have extracted capital letters, of which there were no examples in\\nour original set of glyphs. This happens when the combination of constraints from the\\ndictionary the surrounding glyphs make a ?q? or ?a? the only possible explanation for\\nthis region, even though its local likelihood is poor.\\n\\nsites as training examples, weighted by their posteriors. Unfortunately, the constraints imposed by 3 and even 4-gram character models seem to be insufficient. The posteriors of\\nsites are not discriminative enough to get learning off the ground.\\nThe key to successfully learning new templates lies is the observation from our previous\\nwork [2], that even when the posteriors of individual characters are not discriminative, one\\ncan still achieve very good search results with the same model. The search word in effect\\nserves as its own language model, only allowing paths through the state graph that actually\\ncontain it, and the longer the word the more it constrains the model. Whole words impose\\nmuch tighter constraints than a 2 or 3-gram character model, and it is only with this added\\npower that we can successfully learn new character templates.\\nWe define the score for a search as the negative log likelihood of the best path containing\\nthat word. With sufficiently long words, it becomes increasingly unlikely that a spurious\\npath will achieve a high score. Moreover, if we are given a large dictionary of words and\\nno alternative word explains a region of ink nearly as well as the best scoring word, then\\nwe can be extremely confident that this is a true transcription of that piece of ink.\\nStarting with a weak character model, we do not expect to find many of these ?high confidence? regions, but with a large enough document, we should expect to find some. From\\nthese regions, we can extract new, reliable templates with which to improve our character\\nmodels. The most valuable of these new templates will be those that are significantly different from any in our current set. For example, in Figure 3, note that our system identifies\\ncapital Q?s, even though our only input template was lower case. It identifies this ink as\\na Q in much the same way that a person solves a crossword puzzle. We can easily infer\\nthe missing character in the string ?obv-ous? because the other letters constrain us to one\\npossible solution. Similarly, if other character templates in a word match well, then we can\\nunambiguously identify the other, more ambiguous ones. In our Latin case, ?Quid? is the\\nonly likely explanation for ?-uid?.\\n3.1 Extracting New Templates and Updating The Model\\nWithin a high confidence region we have both a transcription and a localization of template\\ncenters. It remains only to cut out new templates. We accomplish this by creating a template\\nimage for the column of pixels from the corresponding block templates and then assigning\\nimage pixels to the nearest template character (measured by Euclidean distance).\\nGiven a set of templates extracted from high confidence regions, we choose a subset of\\n\\n\\fScore Under Model\\n\\nworse\\n3400\\n3350\\n3300\\nbest\\nConfidence Margins\\n\\nFigure 4: Each line segment in the lower figure represents a proposed location for a word\\nfrom our dictionary. It?s vertical height is the score of that location under our model. A\\nlower score represents a better fit. The dotted line is the score of our model?s best possible\\npath. Three correct words, ?nec?, ?quin? and ?dari?, are actually on the best path. We\\ndefine the confidence margin of a location as the difference in score between the best\\nfitting word from our dictionary and the next best.\\n\\nFigure 5: Extracting Templates For a region with sufficiently high confidence margin, we\\nconstruct the maximum likelihood template from our current exemplars. left, and we assign\\npixels from the original image to a template based on its distance to the nearest pixel in\\nthe template image, extracting new glyph exemplars right. These new glyphs become the\\nexemplars for our next round of training.\\n\\ntemplates that best explain the remaining examples. We do this in a greedy fashion by\\nchoosing the example whose likelihood is lowest under our current model and adding it to\\nour set. Currently, we threshold the number of new templates for the sake of efficiency. Finally, given the new set of templates, we can add them to the model and rerun our searches,\\npotentially identifying new high confidence regions.\\n\\n4 Results\\nOur algorithm iteratively improves the character model by gathering new training data from\\nhigh confidence regions. Figure 3 shows that this method finds new templates significantly\\ndifferent from the originals. In this document, our set of examples after one round appears\\nto cover the space of character images well, at least those in lower case. Our templates are\\nnot perfect. The ?a?, for instance, has become associated with at least one block that is in\\nfact an ?o?. These mistakes are uncommon, particularly if we restrict ourselves to longer\\nwords. Those that do occur introduce a tolerable level noise into our model. They make\\ncertain regions of the document more ambiguous locally, but that local ambiguity can be\\novercome with the context provided by surrounding characters and a language model.\\nImproved Character Models We evaluate the method more quantitatively by testing the\\nimpact of the new templates on the quality of searches performed against the document.\\nTo search for a given word, we rank lines by the ratio of the maximum likelihood transcription/segmentation that contains the search word to the likelihood of the best possible\\nsegmentation/transcription under our model. The lowest possible search score is 1, happening when the search word is actually a substring of the maximum likelihood transcription.\\nHigher scores mean that the word is increasingly unlikely under our model. In Figure 7, the\\nfigure on the left shows the improvement in ranking of the lines that truly contain selected\\nsearch words. The odd rows (in red) are search results using only the original 22 glyphs,\\n\\n\\f20\\n40\\n60\\n80\\n100\\n\\n200\\n\\n300\\n\\n400\\n\\n500\\n\\n600\\n\\nRnd 2\\n\\nRnd 1\\n\\n2700\\n2650\\n2600\\ndotted (wrong):\\nsolid (correct):\\n1920\\n1900\\n1880\\n1860\\n1840\\ndotted (wrong):\\nsolid (correct):\\n\\niam\\n\\nnupta\\nnuptiis\\n\\ninquam\\n\\n(v|u)ideo\\nvidet\\n\\nnupta\\nnuptiis\\n\\npost inquam\\npostquam\\n\\n(v|u)ideo\\nvidet\\n\\nFigure 6: Search Results with (Rnd 1) initial templates only and with (Rnd 2) templates\\nextracted from high confidence regions. We show results that have a score within 5% of the\\nbest path. Solid Lines are the results for the correct word. Dotted lines represent other\\nsearch results, where we have made a few larger in order to show those words that are\\nthe closest competitors to the true word. Many alternative searches, like the highlighted\\n?post? are actually portions of the correct larger words. These restrict our selection of\\nconfidence regions, but do not impinge on search quality.\\nEach correct word has significantly improved after one round of template reestimation.\\n?iam? has been correctly identified, and is a new high confidence region. Both ?nuptiis?\\nand ?postquam? are now the highest likelihood words for their region barring smaller\\nsubsequences, and ?videt? has narrowed the gap between its competitor ?video?.\\nwhile the even rows (in green) use an additional 332 glyphs extracted from high confidence\\nregions. Search results are markedly improved in the second model. The word ?est?, for\\ninstance, only had 15 of 24 of the correct lines in the top 100 under the original model,\\nwhile under the learned model all 24 are not only present but also more highly ranked.\\nImproved Search Figure 6 shows the improved performance of our refitted model for\\na single line. Most words have greatly improved relative to their next best alternative.\\n?postquam? and ?iam? were not even considered by the original model and now are nearly\\noptimal. The right of Figure 7 shows the average precision/recall curve under each model\\nfor 21 words with more than 4 occurrences in the dataset. Precision is the percentage\\nof lines truly containing a word in the top n search results, and recall is the percentage\\nof all lines containing the word returned in the top n results. The learned model clearly\\ndominates. The new model also greatly improves performance for rare words. For 320\\nwords ocurring just once in the dataset, 50% are correctly returned as the top ranked result\\nunder the original model. Under the learned model, this number jumps to 78%.\\n\\n5 Conclusions and Future Work\\nIn most fonts, characters are quite ambiguous locally. An ?n? looks like a ?u?, looks like\\n?ii?, etc. This ambiguity is the major hurdle to the unsupervised learning of character\\ntemplates. Language models help, but the standard n-gram models provide insufficient\\nconstraints, giving posteriors for character sites too uninformative to get EM off the ground.\\n\\n\\fAggregate Precision/Recall Curve\\n\\nSelected Words, Top 100 Returned Lines\\n\\nPrecision\\n\\nest\\n(15,24)/24\\nnescio\\n( 1, 1)/ 1\\npostquam\\n( 0, 2)/ 2\\nquod\\n(14,14)/14\\nmoram\\n( 0, 2)/ 2\\nnon\\n( 8, 8)/ 8\\nquid\\n( 9, 9)/ 9\\n10 20 30 40 50 60 70 80 90100\\n\\n0.75\\n0.7\\n0.65\\n0.6\\n0.55\\n0.5\\n0.45\\n0.4\\n0.35\\n\\nOriginal Model\\nRefit Model\\n0.2\\n\\n0.4\\n0.6\\nRecall\\n\\n0.8\\n\\n1\\n\\nFigure 7: The figure on the left shows the those lines with the top 100 scores that actually\\ncontain the specified word. The first of each set of two rows (in red) is the results from\\nRound 1. The second (in green) is the results for Round 2. Almost all search words in our\\ncorpus show a significant improvement. The numbers to the right (x/y) mean that out of\\ny lines that actually contained the search word in our document, x of them made it into\\nthe top ten. On the right are average precision/recall curves for 21 high frequency words\\nunder the model with our original templates (Rnd 1) and after refitting with new extracted\\ntemplates (Rnd 2). Extracting new templates vastly improves our search quality\\nAn entire word is much different. Given a dictionary, we expect many word images to have\\na single likely transcription even if many characters are locally ambiguous. We show that\\nwe can identify these high confidence regions even with a poorly tuned character model. By\\nextracting new templates only from these regions of the document, we overcome the noise\\nproblem and significantly improve our character models. We demonstrate this improvement\\nfor the task of search where the refitted models have drastically better search responses than\\nwith the original. Our method is indifferent to the form of the actual character emission\\nmodel. There is a rich literature in character prediction from isolated image windows, and\\nwe expect that incorporating more powerful character models should provide even greater\\nreturns and help us in learning less regular scripts.\\nFinding high confidence regions to extract good training examples is a broadly applicable concept. We believe this work should extend to other problems, most notably speech\\nrecognition. Looked at more abstractly, our use of language model in this work is actually encoding spatial constraints. The probability of a character given an image window\\ndepends not only on the identify of surrounding characters but also on their spatial configuration. Integrating context into recognition problems is an area of intense research in\\nthe computer vision community, and we are investigating extending the idea of confidence\\nregions to more general object recognition problems.\\n\\nReferences\\n[1] Early Manuscripts at Oxford University. Bodleian library ms. auct. f. 2.13. http://image.ox.ac.uk/.\\n[2] J. Edwards, Y.W. Teh, D. Forsyth, R. Bock, M. Maire, and G. Vesom. Making latin manuscripts\\nsearchable using ghmm?s. In NIPS 17, pages 385?392. 2005.\\n[3] G. Kopec and M. Lomelin. Document-specific character template estimation. In Proceedings,\\nDocument Image Recognition III, SPIE, 1996.\\n[4] V. Lavrenko, T. Rath, and R. Manmatha. Holistic word recognition for handwritten historical\\ndocuments. In dial, pages 278?287, 2004.\\n[5] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document\\nrecognition. Proceedings of the IEEE, 86(11):2278?2324, 1998.\\n[6] M. Mohri, F. Pereira, and M. Riley. Weighted finite state transducers in speech recognition. ISCA\\nITRW Automatic Speech Recognition, pages 97?106, 2000.\\n\\n\\f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 152
        }
      ],
      "source": [
        "df.head()\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "jEEeuJpfsF3N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7105cff3-5302-4582-b0ea-f02c7db678cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7241, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "ly1OD3amsIWZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "outputId": "c88fb2eb-1b3f-4d68-b6fc-f6fbba3756de"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id  year                                              title  \\\n",
              "0        1  1987  Self-Organization of Associative Database and ...   \n",
              "1       10  1987  A Mean Field Theory of Layer IV of Visual Cort...   \n",
              "2      100  1988  Storing Covariance by the Associative Long-Ter...   \n",
              "3     1000  1994  Bayesian Query Construction for Neural Network...   \n",
              "4     1001  1994  Neural Network Ensembles, Cross Validation, an...   \n",
              "...    ...   ...                                                ...   \n",
              "4995  5522  2014                  Low-Rank Time-Frequency Synthesis   \n",
              "4996  5523  2014  A State-Space Model for Decoding Auditory Atte...   \n",
              "4997  5524  2014      Efficient Structured Matrix Rank Minimization   \n",
              "4998  5525  2014       Efficient Minimax Signal Detection on Graphs   \n",
              "4999  5526  2014  Signal Aggregate Constraints in Additive Facto...   \n",
              "\n",
              "     event_type                                           pdf_name  \\\n",
              "0           NaN  1-self-organization-of-associative-database-an...   \n",
              "1           NaN  10-a-mean-field-theory-of-layer-iv-of-visual-c...   \n",
              "2           NaN  100-storing-covariance-by-the-associative-long...   \n",
              "3           NaN  1000-bayesian-query-construction-for-neural-ne...   \n",
              "4           NaN  1001-neural-network-ensembles-cross-validation...   \n",
              "...         ...                                                ...   \n",
              "4995     Poster         5522-low-rank-time-frequency-synthesis.pdf   \n",
              "4996     Poster  5523-a-state-space-model-for-decoding-auditory...   \n",
              "4997     Poster  5524-efficient-structured-matrix-rank-minimiza...   \n",
              "4998     Poster  5525-efficient-minimax-signal-detection-on-gra...   \n",
              "4999     Poster  5526-signal-aggregate-constraints-in-additive-...   \n",
              "\n",
              "                                               abstract  \\\n",
              "0                                      Abstract Missing   \n",
              "1                                      Abstract Missing   \n",
              "2                                      Abstract Missing   \n",
              "3                                      Abstract Missing   \n",
              "4                                      Abstract Missing   \n",
              "...                                                 ...   \n",
              "4995  Many single-channel signal decomposition techn...   \n",
              "4996  Humans are able to segregate auditory objects ...   \n",
              "4997  We study the problem of finding structured low...   \n",
              "4998  Several problems such as network intrusion, co...   \n",
              "4999  Blind source separation problems are difficult...   \n",
              "\n",
              "                                             paper_text  \n",
              "0     767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
              "1     683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
              "2     394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
              "3     Bayesian Query Construction for Neural\\nNetwor...  \n",
              "4     Neural Network Ensembles, Cross\\nValidation, a...  \n",
              "...                                                 ...  \n",
              "4995  Low-Rank Time-Frequency Synthesis\\n\\nMatthieu ...  \n",
              "4996  A State-Space Model for Decoding Auditory\\nAtt...  \n",
              "4997  Efficient Structured Matrix Rank Minimization\\...  \n",
              "4998  Ef?cient Minimax Signal Detection on Graphs\\n\\...  \n",
              "4999  Signal Aggregate Constraints in Additive Facto...  \n",
              "\n",
              "[5000 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e9311679-ed30-438c-bb09-a6dc292f5a5a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1987</td>\n",
              "      <td>Self-Organization of Associative Database and ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1-self-organization-of-associative-database-an...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>1987</td>\n",
              "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>1988</td>\n",
              "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000</td>\n",
              "      <td>1994</td>\n",
              "      <td>Bayesian Query Construction for Neural Network...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1001</td>\n",
              "      <td>1994</td>\n",
              "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4995</th>\n",
              "      <td>5522</td>\n",
              "      <td>2014</td>\n",
              "      <td>Low-Rank Time-Frequency Synthesis</td>\n",
              "      <td>Poster</td>\n",
              "      <td>5522-low-rank-time-frequency-synthesis.pdf</td>\n",
              "      <td>Many single-channel signal decomposition techn...</td>\n",
              "      <td>Low-Rank Time-Frequency Synthesis\\n\\nMatthieu ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4996</th>\n",
              "      <td>5523</td>\n",
              "      <td>2014</td>\n",
              "      <td>A State-Space Model for Decoding Auditory Atte...</td>\n",
              "      <td>Poster</td>\n",
              "      <td>5523-a-state-space-model-for-decoding-auditory...</td>\n",
              "      <td>Humans are able to segregate auditory objects ...</td>\n",
              "      <td>A State-Space Model for Decoding Auditory\\nAtt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4997</th>\n",
              "      <td>5524</td>\n",
              "      <td>2014</td>\n",
              "      <td>Efficient Structured Matrix Rank Minimization</td>\n",
              "      <td>Poster</td>\n",
              "      <td>5524-efficient-structured-matrix-rank-minimiza...</td>\n",
              "      <td>We study the problem of finding structured low...</td>\n",
              "      <td>Efficient Structured Matrix Rank Minimization\\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4998</th>\n",
              "      <td>5525</td>\n",
              "      <td>2014</td>\n",
              "      <td>Efficient Minimax Signal Detection on Graphs</td>\n",
              "      <td>Poster</td>\n",
              "      <td>5525-efficient-minimax-signal-detection-on-gra...</td>\n",
              "      <td>Several problems such as network intrusion, co...</td>\n",
              "      <td>Ef?cient Minimax Signal Detection on Graphs\\n\\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4999</th>\n",
              "      <td>5526</td>\n",
              "      <td>2014</td>\n",
              "      <td>Signal Aggregate Constraints in Additive Facto...</td>\n",
              "      <td>Poster</td>\n",
              "      <td>5526-signal-aggregate-constraints-in-additive-...</td>\n",
              "      <td>Blind source separation problems are difficult...</td>\n",
              "      <td>Signal Aggregate Constraints in Additive Facto...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5000 rows × 7 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e9311679-ed30-438c-bb09-a6dc292f5a5a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e9311679-ed30-438c-bb09-a6dc292f5a5a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e9311679-ed30-438c-bb09-a6dc292f5a5a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5a6cbc0f-6f25-4518-98c4-d50149b829e5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5a6cbc0f-6f25-4518-98c4-d50149b829e5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5a6cbc0f-6f25-4518-98c4-d50149b829e5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_dfd9372c-5a9a-4da6-bfb3-70026f0fcdec\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_dfd9372c-5a9a-4da6-bfb3-70026f0fcdec button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5000,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1522,\n        \"min\": 1,\n        \"max\": 5526,\n        \"num_unique_values\": 5000,\n        \"samples\": [\n          2365,\n          3345,\n          3405\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7,\n        \"min\": 1987,\n        \"max\": 2014,\n        \"num_unique_values\": 26,\n        \"samples\": [\n          2000,\n          2006,\n          1987\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5000,\n        \"samples\": [\n          \"Large Scale Online Learning\",\n          \"The Value of Labeled and Unlabeled Examples when the Model is Imperfect\",\n          \"Hierarchical Semi-Markov Conditional Random Fields for Recursive Sequential Data\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"event_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Oral\",\n          \"Spotlight\",\n          \"Poster\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pdf_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5000,\n        \"samples\": [\n          \"2365-large-scale-online-learning.pdf\",\n          \"3345-the-value-of-labeled-and-unlabeled-examples-when-the-model-is-imperfect.pdf\",\n          \"3405-hierarchical-semi-markov-conditional-random-fields-for-recursive-sequential-data.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2167,\n        \"samples\": [\n          \"In this paper we seek to detect rectangular cuboids and localize their corners in uncalibrated single-view images depicting everyday scenes. In contrast to recent approaches that rely on detecting vanishing points of the scene and grouping line segments to form cuboids, we build a discriminative parts-based detector that models the appearance of the cuboid corners and internal edges while enforcing consistency to a 3D cuboid model. Our model is invariant to the different 3D viewpoints and aspect ratios and is able to detect cuboids across many different object categories. We introduce a database of images with cuboid annotations that spans a variety of indoor and outdoor scenes and show qualitative and quantitative results on our collected database. Our model out-performs baseline detectors that use 2D constraints alone on the task of localizing cuboid corners.\",\n          \"We introduce a new objective function for pool-based Bayesian active learning with probabilistic hypotheses. This objective function, called the policy Gibbs error, is the expected error rate of a random classifier drawn from the prior distribution on the examples adaptively selected by the active learning policy. Exact maximization of the policy Gibbs error is hard, so we propose a greedy strategy that maximizes the Gibbs error at each iteration, where the Gibbs error on an instance is the expected error of a random classifier selected from the posterior label distribution on that instance. We apply this maximum Gibbs error criterion to three active learning scenarios: non-adaptive, adaptive, and batch active learning. In each scenario, we prove that the criterion achieves near-maximal policy Gibbs error when constrained to a fixed budget. For practical implementations, we provide approximations to the maximum Gibbs error criterion for Bayesian conditional random fields and transductive Naive Bayes. Our experimental results on a named entity recognition task and a text classification task show that the maximum Gibbs error criterion is an effective active learning criterion for noisy models.\",\n          \"We present a dynamic nonlinear generative model for visual motion based on a latent representation of binary-gated Gaussian variables. Trained on sequences of images, the model learns to represent different movement directions in different variables. We use an online approximate-inference scheme that can be mapped to the dynamics of networks of neurons. Probed with drifting grating stimuli and moving bars of light, neurons in the model show patterns of responses analogous to those of direction-selective simple cells in primary visual cortex. Most model neurons also show speed tuning and respond equally well to a range of motion directions and speeds aligned to the constraint line of their respective preferred speed. We show how these computations are enabled by a specific pattern of recurrent connections learned by the model.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4998,\n        \"samples\": [\n          \"Visual gesture-based robot guidance\\nwith a modular neural system\\n\\nE. Littmann,\\n\\nA. Drees, and H. Ritter\\n\\nAbt. Neuroinformatik, Fak. f. Informatik\\nUniversitat Ulm, D-89069 Ulm, FRG\\nenno@neuro.informatik.uni-ulm.de\\n\\nAG Neuroinformatik, Techn. Fakultat\\nUniv. Bielefeld, D-33615 Bielefeld, FRG\\nandrea,helge@techfak.uni-bielefeld.de\\n\\nAbstract\\nWe report on the development of the modular neural system \\\"SEEEAGLE\\\" for the visual guidance of robot pick-and-place actions.\\nSeveral neural networks are integrated to a single system that visually recognizes human hand pointing gestures from stereo pairs\\nof color video images. The output of the hand recognition stage is\\nprocessed by a set of color-sensitive neural networks to determine\\nthe cartesian location of the target object that is referenced by the\\npointing gesture. Finally, this information is used to guide a robot\\nto grab the target object and put it at another location that can\\nbe specified by a second pointing gesture. The accuracy of the current system allows to identify the location of the referenced target\\nobject to an accuracy of 1 cm in a workspace area of 50x50 cm. In\\nour current environment, this is sufficient to pick and place arbitrarily positioned target objects within the workspace. The system\\nconsists of neural networks that perform the tasks of image segmentation, estimation of hand location, estimation of 3D-pointing\\ndirection, object recognition, and necessary coordinate transforms.\\nDrawing heavily on the use of learning algorithms, the functions of\\nall network modules were created from data examples only.\\n\\n1\\n\\nIntroduction\\n\\nThe rapidly developing technology in the fields of robotics and virtual reality requires the development of new and more powerful interfaces for configuration and\\ncontrol of such devices. These interfaces should be intuitive for the human advisor\\nand comfortable to use. Practical solutions so far require the human to wear a\\ndevice that can transfer the necessary information. One typical example is the data\\nglove [14, 12]. Clearly, in the long run solutions that are contactless will be much\\nmore desirable, and vision is one of the major modalities that appears especially\\nsuited for the realization of such solutions.\\nIn the present paper, we focus on a still restricted but very important task in robot\\ncontrol, the guidance of robot pick-and-place actions by unconstrained human pointing gestures in a realistic laboratory environment. The input of target locations by\\n\\n\\f904\\n\\nE. LITTMANN, A. DREES, H. RITTER\\n\\npointing gestures provides a powerful, very intuitive and comfortable functionality\\nfor a vision-based man-machine interface for guiding robots and extends previous\\nwork that focused on the detection of hand location or the discrimination of a small,\\ndiscrete number of hand gestures only [10, 1, 2, 8]. Besides two color cameras, no\\nspecial device is necessary to evaluate the gesture of the human operator.\\nA second goal of our approach is to investigate how to build a neural system for\\nsuch a complex task from several neural modules. The development of advanced\\nartificial neural systems challenges us with the task of finding architect.ures for the\\ncooperat.ion of multiple functional modules such that. part of the structure of the\\noverall system can be designed at a useful level of abstraction, but at the same t.ime\\nlearning can be used to create or fine-tune the functionality of parts of t.he system\\non the basis of suit.able training examples.\\nTo approach this goal requires to shift the focus from exploring t.he properties of\\nsingle networks to exploring the propert.ies of entire systems of neural networks.\\nThe work on \\\"mixtures of experts\\\" [3, 4] is one important contribution along these\\nlines. While this is a widely applicable and powerful approach, there clearly is\\na need to go beyond the exploration of strictly hierarchical systems and to gain\\nexperience with architectures t.hat admit more complex types of information flow\\nas required e.g. by the inclusion of feat.ures such as control of focal attention or\\nreent.rant processing branches. The need for such features arose very naturally in\\nthe context of the task described above, and in the following sect.ion we will report\\nour results wit.h a system architecture that is crucially based on the exploitation of\\nsuch elements.\\n\\n2\\n\\nSystem architecture\\n\\nOur system, described in fig. 1, is situated in a complex laboratory environment. A\\nrobot arm with manipulator is mounted at one side of a table with several objects\\nof different color placed on it. A human operator is positioned at the next side to\\nthe right of the robot. This scenery is watched by two cameras from the other two\\nsides from high above. The cameras yield a stereo color image of t.he scene (images\\n10). The operator points with one hand at one of the objects on the table. On the\\nbasis of the image information, the object is located and the robot grabs it. Then,\\nthe operator points at another location, where the robot releases the object. 1\\nThe syst.em consists of several hardware components: a PUMA 560 robot arm with\\nsix axes and a three-fingered manipulator 2; two single-chip PULNIX color cameras;\\ntwo ANDRox vision boards with software for data acquisition and processing; a\\nwork space consisting of a table with a black grid on a yellow surface. Robot and\\nperson refer to the same work space. Bot.h cameras must show both the human\\nhand and the table with the objects. Within this constraint, the position of the\\ncameras can be chosen freely as long as they yield significantly different views.\\nAn important prerequisite for the recognition of the pointing direction is the segmentation of the human hand from the background scenery. This task is solved by\\na LLM network (Sl) trained to yield a probability value for each image pixel to\\nbelong to the hand region. The training is based on t.he local color information.\\nThis procedure has been investigated in [7].\\nAn important feature of the chosen method is the great reliability and robustness\\nof both the classification performance and the localization accuracy of the searched\\nobject. Furthermore, the performance is quite constant over a wide range of image\\nresolutions. This allows a fast two-step procedure: First, the images are segmented\\nin low resolution (Sl: 11 -+ A1) and the hand position is extracted. Then, a small\\n1 In analogy to the sea eagle who watches its prey from high above, shoots down to grab\\nthe prey, and then flies to a safe place to feed, we nicknamed our system \\\"SEE-EAGLE\\\".\\n2Development by Prof. Pfeiffer, TV Munich\\n\\n\\fVisual Gesture-based Robot Guidance with a Modular Neural System\\n\\n905\\n\\nFig. 1: System architecture. From two color camera images 10 we extract the hand position\\n(11 I> Sl I> A1 (pixel coord.) I> P1 I> cartesian hand coord.). In a subframe centered on\\nthe hand location (12) we determine the pointing direction (12 I> S2 I> A2 (pixel coord.) I>\\nG I> D I> pointing angles). Pointing direction and hand location define a cartesian target\\nlocation that is mapped to image coord. that define the centers of object subframes (10 I>\\nP2 I> 13). There we determine the target object (13 I> S3 I> A3) and map the pixel coord.\\nof its centers to world coord. (A3 I> P3 I> world target loc.). These coordinates are used\\nto guide the robot R to the target object.\\n\\n\\f906\\n\\nE. LITTMANN. A. DREES. H. RlTIER\\n\\nsubframe (12) around the estimated hand position is processed in high resolution\\nby another dedicated LLM network (S2: 12 - t A2). For details of the segmentation\\nprocess, refer to [6].\\nThe extraction of hand information by LLMs on the basis of Gabor masks has\\nalready been studied for hand posture [9] and orientation [5]. The method is based\\non a segmented image containing the hand only (A2). This image is filtered by 36\\nGabor masks that are arranged on a 3x3 grid with 4 directions per grid position\\nand centered on the hand. The filter kernels have a radius of 10 pixels, the distance\\nbetween the grid points is 20 pixels. The 36 filter responses (G) form the input\\nvector for a LLM network (D). Further details of the processing are reported in [6].\\nThe network yields the pointing direction of the hand (D: 12 - t G - t pointing\\ndirection). Together with the hand position which is computed by a parametrized\\nself-organizing map (\\\"PSOM\\\", see below and [11, 13]) (P1: Al - t cartesian hand\\nposition), a (cartesian) target location in the workspace can be calculated. This\\nlocation can be retransformed by the PSOM into pixel coordinates (P2: cartesian\\ntarget location - t target pixel coordinates). These coordinates define the center of\\nan \\\"attention region\\\" (13) that is searched for a set of predefined target objects.\\nThis object recognition is performed by a set of LLM color segmentation networks\\n(S3: 13 - t A3), each previously trained for one of the defined targets. A ranking\\nprocedure is used to determine the target object. The pixel coordinates ofthe target\\nin the segmented image are mapped by the PSOM to world coordinates (P3: A3 - t\\ncartesian target position). The robot R now moves to above these world coordinates,\\nmoves vertically down, grabs whatever is there, and moves upward again. Now, the\\nsystem evaluates a second pointing gesture that specifies the place where to place\\nthe object. This time, the world coordinates calculated on the basis of the pointing\\ndirection from network D and the cartesian hand location from PSOM PI serve\\ndirectly as target location for the robot.\\nFor our processing we must map corresponding pixels in the stereo images to cartesian world coordinates. For these transformations, training data was generated\\nwith aid of the robot on a precise sampling grid. We automatically extract the\\npixel coordinates of a LED at the tip of the robot manipulator from both images.\\nThe seven-dimensional feature vector serves as training input for an PSOM network [11]. By virtue of its capability to represent a transformation in a symmetric,\\n\\\"multiway\\\" -fashion, this offers the additional benefit that both the camera-to-world\\nmapping and its inverse can be obtained with a single network trained only once on\\na data set of 27 calibration positions of the robot. A detailed description for such\\na procedure can be found in [13].\\n\\n3\\n\\nResults\\n\\n3.1 System performance\\nThe accuracy of the current system allows to estimate the pointing target to an\\naccuracy of 1 ? 0.4 cm (average over N = 7 objects at randomly chosen locations\\nin the workspace) in a workspace area of 50x50 cm. In our current environment,\\nthis is sufficient to pick and place any of the seven defined target objects at any\\nlocation in the workspace. This accuracy can only be achieved if we use the object\\nrecognition module described in sec. 2. The output of the pointing direction module\\napproximates the target location with an considerably lower accuracy of 3.6? 1.6 cm.\\n3.2 Image segmentation\\nThe problem to evaluate these preprocessing steps has been discussed previously [7],\\nespecially the relation of specifity and sensitivity of the network for the given task.\\nAs the pointing recognition is based on a subframe centered on the hand center, it\\nis very sensitive to deviations from this center so that a good localization accuracy\\n\\n\\fVisual Gesture-based Robot Guidance with a Modular Neural System\\n\\n907\\n\\nis even more important than the classification rate. The localization accuracy is\\ncalculated by measuring the pixel distance between the centers determined manually on the original image and as the center of mass in the image obtained after\\napplication of the neural network. Table 1 provides quantitative results.\\nOn the whole) the two-step cascade of LLM networks yields for 399 out of 400 images\\nan activity image precisely centered on the human hand. Only in one image) the\\nfirst LLM net missed the hand completely) due to a second hand in the image that\\ncould be clearly seen in this view. This image was excluded from further processing\\nand from the evaluation of the localization accuracy.\\n\\nPerson A\\nPerson H\\n\\nCamera A\\nPixel deviatIOn\\nNRMSE\\n0.8 ? 1.2\\n0.03 ? 0.06\\n1.3 ? 1.4\\n0.06 ? 0.11\\n\\nCamera B\\nPixel deViatIOn\\nNRMSE\\n0.8 ? 2.2\\n0.03 ? 0.09\\n2.2 ? 2.8\\n0.11 ? 0.21\\n\\nTable 1: Estimation error of the hand localization on the test set. Absolute error in pixels\\nand normalized error for both persons and both camera images.\\n\\n3.3 Recognition performance\\nOne major problem in recognizing human pointing gestures is the variability of these\\ngestures and their measurement for the acquisition of reliable training information.\\nDifferent persons follow different strategies where and how to point (fig. 2 (center)\\nand (right?. Therefore) we calculate this information indirectly. The person is\\ntold to point at a certain grid position with known world coordinates. From the\\ncamera images we extract the pixel positions of the hand center and map them to\\nworld coordinates using the PSOM net (PI in fig . 1). Given these coordinates the\\nangles of the intended pointing vector with the basis vectors of the world coordinate\\nsystem can be calculated trigonometrically. These angles form the target vector for\\nthe supervised training of a LLM network (D in fig. 1).\\n\\nAfter training) the output of the net is used to calculate the point where the pointing\\nvector intersects the table surface. For evaluation of the network performance we\\nmeasure the Euclidian distance between this point and the actual grid point where\\nthe person intended to point at. Fig. 3 (left) shows the mean euclidean error MEE\\nof the estimated target position as a function of the number of learning steps. The\\nerror on the training set can be considerably reduced) whereas on the test set the\\nimprovement stagnates after some 500 training steps. If we perform even more\\ntraining steps the performance might actually suffer from overfitting. The graph\\ncompares training and test results achieved on images obtained by two different\\nways of determining the hand center. The \\\"manual\\\" curves show the performance\\nthat can be achieved if the Gabor masks are manually centered on the hand. For\\nthe \\\"neuronal)) curves) the center of mass calculated in the fine-segmented and postprocessed subframe was used. This allows us to study the influence of the error of\\nthe segmentation and localization steps on the pointing recognition. This influence\\nis rather small. The MEE increases from 17 mm for the optimal method to 19 mm\\nfor the neural method) which is hardly visible in practice.\\nThe curves in fig. 3 (center) are obtained if we apply the networks to images of\\nanother person. The MEE is considerably larger but a detailed analysis' shows\\nthat part of this deviation is due to systematic differences in the pointing strategy\\nas shown in fig. 2 (right). Over a wide range, the number of nodes used for the\\nLLM network has only minor influence on the performance. While obviously the\\nperformance on the training set can be arbitrarily improved by spending more nodes,\\nthe differences in the MEE on the test set are negligible in a range of 5 to 15 nodes.\\nUsing more nodes is problematic as the training data consists of 50 examples only.\\nIf not indicated otherwise) we use LLM networks with 10 nodes. Further results)\\n\\n\\f908\\n\\nE. LIITMANN. A. DREES. H. RIITER\\n\\nFig. 2: The table grid points can be reconstructed according to the network output. The\\ntarget grid is dotted . Reconstruction of training grid (left) and test grid (center) for one\\nperson, and of the test grid for another person (right).\\nMEB on test oet of unknown perron\\n\\nMER\\n30\\n\\n20\\n\\ne?\\n\\nI~\\n\\n10\\n\\n~\\n\\n---- ~--.---\\n\\n~\\n~-\\n\\n0\\n\\nn\\n\\nm..... aI,trainneuronal, train manual, test -\\n\\n:l~\\n\\n100\\n\\n250\\n\\nsao\\n\\n1000 2SOO SOOO\\n\\ntrain.., itHabonr\\n\\ne\\n?\\n\\n70\\n68\\n66\\n64\\n62\\n60\\n58\\n56\\n\\n4\\n\\n-~.\\n\\n100\\n\\n:l~\\n\\nsao\\n\\n1000\\n\\n2SOO SOOO\\n\\nFig. 3: The euclidean error of\\nestimated target point calculated using the network output depends on the preprocessing (left), and the person\\n(center).\\n\\ntrairq IteratioN\\n\\ncomparing the pointing recognition based on only one of the camera images, indicate\\nthat the method works better if the camera takes a lateral view rather than a frontal\\nview . All evaluations were done for both persons. The performance was always very\\nsimilar.\\n\\n4\\n\\nDiscussion\\n\\nWhile we begin to understand many properties of neural networks at the single\\nnetwork level, our insight into principled ways of how to build neural systems is\\nstill rather limited . Due to the complexity of this task, theoretical progress is\\n(and probably will continue to be) very slow. What we can do in the mean time,\\nhowever, is to experiment with different design strategies for neural systems and\\ntry to \\\"evolve\\\" useful approaches by carefully chosen case studies.\\nThe current work is an effort along these lines. It is focused on a challenging,\\npractically important vision task with a number of generic features that are shared\\nwith vision tasks for which biological vision systems were evolved.\\nOne important issue is how to achieve robustness at the different processing levels\\nof the system. There are only very limited possibilities to study this issue in simulations, since practically nothing is known about the statistical properties of the\\nvarious sources of error that occur when dealing with real world data. Thus, a real\\nimplementation that works with actual data is practically the only way to study\\nthe robustness issue in a realistic fashion. Therefore, the demonstrated integration\\nof several functional modules that we had developed previously in more restricted\\nsettings [7, 6] was a non-trivial test of the feasability of having these functions\\ncooperate in a larger, modular system. It also gives confidence that the scaling\\nproblem can be dealt with successfully if we apply modular neural nets.\\nA related and equally important issue was the use of a processing strategy in which\\nearlier processing stages incrementally restrict the search space for the subsequent\\nstages. Thus, the responsibility for achieving the goal is not centralized in any single\\nmodule and subsequent modules have always the chance to compensate for limited\\nerrors of earlier stages. This appears to be a generally useful strategy for achieving\\n\\n\\fVisual Gesture-based Robot Guidance with a Modular Neural System\\n\\n909\\n\\nrobustness and for cutting computational costs that is related to the use of \\\"focal\\nattention\\\" , which is clearly an important element of many biological vision systems.\\nA third important point is the extensive use of learning to build the essential constituent functions of the system from data examples. We are not yet able to train\\nthe assembled system as a whole. Instead, different modules are trained separately\\nand are integrated only later. Still, the experience gained with assembling a complex system via this \\\"engineering-type\\\" of approach will be extremely valuable for\\ngradually developing the capability of crafting larger functional building blocks by\\nlearning methods.\\nWe conclude that carefully designed experiments with modular neural systems that\\nare based on the use of real world data and that focus on similar tasks for which\\nalso biological neural systems were evolved can make a significant contribution in\\ntackling the challenge that lies ahead of us: to develop a reliable technology for the\\nconstruction of large-scale artificial neural systems that can solve complex tasks in\\nreal world environments.\\nAcknowledgements\\nWe want to thank Th. Wengerek (robot control), J. Walter (PSOM implementation), and\\nP. Ziemeck (image acquisition software). This work was supported by BMFT Grant No.\\nITN9104AO.\\n\\nReferences\\n[1] T. J. Darell and A. P. Pentland. Classifying hand gestures with a view-based distributed representation. In J . D. Cowan, G. Tesauro, and J. Alspector, editors, Neural\\nInformation Processing Systems 6, pages 945-952. Morgan Kaufman, 1994.\\n[2] J. Davis and M. Shah. Recognizing hand gestures. In J.-O. Eklundh, editor, Computer\\nVision - ECCV '94, volume 800 of Lecture Notes in Computer Science, pages 331340. Springer-Verlag, Berlin Heidelberg New York, 1994.\\n[3] R.A. Jacobs, M.1. Jordan, S.J. Nowlan, and G.E. Hinton. Adaptive mixtures of local\\nexperts. Neural Computation, 3:79- 87, 1991.\\n[4] M.1. Jordan and R.A. Jacobs. Hierarchical mixtures of experts and the EM algorithm.\\nNeural Computation, 6(2):181-214, 1994.\\n[5] F. Kummert, E. Littmann, A. Meyering, S. Posch, H. Ritter, and G. Sagerer. A\\nhybrid approach to signal interpretation using neural and semantic networks. In\\nMustererkennung 1993, pages 245-252. Springer, 1993.\\n[6] E. Littmann, A. Drees, and H. Ritter. Neural recognition of human pointing gestures\\nin real images. Submitted to Neural Processing Letters, 1996.\\n[7] E. Littmann and H. Ritter. Neural and statistical methods for adaptive color segmentation - a comparison. In G. Sagerer, S. Posch, and F. Kummert, editors,\\nMustererkennung 1995, pages 84-93. Springer-Verlag, Heidelberg, 1995.\\n[8] C. Maggioni. A novel device for using the hand as a human-computer interface. In\\nProceedings HC1'93 - Human Control Interface, Loughborough, Great Britain, 1993.\\n[9] A. Meyering and H. Ritter. Learning 3D shape perception with local linear maps. In\\nProc. of the lJCNN, volume IV, pages 432-436, Baltimore, MD, 1992.\\n[10] Steven J. Nowlan and John C. Platt. A convolutional neural network hand tracker.\\nIn Neural Information Processing Systems 7. Morgan Kaufman Publishers, 1995.\\n[11] H. Ritter. Parametrized self-organizing maps for vision learning tasks. In P. Morasso,\\neditor, ICANN '94. Springer-Verlag, Berlin Heidelberg New York, 1994.\\n[12] K. Viiiina.nen and K. Bohm. Gesture driven interaction as a human factor in virtual\\nenvironments - an approach with neural networks. In R. Earnshaw, M. Gigante, and\\nH. Jones, editors, Virtual reality systems, pages 93-106. Academic Press, 1993.\\n[13] J. Walter and H. Ritter. Rapid learning with parametrized self-organizing maps.\\nNeural Computing, 1995. Submitted.\\n[14] T. G. Zimmermann, J. Lanier, C. Blanchard, S. Bryson, and Y. Harvill. A hand\\ngesture interface device. In Proc. CHI+GI, pages 189-192, 1987.\\n\\n\\f\",\n          \"Iterative Non-linear Dimensionality Reduction by\\nManifold Sculpting\\n\\nMike Gashler, Dan Ventura, and Tony Martinez ?\\nBrigham Young University\\nProvo, UT 84604\\n\\nAbstract\\nMany algorithms have been recently developed for reducing dimensionality by\\nprojecting data onto an intrinsic non-linear manifold. Unfortunately, existing algorithms often lose significant precision in this transformation. Manifold Sculpting\\nis a new algorithm that iteratively reduces dimensionality by simulating surface\\ntension in local neighborhoods. We present several experiments that show Manifold Sculpting yields more accurate results than existing algorithms with both\\ngenerated and natural data-sets. Manifold Sculpting is also able to benefit from\\nboth prior dimensionality reduction efforts.\\n\\n1\\n\\nIntroduction\\n\\nDimensionality reduction is a two-step process: 1) Transform the data so that more information\\nwill survive the projection, and 2) project the data into fewer dimensions. The more relationships\\nbetween data points that the transformation step is required to preserve, the less flexibility it will have\\nto position the points in a manner that will cause information to survive the projection step. Due\\nto this inverse relationship, dimensionality reduction algorithms must seek a balance that preserves\\ninformation in the transformation without losing it in the projection. The key to finding the right\\nbalance is to identify where the majority of the information lies.\\nNonlinear dimensionality reduction (NLDR) algorithms seek this balance by assuming that the relationships between neighboring points contain more informational content than the relationships\\nbetween distant points. Although non-linear transformations have more potential than do linear\\ntransformations to lose information in the structure of the data, they also have more potential to\\nposition the data to cause more information to survive the projection. In this process, NLDR algorithms expose patterns and structures of lower dimensionality (manifolds) that exist in the original\\ndata. NLDR algorithms, or manifold learning algorithms, have potential to make the high-level\\nconcepts embedded in multidimensional data accessible to both humans and machines.\\nThis paper introduces a new algorithm for manifold learning called Manifold Sculpting, which discovers manifolds through a process of progressive refinement. Experiments show that it yields\\nmore accurate results than other algorithms in many cases. Additionally, it can be used as a postprocessing step to enhance the transformation of other manifold learning algorithms.\\n\\n2\\n\\nRelated Work\\n\\nMany algorithms have been developed for performing non-linear dimensionality reduction. Recent\\nworks include Isomap [1], which solves for an isometric embedding of data into fewer dimensions\\nwith an algebraic technique. Unfortunately, it is somewhat computationally expensive as it requires\\nsolving for the eigenvectors of a large dense matrix, and has difficulty with poorly sampled areas of\\n?\\n\\nmikegashler@gmail.com, ventura@cs.byu.edu, martinez@cs.byu.edu\\n\\n1\\n\\n\\fFigure 1: Comparison of several manifold learners on a Swiss Roll manifold. Color is used to\\nindicate how points in the results correspond to points on the manifold. Isomap and L-Isomap have\\ntrouble with sampling holes. LLE has trouble with changes in sample density.\\n\\nthe manifold. (See Figure 1.A.) Locally Linear Embedding (LLE) [2] is able to perform a similar\\ncomputation using a sparse matrix by using a metric that measures only relationships between vectors in local neighborhoods. Unfortunately it produces distorted results when the sample density is\\nnon-uniform. (See Figure 1.B.) An improvement to the Isomap algorithm was later proposed that\\nuses landmarks to reduce the amount of necessary computation [3]. (See Figure 1.C.) Many other\\nNLDR algorithms have been proposed, including Kernel Principle Component Analysis [4], Laplacian Eigenmaps [5], Manifold Charting [6], Manifold Parzen Windows [7], Hessian LLE [8], and\\nothers [9, 10, 11]. Hessian LLE preserves the manifold structure better than the other algorithms but\\nis, unfortunately, computationally expensive. (See Figure 1.D.).\\nIn contrast with these algorithms, Manifold Sculpting is robust to sampling issues and still produces\\nvery accurate results. This algorithm iteratively transforms data by balancing two opposing heuristics, one that scales information out of unwanted dimensions, and one that preserves local structure\\nin the data. Experimental results show that this technique preserves information into fewer dimensions with more accuracy than existing manifold learning algorithms. (See Figure 1.E.)\\n\\n3\\n\\nThe Algorithm\\n\\nAn overview of the Manifold Sculpting algorithm is given in Figure 2a.\\n\\nFigure 2: ? and ? define the relationships that Manifold Sculpting attempts to preserve.\\n\\n2\\n\\n\\fStep 1: Find the k nearest neighbors of each point. For each data point pi in P (where P is the set\\nof all data points represented as vectors in Rn ), find the k-nearest neighbors Ni (such that nij ? Ni\\nis the j th neighbor of point pi ).\\nStep 2: Compute relationships between neighbors. For each j (where 0 < j ? k) compute the\\nEuclidean distance ?ij between pi and each nij ? Ni . Also compute the angle ?ij formed by the\\ntwo line segments (pi to nij ) and (nij to mij ), where mij is the most colinear neighbor of nij with\\npi . (See Figure 2b.) The most colinear neighbor is the neighbor point that forms the angle closest\\nto ?. The values of ? and ? are the relationships that the algorithm will attempt to preserve during\\ntransformation. The global average distance between all the neighbors of all points ?ave is also\\ncomputed.\\nStep 3: Optionally preprocess the data. The data may optionally be preprocessed with the transformation step of Principle Component Analysis (PCA), or another efficient algorithm. Manifold\\nSculpting will work without this step; however, preprocessing can result in significantly faster convergence. To the extent that there is a linear component in the manifold, PCA will move the information in the data into as few dimensions as possible, thus leaving less work to be done in step 4\\n(which handles the non-linear component). This step is performed by computing the first |Dpres |\\nprinciple components of the data (where Dpres is the set of dimensions that will be preserved in\\nthe projection), and rotating the dimensional axes to align with these principle components. (An\\nefficient algorithm for computing principle components is presented in [12].)\\nStep 4: Transform the data. The data is iteratively transformed until some stopping criterion has\\nbeen met. One effective technique is to stop when the sum change of all points during the current\\niteration falls below a threshold. The best stopping criteria depend on the desired quality of results ?\\nif precision is important, the algorithm may iterate longer; if speed is important it may stop earlier.\\nStep 4a: Scale values. All the values in Dscal (The set of dimensions that will be eliminated by the\\nprojection) are scaled by a constant factor ?, where 0 < ? < 1 (? = 0.99 was used in this paper).\\nOver time, the values in Dscal will converge to 0. When Dscal is dropped by the projection (step 5),\\nthere will be very little informational content left in these dimensions.\\nStep 4b: Restore original relationships. For each pi ? P , the values in Dpres are adjusted to\\nrecover the relationships that are distorted by scaling. Intuitively, this step simulates tension on the\\nmanifold surface. A heuristic error value is used to evaluate the current relationships among data\\npoints relative to the original relationships:\\n\\u0012\\n\\u00132 \\u0012\\n\\u00132 !\\nk\\nX\\n?ij ? ?ij0\\n?ij ? ?ij0\\nwij\\n\\u000fpi =\\n+\\n(1)\\n2?ave\\n?\\nj=0\\nwhere ?ij is the current distance to nij , ?ij0 is the original distance to nij measured in step 2, ?ij\\nis the current angle, and ?ij0 is the original angle measured in step 2. The denominator values\\nwere chosen as normalizing factors because the value of the angle term can range from 0 to ?, and\\nthe value of the distance term will tend to have a mean of about ?ave with some variance in both\\ndirections. We adjust the values in Dpres for each point to minimize this heuristic error value.\\nThe order in which points are adjusted has some impact on the rate of convergence. Best results were\\nobtained by employing a breadth-first neighborhood graph traversal from a randomly selected point.\\n(A new starting point is randomly selected for each iteration.) Intuitively this may be analogous to\\nthe manner in which a person smoothes a crumpled piece of paper by starting at an arbitrary point\\nand smoothing outward. To further speed convergence, higher weight, wij , is given to the component\\nof the error contributed by neighbors that have already been adjusted in the current iteration. For all\\nof our experiments, we use wij = 1 if ni has not yet been adjusted in this iteration, and wij = 10,\\nif nij has been adjusted in this iteration.\\nUnfortunately the equation for the true gradient of the error surface defined by this heuristic is\\ncomplex, and is in O(|D|3 ). We therefore use the simple hill-climbing technique of adjusting in\\neach dimension in the direction that yields improvement.\\nSince the error surface is not necessarily convex, the algorithm may potentially converge to local\\nminima. At least three factors, however, mitigate this risk: First, the PCA pre-processing step often\\ntends to move the whole system to a state somewhat close to the global minimum. Even if a local\\n3\\n\\n\\fFigure 3: The mean squared error of four algorithms with a Swiss Roll manifold using a varying\\nnumber of neighbors k. When k > 57, neighbor paths cut across the manifold. Isomap is more\\nrobust to this problem than other algorithms, but HLLE and Manifold Sculpting still yield better\\nresults. Results are shown on a logarithmic scale.\\nminimum exists so close to the globally optimal state, it may have a sufficiently small error as to be\\nacceptable. Second, every point has a unique error surface. Even if one point becomes temporarily\\nstuck in a local minimum, its neighbors are likely to pull it out, or change the topology of its error\\nsurface when their values are adjusted. Very particular conditions are necessary for every point to\\nsimultaneously find a local minimum. Third, by gradually scaling the values in Dscaled (instead of\\ndirectly setting them to 0), the system always remains in a state very close to the current globally\\noptimal state. As long as it stays close to the current optimal state, it is unlikely for the error\\nsurface to change in a manner that permanently separates it from being able to reach the globally\\noptimal state. (This is why all the dimensions need to be preserved in the PCA pre-processing step.)\\nAnd perhaps most significantly, our experiments show that Manifold Sculpting generally tends to\\nconverge to very good results.\\nStep 5: Project the data. At this point Dscal contains only values that are very close to zero. The\\ndata is projected by simply dropping these dimensions from the representation.\\n\\n4\\n\\nEmpirical Results\\n\\nFigure 1 shows that Manifold Sculpting appears visually to produce results of higher quality than\\nLLE and Isomap with the Swiss Roll manifold, a common visual test for manifold learning algorithms. Quantitative analysis shows that it also yields better results than HLLE. Since the actual\\nstructure of this manifold is known prior to using any manifold learner, we can use this prior information to quantitatively measure the accuracy of each algorithm.\\n4.1\\n\\nVarying number of neighbors.\\n\\nWe define a Swiss Roll in 3D space with n points (xi , yi , zi ) for each 0 ? i < n, such that xi =\\nt sin(t), yi is a random number ?6 ? yi < 6, and zi = t cos(t), ?where t = 8i/n + 2. In 2D\\n?1\\nt2 +1\\nand vi = yi .\\nmanifold coordinates, the point is (ui , vi ), such that ui = sinh (t)+t\\n2\\nWe created a Swiss Roll with 2000 data points and reduced the dimensionality to 2 with each of four\\nalgorithms. Next we tested how well these results align with the expected values by measuring the\\nmean squared distance from each point to its expected value. (See Figure 3.) We rotated, scaled,\\nand translated the values as required to obtain the minimum possible error measurement for each\\nalgorithm. These results are consistent with a qualitative assessment of Figure 1. Results are shown\\nwith a varying number of neighbors k. In this example, when k = 57, local neighborhoods begin\\nto cut across the manifold. Isomap is more robust to this problem than other algorithms, but HLLE\\nand Manifold Sculpting still yield better results.\\n4\\n\\n\\fFigure 4: The mean squared error of points from an S-Curve manifold for four algorithms with a\\nvarying number of data points. Manifold Sculpting shows a trend of increasing accuracy with an\\nincreasing number of points. This experiment was performed with 20 neighbors. Results are shown\\non a logarithmic scale.\\n4.2\\n\\nVarying sample densities.\\n\\nA similar experiment was performed with an S-Curve manifold. We defined the S-Curve points in\\n3D space with n points (xi , yi , zi ) for each 0 ? i < n, such that xi = t, yi = sin(t), and zi is\\na random number 0 ? zi < 2, where t = (2.2i?0.1)?\\n. In 2D manifold coordinates, the point is\\nn\\nZ t \\u0010p\\n\\u0011\\ncos2 (w) + 1 dw and vi = yi .\\n(ui , vi ), such that ui =\\n0\\n\\nFigure 4 shows the mean squared error of the transformed points from their expected values using\\nthe same regression technique described for the experiment with the Swiss Roll problem. We varied\\nthe sampling density to show how this affects each algorithm. A trend can be observed in this data\\nthat as the number of sample points increases, the quality of results from Manifold Sculpting also\\nincreases. This trend does not appear in the results from other algorithms.\\nOne drawback to the Manifold Sculpting algorithm is that convergence may take longer when the\\nvalue for k is too small. This experiment was also performed with 6 neighbors, but Manifold Sculpting did not always converge within a reasonable time when so few neighbors were used. The other\\nthree algorithms do not have this limitation, but the quality of their results still tend to be poor when\\nvery few neighbors are used.\\n4.3\\n\\nEntwined spirals manifold.\\n\\nA test was also performed with an Entwined Spirals manifold. In this case, Isomap was able to\\nproduce better results than Manifold Sculpting (see Figure 5), even though Isomap yielded the worst\\naccuracy in previous problems. This can be attributed to the nature of the Isomap algorithm. In cases\\nwhere the manifold has an intrinsic dimensionality of exactly 1, a path from neighbor to neighbor\\nprovides an accurate estimate of isolinear distance. Thus an algorithm that seeks to globally optimize isolinear distances will be less susceptible to the noise from cutting across local corners. When\\nthe intrinsic dimensionality is higher than 1, however, paths that follow from neighbor to neighbor\\nproduce a zig-zag pattern that introduces excessive noise into the isolinear distance measurement. In\\nthese cases, preserving local neighborhood relationships with precision yields better overall results\\nthan globally optimizing an error-prone metric. Consistent with this intuition, Isomap is the closest\\ncompetitor to Manifold Sculpting in other experiments that involved a manifold with a single intrinsic dimension, and yields the poorest results of the four algorithms when the intrinsic dimensionality\\nis larger than one.\\n5\\n\\n\\fFigure 5: Mean squared error for four algorithms with an Entwined Spirals manifold.\\n4.4\\n\\nImage-based manifolds.\\n\\nThe accuracy of Manifold Sculpting is not limited to generated manifolds in three dimensional\\nspace. Unfortunately, the manifold structure represented by most real-world problems is not known\\na priori. The accuracy of a manifold learner, however, can still be estimated when the problem\\ninvolves a video sequence by simply counting the percentage of frames that are sorted into the same\\norder as the video sequence. Figure 6 shows several frames from a video sequence of a person\\nturning his head while gradually smiling. Each image was encoded as a vector of 1, 634 pixel\\nintensity values. This data was then reduced to a single dimension. (Results are shown on three\\nseparate lines in order to fit the page.) The one preserved dimension could then characterize each\\nframe according to the high-level concepts that were previously encoded in many dimensions. The\\ndot below each image corresponds to the single-dimensional value in the preserved dimension for\\nthat image. In this case, the ordering of every frame was consistent with the video sequence.\\n4.5\\n\\nControlled manifold topologies.\\n\\nFigure 7 shows a comparison of results obtained from a manifold generated by translating an image\\nover a background of random noise. Nine of the 400 input images are shown as a sample, and\\nresults with each algorithm are shown as a mesh. Each vertex is placed at a position corresponding\\nto the two values obtained from one of the 400 images. For increased visibility of the inherent\\nstructure, the vertexes are connected with their nearest input space neighbors. Because two variables\\n(horizontal position and vertical position) were used to generate the dataset, this data creates a\\nmanifold with an intrinsic dimensionality of two in a space with an extrinsic dimensionality of\\n2,401 (the total number of pixels in each image). Because the background is random, the average\\ndistance between neighboring points in the input space is uniform, so the ideal result is known to\\nbe a square. The distortions produced by Manifold Sculpting tend to be local in nature, while the\\ndistortions produced by other algorithms tend to be more global. Note that the points are spread\\nnearly uniformly across the manifold in the results from Manifold Sculpting. This explains why the\\nresults from Manifold Sculpting tend to fit the ideal results with much lower total error (as shown in\\n\\nFigure 6: Images of a face reduced by Manifold Sculpting into a single dimension. The values are\\nare shown here on three wrapped lines in order to fit the page. The original image is shown above\\neach point.\\n6\\n\\n\\fFigure 7: A comparison of results with a manifold generated by translating an image over a background of noise. Manifold Sculpting tends to produce less global distortion, while other algorithms\\ntend to produce less local distortion. Each point represents an image. This experiment was done\\nin each case with 8 neighbors. (LLE fails to yield results with these parameters, but [13] reports a\\nsimilar experiment in which LLE produces results. In that case, as with Isomap and HLLE as shown\\nhere, distortion is clearly visible near the edges.)\\n\\nFigure 3 and Figure 4). Perhaps more significantly, it also tends to keep the intrinsic variables in the\\ndataset more linearly separable. This is particularly important when the dimensionality reduction is\\nused as a pre-processing step for a supervised learning algorithm.\\nWe created four video sequences designed to show various types of manifold topologies and measured the accuracy of each manifold learning algorithm. These results (and sample frames from each\\nvideo) are shown in Figure 8. The first video shows a rotating stuffed animal. Since the background\\npixels remain nearly constant while the pixels on the rotating object change in value, the manifold\\ncorresponding to the vector encoding of this video will contain both smooth and changing areas.\\nThe second video was made by moving a camera down a hallway. This produces a manifold with a\\ncontinuous range of variability, since pixels near the center of the frame change slowly while pixels\\nnear the edges change rapidly. The third video pans across a scene. Unlike the video of the rotating\\nstuffed animal, there are no background pixels that remain constant. The last video shows another\\nrotating stuffed animal. Unlike the first video, however, the high-contrast texture of the object used\\nin this video results in a topology with much more variation. As the black spots shift across the\\npixels, a manifold is created that swings wildly in the respective dimensions. Due to the large hills\\nand valleys in the topology of this manifold, the nearest neighbors of a frame frequently create paths\\nthat cut across the manifold. In all four cases, Manifold Sculpting produced results competitive\\nwith Isomap, which does particularly well with manifolds that have an intrinsic dimensionality of\\n\\nFigure 8: Four video sequences were created with varying properties in the corresponding manfolds.\\nDimensionality was reduced to one with each of four manifold learning algorithms. The percentage\\nof frames that were correctly ordered by each algorithm is shown.\\n\\n7\\n\\n\\fone, but Manifold Sculpting is not limited by the intrinsic dimensionality as shown in the previous\\nexperiments.\\n\\n5\\n\\nDiscussion\\n\\nThe experiments tested in this paper show that Manifold Sculpting yields more accurate results\\nthan other well-known manifold learning algorithms. Manifold Sculpting is robust to holes in the\\nsampled area. Manifold Sculpting is more accurate than other algorithms when the manifold is\\nsparsely sampled, and the gap is even wider with higher sampling densities. Manifold Sculpting\\nhas difficulty when the selected number of neighbors is too small but consistently outperforms other\\nalgorithms when it is larger.\\nDue to the iterative nature of Manifold Sculpting, it?s difficult to produce a valid complexity analysis.\\nConsequently, we measured the scalability of Manifold Sculpting empirically and compared it with\\nthat of HLLE, L-Isomap, and LLE. Due to space constraints these results are not included here, but\\nthey indicate that Manifold Sculpting scales better than the other algorithms when when the number\\nof data points is much larger than the number of input dimensions.\\nManifold Sculpting benefits significantly when the data is pre-processed with the transformation step of PCA. The transformation step of any algorithm may be used in place of this step.\\nCurrent research seeks to identify which algorithms work best with Manifold Sculpting to efficiently produce high quality results. (An implementation of Manifold Sculpting is included at\\nhttp://waffles.sourceforge.net.)\\n\\nReferences\\n[1] Joshua B. Tenenbaum, Vin de Silva, and John C. Langford. A global geometric framework for\\nnonlinear dimensionality reduction. Science, 290:2319?2323, 2000.\\n[2] Sam T. Roweis and Lawrence K. Saul. Nonlinear dimensionality reduction by locally linear\\nembedding. Science, 290:2323?2326, 2000.\\n[3] Vin de Silva and Joshua B. Tenenbaum. Global versus local methods in nonlinear dimensionality reduction. In NIPS, pages 705?712, 2002.\\n[4] Bernhard Sch?olkopf, Alexander J. Smola, and Klaus-Robert M?uller. Kernel principal component analysis. Advances in kernel methods: support vector learning, pages 327?352, 1999.\\n[5] Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In Advances in Neural Information Processing Systems, 14, pages 585?\\n591, 2001.\\n[6] Matthew Brand. Charting a manifold. In Advances in Neural Information Processing Systems,\\n15, pages 961?968. MIT Press, Cambridge, MA, 2003.\\n[7] Pascal Vincent and Yoshua Bengio. Manifold parzen windows. In Advances in Neural Information Processing Systems 15, pages 825?832. MIT Press, Cambridge, MA, 2003.\\n[8] D. Donoho and C. Grimes. Hessian eigenmaps: locally linear embedding techniques for high\\ndimensional data. Proc. of National Academy of Sciences, 100(10):5591?5596, 2003.\\n[9] Yoshua Bengio and Martin Monperrus. Non-local manifold tangent learning. In Advances\\nin Neural Information Processing Systems 17, pages 129?136. MIT Press, Cambridge, MA,\\n2005.\\n[10] Elizaveta Levina and Peter J. Bickel. Maximum likelihood estimation of intrinsic dimension.\\nIn NIPS, 2004.\\n[11] Zhenyue Zhang and Hongyuan Zha. A domain decomposition method for fast manifold learning. In Y. Weiss, B. Sch?olkopf, and J. Platt, editors, Advances in Neural Information Processing\\nSystems 18. MIT Press, Cambridge, MA, 2006.\\n[12] Sam Roweis. Em algorithms for PCA and SPCA. In Michael I. Jordan, Michael J. Kearns, and\\nSara A. Solla, editors, Advances in Neural Information Processing Systems, volume 10, 1998.\\n[13] Lawrence K. Saul and Sam T. Roweis. Think globally, fit locally: Unsupervised learning of\\nlow dimensional manifolds. Journal of Machine Learning Research, 4:119?155, 2003.\\n\\n8\\n\\n\\f\",\n          \"Diffeomorphic Dimensionality Reduction\\n\\nChristian Walder and Bernhard Sch?olkopf\\nMax Planck Institute for Biological Cybernetics\\n72076 T?ubingen, Germany\\nfirst.last@tuebingen.mpg.de\\n\\nAbstract\\nThis paper introduces a new approach to constructing meaningful lower dimensional representations of sets of data points. We argue that constraining the mapping between the high and low dimensional spaces to be a diffeomorphism is a\\nnatural way of ensuring that pairwise distances are approximately preserved. Accordingly we develop an algorithm which diffeomorphically maps the data near to\\na lower dimensional subspace and then projects onto that subspace. The problem\\nof solving for the mapping is transformed into one of solving for an Eulerian flow\\nfield which we compute using ideas from kernel methods. We demonstrate the\\nefficacy of our approach on various real world data sets.\\n\\n1\\n\\nIntroduction\\n\\nThe problem of visualizing high dimensional data often arises in the context of exploratory data\\nanalysis. For many real world data sets this is a challenging task, as the spaces in which the data\\nlie are often too high dimensional to be visualized directly. If the data themselves lie on a lower\\ndimensional subspace however, dimensionality reduction techniques may be employed, which aim\\nto meaningfully represent the data as elements of this lower dimensional subspace.\\nThe earliest approaches to dimensionality reduction are the linear methods known as principal components analysis (PCA) and factor analysis (Duda et al., 2000). More recently however, the majority of research has focussed on non-linear methods, in order to overcome the limitations of linear\\napproaches?for an overview and numerical comparison see e.g. (Venna, 2007; van der Maaten\\net al., 2008), respectively. In an effort to better understand the numerous methods which have been\\nproposed, various categorizations have been proposed. In the present case, it is pertinent to make\\nthe distinction between methods which focus on properties of the mapping to the lower dimensional\\nspace, and methods which focus on properties of the mapped data, in that space. A canonical example of the latter is multidimensional scaling (MDS), which in its basic form finds the minimizer\\nwith respect to y1 , y2 , . . . , ym of (Cox & Cox, 1994)\\nm\\nX\\n\\n2\\n\\n(kxi ? xj k ? kyi ? yj k) ,\\n\\n(1)\\n\\ni,j=1\\n\\nwhere here, as throughout the paper, the xi ? Ra are input or high dimensional points, and the\\nyi ? Rb are output or low dimensional points, so that b < a. Note that the above term is a\\nfunction only of the input points and the corresponding mapped points, and is designed to preserve\\nthe pairwise distances of the data set.\\nThe methods which focus on the mapping itself (from the higher to the lower dimensional space,\\nwhich we refer to as the downward mapping, or the upward mapping which is the converse) are less\\ncommon, and form a category into which the present work falls. Both auto-encoders (DeMers &\\nCottrell, 1993) and the Gaussian process latent variable model (GP-LVM) (Lawrence, 2004) also\\nfall into this category, but we focus on the latter as it provides an appropriate transition into the\\n1\\n\\n\\fmain part the paper. The GP-LVM places a Gaussian process (GP) prior over each high dimensional component of the upward mapping, and optimizes with respect to the set of low dimensional\\npoints?which can be thought of as hyper-parameters of the model?the likelihood of the high dimensional points. Hence the GP-LVM constructs a regular (in the sense of regularization, i.e. likely\\nunder the GP prior) upward mapping. By doing so, the model guarantees that nearby points in\\nthe low dimensional space should be mapped to nearby points in the high dimensional space?an\\nintuitive idea for dimensionality reduction which is also present in the MDS objective (1), above.\\nThe converse is not guaranteed in the original GP-LVM however, and this has lead to the more recent development of the so-called back-constrained GP-LVM (Lawrence & Candela, 2006), which\\nessentially places an additional GP prior over the downward mapping. By guaranteeing in this way\\nthat (the modes of the posterior distributions over) both the upward and downward mappings are\\nregular, the back constrained GP-LVM induces something reminiscent of a diffeomorphic mapping\\nbetween the two spaces. This leads us to the present work, in which we derive our new algorithm,\\nDiffeomap, by explicitly casting the dimensionality reduction problem as one of constructing a diffeomorphic mapping between the low dimensional space and the subspace of the high dimensional\\nspace on which the data lie.\\n\\n2\\n\\nDiffeomorphic Mappings and their Practical Construction\\n\\nIn this paper we use the following definition:\\nDefinition 2.1. Let U and V be open subsets of Ra and Rb , respectively. The mapping F : U ? V\\nis said to be a diffeomorphism if it is bijective (i.e. one to one), smooth (i.e. belonging to C ? ), and\\nhas a smooth inverse map F ?1 .\\nWe note in passing the connection between this definition, our discussion of the GP-LVM, and dimensionality reduction. The GP-LVM constructs a regular upward mapping (analogous to F ?1 )\\nwhich ensures that points nearby in Rb will be mapped to points nearby in Ra , a property referred\\nto as similarity preservation in (Lawrence & Candela, 2006). The back constrained GP-LVM simultaneously ensures that the downward mapping (analogous to F ) is regular, thereby additionally\\nimplementing what its authors refer to as dissimilarity preservation. Finally, the similarity between\\nsmoothness (required of F and F ?1 in Definition 2.1) and regularity (imposed on the downward and\\nupward mappings by the GP prior in the back constrained GP-LVM) complete the analogy. There is\\nalso an alternative, more direct motivation for diffeomorphic mappings in the context of dimensionality reduction, however. In particular, a diffeomorphic mapping has the property that it does not\\nlose any information. That is, given the mapping itself and the lower dimensional representation of\\nthe data set, it is always possible to reconstruct the original data.\\nThere has been significant interest from within the image processing community, in the construction\\nof diffeomorphic mappings for the purpose of image warping (Dupuis & Grenander, 1998; Joshi\\n& Miller, 2000; Karac?ali & Davatzikos, 2003). The reason for this can be understood as follows.\\nLet I : U ? R3 represent the RGB values of an image, where U ? R2 is the image plane. If we\\nnow define the warped version of I to be I ? W , then we can guarantee that the warp is topology\\npreserving, i.e. that it does not ?tear? the image, by ensuring the W be a diffeomorphism U ? U .\\nThe following two main approaches to constructing such diffeomorphisms have been taken by the\\nimage processing community, the first of which we mention for reference, while the second forms\\nthe basis of Diffeomap. It is a notable aside that there seem to be no image warping algorithms\\nanalogous to the back constrained GP-LVM, in which regular forward and inverse mappings are\\nsimultaneously constructed.\\n1. Enforcement of the constraint that |J(W )|, the determinant of the Jacobian of the mapping, be positive everywhere. This approach has been successfully applied to the problem\\nof warping 3D magnetic resonance images (Karac?ali & Davatzikos, 2003), for example,\\nbut a key ingredient of that success was the fact that the authors defined the mapping W\\nnumerically on a regular grid. For the high dimensional cases relevant to dimensionality\\nreduction however, such a numerical grid is highly computationally unattractive.\\n2. Recasting the problem of constructing W as an Eulerian flow problem (Dupuis & Grenander, 1998; Joshi & Miller, 2000). This approach is the focus of the next section.\\n2\\n\\n\\fR\\n\\nR\\n?(x, 1) = ?(x)\\n(s, ?(x, s))\\n(1, v(?(x, s), s))\\n\\nx\\nt\\n0\\n\\ns\\n\\n1\\n\\nFigure 1: The relationship between v(?, ?), ?(?, ?) and ?(?) for the one dimensional case ? : R ? R.\\n\\n2.1\\n\\nDiffeomorphisms via Flow Fields\\n\\nThe idea here is to indirectly define the mapping of interest, call it ? : Ra ? Ra , by way of a ?time?\\nindexed velocity field v : Ra ? R ? Ra . In particular we write ?(x) = ?(x, 1), where\\nZ t\\nv(?(x, s), s)ds.\\n(2)\\n?(x, t) = x +\\ns=0\\n\\nThis choice of ? satisfies the following Eulerian transport equation with boundary conditions:\\n??(x, s)\\n= v(?(x, s), s),\\n?s\\n\\n?(x, 0) = x.\\n\\n(3)\\n\\nThe role of v is to transport a given point x from its original location at time 0 to its mapped location\\n?(x, 1) by way of a trajectory whose position and tangent vector at time s are given by ?(x, s) and\\nv(?(x, s), s), respectively (see Figure 1). The point of this construction is that if v satisfies certain\\nregularity properties, then the mapping ? will be a diffeomorphism. This fact has been proven in a\\nnumber of places?one particularly accessible example is (Dupuis & Grenander, 1998), where the\\nnecessary conditions are provided for the three dimensional case along with a proof that the induced\\nmapping is a diffeomorphism. Generalizing the result to higher dimensions is straightforward?this\\nfact is stated in (Dupuis & Grenander, 1998) along with the basic idea of how to do so.\\nWe now offer an intuitive argument for the result. Consider Figure 1, and imagine adding a new\\nstarting point x? , along with its associated trajectory. It is clear that for the mapping ? to be a\\ndiffeomorphism, then for any such pair of points x and x? , the associated trajectories must not\\ncollide. This is because the two trajectories would be identical after the collision, x and x? would\\nmap to the same point, and hence the mapping would not be invertible. But if v is sufficiently regular\\nthen such collisions cannot occur.\\n\\n3\\n\\nDiffeomorphic Dimensionality Reduction\\n\\nThe framework of Eulerian flow fields which we have just introduced provides an elegant means\\nof constructing diffeomorphic mappings Ra ? Ra , but for dimensionality reduction we require\\nadditional ingredients, which we now introduce. The basic idea is to construct a diffeomorphic\\nmapping in such a way that it maps our data set near to a subspace of Ra , and then to project onto\\nthis subspace. The subspace we use, call it Sb , is the b-dimensional one spanned by the first b\\ncanonical basis vectors of Ra . Let P(a?b) : Ra ? Rb be the projection operator which extracts the\\nfirst b components of the vector it is applied to, i.e.\\nP(a?b) x = (I Z) x,\\n\\n(4)\\n\\nwhere I ? Ra?a is the identity matrix and Z ? Ra?b?a is a matrix of zeros. We can now write the\\nmapping ? : Ra ? Rb which we propose for dimensionality reduction as\\n?(x) = P(a?b) ?(x, 1),\\n3\\n\\n(5)\\n\\n\\fwhere ? is given by (2). We choose each component of v at each time to belong to a reproducing\\nkernel Hilbert Space (RKHS) H, so that v(?, t) ? Ha , t ? [0, 1]. If we define the norm1\\na \\n\\n2\\nX\\n\\n\\n2\\nkv(?, t)kHa ,\\n(6)\\n\\n[v(?, t)]j \\n ,\\nH\\n\\nj=1\\n\\n2\\n\\nthen kv(?, t)kHa < ?, ?t ? [0, 1] is a sufficient condition which guarantees that ? is a diffeomorphism, provided that some technical conditions are satisfied (Dupuis & Grenander, 1998; Joshi\\n& Miller, 2000). In particular v need not be regular in its second argument. For dimensionality\\nreduction we propose to construct v as the minimizer of\\nZ 1\\nm\\nX\\n2\\nkv(?, t)kHd dt +\\nL (?(xj )) ,\\n(7)\\nO=?\\nt=0\\n\\nj=1\\n\\n+\\n\\nwhere ? ? R is a regularization parameter. Here, L measures the squared distance to our b\\ndimensional linear subspace of interest Sb , i.e.\\nL(x) =\\n\\na\\nX\\n\\n2\\n\\n[x]d .\\n\\n(8)\\n\\nd=b+1\\n\\nNote that this places special importance on the first b dimensions of the input space of interest?\\naccordingly we make the natural and important preprocessing step of applying PCA such that as\\nmuch as possible of the variance of the data is captured in these first b dimensions.\\n3.1\\n\\nImplementation\\n\\nOne can show that the minimizer in v of (7) takes the form\\n[v(?, t)]d =\\n\\nm\\nX\\n\\n[?d (t)]j k(?(xj , t), ?),\\n\\nd = 1 . . . a,\\n\\n(9)\\n\\nj=1\\n\\nwhere k is the reproducing kernel of H and ?d is a function [0, 1] ? Rm . This was proven directly\\nfor a similar specific case (Joshi & Miller, 2000), but we note in passing that it follows immediately\\nfrom the celebrated representer theorem of RKHS?s (Sch?olkopf et al., 2001), by considering a fixed\\ntime t. Hence, we have simplified the problem of determining v to one of determining m trajectories\\n?(xj , ?). This is because not only does (9) hold, but we can use standard manipulations (in the\\ncontext of kernel ridge regression, for example) to determine that for a given set of such trajectories,\\n?d (t) = K(t)?1 ud (t),\\nm?m\\n\\nd = 1, 2, . . . , a,\\n\\n(10)\\n\\nm\\n\\nwhere t ? [0, 1], K(t) ? R\\n, ud (t) ? R and we have let [K(t)]j,k = k(?(xj , t), ?(xk , t))\\nalong with [ud (t)]j = ?t ?(xj , t). Note that the invertibility of K(t) is guaranteed for certain kernel\\nfunctions (including the Gaussian kernel which we employ in all our Experiments, see Section 4),\\nprovided that the set ?(xj , t) are distinct. Hence, one can verify using (9), (10) and the reproducing\\nproperty of k in H (i.e. the fact that hf, k(x, ?)iH = f (x), ?f ? H), that for the optimal v,\\n2\\n\\nkv(?, t)kHa =\\n\\na\\nX\\n\\nud (t)? K(t)?1 ud (t).\\n\\n(11)\\n\\nd=1\\n\\nThis allows us to write our objective (7) in terms of the m trajectories mentioned above:\\nZ 1 X\\na\\nm X\\na\\nX\\n2\\nO=?\\nud (t)? K(t)?1 ud (t) +\\n[?(xj , 1)]d .\\nt=0 d=1\\n\\n(12)\\n\\nj=1 d=b+1\\n\\nSo far no approximations have been made, and we have constructed an optimal finite dimensional\\nbasis for v(?, t). The second argument of v is not so easily dealt with however, so as an approximate\\nby discretizing the interval [0, 1]. In particular, we let tk = k?, k = 0, 1, . . . , p, where ? = 1/p,\\nand make the approximation ?t=tk ?(xj , t) = (?(xj , tk ) ? ?(xj , tk?1 )) /?. By making the further\\n1\\n\\nSquare brackets w/ subscripts denote matrix elements, and colons denote entire rows or columns.\\n\\n4\\n\\n\\f0.9\\n\\n0.8\\n\\n0.7\\n\\n0.6\\n\\n0.5\\n\\n(d)\\n\\n0.4\\n\\n(c)\\n(b)\\n\\n0.3\\n\\n0.2\\n\\n0.1\\n\\n0\\n0\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\n0.9\\n\\n1\\n\\n(a)\\n\\n(b)\\n\\n(c)\\n\\n(d)\\n\\nFigure 2: Dimensionality reduction of motion capture data. (a) The data mapped from 102 to\\n2 dimensions using Diffeomap (the line shows the temporal order in which the input data were\\nrecorded). (b)-(d) Three rendered input points corresponding to the marked locations in (a).\\nR tk\\napproximation t=t\\nK(t)?1 dt = ?K(tk?1 )?1 , and substituting into (12) we obtain the first form\\nk?1\\nof our problem which is finite dimensional and hence readily optimized, i.e. the minimization of\\np\\na\\nb\\nX\\n? XX\\n?\\n(?k,d ? ?k?1,d ) K(tk )?1 (?k,d ? ?k?1,d ) +\\nk?p,d k2\\n?\\n\\n(13)\\n\\nd=a+1\\n\\nd=1 k=1\\n\\nwith respect to ?k,d ? Rm for k = 1, 2, . . . , p and d = 1, 2, . . . , a, where [?k,d ]j = [?(xj , tk )]d .\\n3.2\\n\\nA Practical Reduced Set Implementation\\n\\nA practical problem with (13) is the computationally expensive matrix inverse. In practice we reduce\\nthis burden by employing a reduced set expansion which replaces the sum over 1, 2, . . . , m in (9)\\nwith a sum over a randomly selected subset I, thereby using |I| = n basis functions to represent\\nv(?, t). In this case it is possible to show using the reproducing property of k(?, ?) that the resulting\\nobjective function is identical to (13), but with the matrix K(tk )?1 replaced by the expression\\nKm,n (Kn,m Km,n )\\n?\\nKn,m\\n\\n?1\\n\\n?1\\n\\nKn,n (Kn,m Km,n )\\n\\nKn,m ,\\n\\n(14)\\n\\nm?n\\n\\nwhere Km,n =\\n? R\\nis the sub-matrix of K(tk ) formed by taking all of the rows, but\\nonly those columns given by I. Similarly, Kn,n ? Rn?n is the square sub-matrix of K(tk ) formed\\nby taking a subset of both the rows and columns, namely those given by I. For optimization we\\nalso use the gradients of the above expression, the derivation of which we have omitted for brevity.\\nNote however that by factorizing appropriately, the computation of the objective function and its\\ngradients can be performed with an asymptotic time complexity of n2 (m + a).\\n\\n4\\n\\nExperiments\\n\\nIt is difficult to objectively compare dimensionality reduction algorithms, as there is no universally\\nagreed upon measure of performance. Algorithms which are generalizations or variations of older\\nones may be compared side by side with their predecessors, but this is not the case with our new\\nalgorithm, Diffeomap. Hence, in this section we attempt to convince the reader of the utility of our\\napproach by visually presenting our results on as many and as varied realistic problems as space\\npermits, while providing pointers to comparable results from other authors. For all experiments\\nwe fixed the parameters which trade off between computational speed and accuracy, i.e. we set the\\ntemporal resolution p = 20, and the number of\\n\\u0001 basis functions n = 300. We used a Gaussian kernel\\nfunction k(x, y) = exp ?kx ? yk2 /(2? 2 ) , and tuned the ? parameter manually along with the\\nregularization parameter ?. For optimization we used a conjugate gradient type method2 fixed to\\n1000 iterations and with starting point [?k,d ]j = [xj ]d , k = 1, 2, . . . p.\\n2\\n\\nCarl Rasmussen?s minimize.m, which is freely available from http://www.kyb.mpg.de/?carl.\\n\\n5\\n\\n\\fa\\n?\\n\\\"\\ne\\ni\\n1\\no\\n@\\nu\\n(a)\\n\\n(b)\\n\\n(c)\\n\\nFigure 3: Vowel data mapped from 24 to 2 dimensions using (a) PCA and (b)-(c) Diffeomap. Plots\\n(b) and (c) differ only in the parameter settings of Diffeomap, with (b) corresponding to minimal\\none nearest neighbor errors in the low dimensional space?see Section 4.2 for details.\\n4.1\\n\\nMotion Capture Data\\n\\nThe first data set we consider consists of the coordinates in R3 of a set of markers placed on a person\\nbreaking into a run, sampled at a constant frequency, resulting in m = 217 data points in a = 102\\ndimensions, which we mapped to b = 2 dimensions using Diffeomap (see Figure 2). This data set\\nis freely available from http://accad.osu.edu/research/mocap/mocap_data.htm\\nas Figure 1 Run, and was also considered in (Lawrence & Candela, 2006), where it was shown\\nthat while the original GP-LVM fails to correctly discover the periodic component of the sequence,\\nthe back constrained version maps poses in the same part of the subject?s step cycle nearby to\\neach other, while simultaneously capturing variations in the inclination of the subject. Diffeomap\\nalso succeeded in this sense, and produced results which are competitive with those of the back\\nconstrained GP-LVM.\\n4.2\\n\\nVowel Data\\n\\nIn this next example we consider a data set of a = 24 features (cepstral coefficients and delta\\ncepstral coefficients) of a single speaker performing nine different vowels 300 times per vowel,\\nacquired as training data for a vocal joystick system (Bilmes & et.al., 2006), and publicly available\\nin pre-processed form from http://www.dcs.shef.ac.uk/?neil/fgplvm/. Once again\\nwe used Diffeomap to map the data to b = 2 dimensions, as depicted in Figure 3. We also depict\\nthe poor result of linear PCA, in order to rule out the hypothesis that it is merely the PCA based\\ninitialization of Diffeomap (mentioned after equation (8) on page 4) which does most of the work.\\nThe results in Figure 3 are directly comparable to those provided in (Lawrence & Candela, 2006)\\nfor the GP-LVM, back constrained GP-LVM, and Isomap (Tenenbaum et al., 2000). Visually, the\\nDiffeomap result appears to be superior to those of the GP-LVM and Isomap, and comparable to the\\nback constrained GP-LVM. We also measured the performance of a one nearest neighbor classifier\\napplied to the mapped data in R2 . For the best choice of the parameters ? and ?, Diffeomap made\\n140 errors, which is favorable to the figures quoted for Isomap (458), the GP-LVM (226) and the\\nback constrained GP-LVM (155) in (Lawrence & Candela, 2006). We emphasize however that this\\nmeasure of performance is at best a rough one, since by manually varying our choice of the parameters ? and ?, we were able to obtain a result (Figure 3 (c)) which, although leads to a significantly\\nhigher number of such errors (418), is arguably superior from a qualitative perspective to the result\\nwith minimal errors (Figure 3 (b)).\\n4.3\\n\\nUSPS Handwritten Digits\\n\\nWe now consider the USPS database of handwritten digits (Hull, 1994). Following the methodology of the stochastic neighbor embedding (SNE) and GP-LVM papers (Hinton & Roweis, 2003;\\nLawrence, 2004), we take 600 images per class from the five classes corresponding to digits 0, 1, 2,\\n3, 4. Since the images are in gray scale and a resolution of 16 by 16 pixels, this results in a data set\\nof m = 3000 examples in a = 256 dimensions, which we again mapped to b = 2 dimensions as\\ndepicted in Figure 4. The figure shows the individual points color coded according to class, along\\n6\\n\\n\\f(a)\\n\\n(b)\\n\\nFigure 4: USPS handwritten digits 0-4 mapped to 2 dimensions using Diffeomap. (a) Mapped points\\ncolor coded by class label. (b) A composite image of the mapped data?see Section 4.3 for details.\\n\\nwith a composite image formed by sequentially drawing each digit in random order at its mapped\\nlocation, but only if it would not obscure a previously drawn digit. Diffeomap manages to arrange\\nthe data in a manner which reveals such image properties as digit angle and stroke thickness. At the\\nsame time the classes are reasonably well separated, with the exception of the ones which are split\\ninto two clusters depending on the angle. Although unfortunate, we believe that this splitting can\\nbe explained by the fact that (a) the left- and right-pointing ones are rather dissimilar in input space,\\nand (b) the number of fairly vertical ones which could help to connect the left- and right-pointing\\nones is rather small. Diffeomap seems to produce a result which is superior to that of the GP-LVM\\n(Lawrence, 2004), for example, but may be inferior to that of the SNE (Hinton & Roweis, 2003). We\\nbelieve this is due to the fact that the nearest neighbor graph used by SNE is highly appropriate to the\\nUSPS data set. This is indicated by the fact that a nearest neighbor classifier in the 256 dimensional\\ninput space is known to perform strongly, with numerous authors having reported error rates of less\\nthan 5% on the ten class classification problem.\\n4.4\\n\\nNIPS Text Data\\n\\nFinally, we present results on the text data of papers from the NIPS conference proceedings volumes\\n0-12, which can be obtained from http://www.cs.toronto.edu/?roweis/data.html.\\nThis experiment is intended to address the natural concern that by working in the input space rather\\nthan on a nearest neighbor graph, for example, Diffeomap may have difficulty with very high dimensional data. Following (Hinton & Roweis, 2003; Song et al., 2008) we represent the data as a word\\nfrequency vs. document matrix in which the author names are treated as words but weighted up by\\na factor 20 (i.e. an author name is worth 20 words). The result is a data set of m = 1740 papers\\nrepresented in a = 13649 words + 2037 authors = 15686 dimensions. Note however that the input\\ndimensionality is effectively reduced by the PCA preprocessing step to m ? 1 = 1739, that being\\nthe rank of the centered covariance matrix of the data.\\nAs this data set is difficult to visualize without taking up large amounts of space, we have included\\nthe results in the supplementary material which accompanies our NIPS submission. In particular,\\nwe provide a first figure which shows the data mapped to b = 2 dimensions, with certain authors (or\\ngroups of authors) color coded?the choice of authors and their corresponding color codes follows\\nprecisely those of (Song et al., 2008). A second figure shows a plain marker drawn at the mapped\\nlocations corresponding to each of the papers. This second figure also contains the paper title and\\nauthors of the corrsponding papers however, which are revealed when the user moves the mouse\\nover the marked locations. Hence, this second figure allows one to browse the NIPS collection con7\\n\\n\\ftextually. Since the mapping may be hard to judge, we note in passing that the correct classification\\nrate of a one nearest neighbor classifier applied to the result of Diffeomap was 48%, which compares\\nfavorably to the rate of 33% achieved by linear PCA (which we use for preprocessing). To compute\\nthis score we treated authors as classes, and considered only those authors who were color coded\\nboth in our supplementary figure and in (Song et al., 2008).\\n\\n5\\n\\nConclusion\\n\\nWe have presented an approach to dimensionality reduction which is based on the idea that the mapping between the lower and higher dimensional spaces should be diffeomorphic. We provided a\\njustification for this approach, by showing that the common intuition that dimensionality reduction\\nalgorithms should approximately preserve pairwise distances of a given data set is closely related to\\nthe idea that the mapping induced by the algorithm should be a diffeomorphism. This realization\\nallowed us to take advantage of established mathematical machinery in order to convert the dimensionality reduction problem into a so called Eulerian flow problem, the solution of which is guaranteed to generate a diffeomorphism. Requiring that the mapping and its inverse both be smooth is\\nreminiscent of the GP-LVM algorithm (Lawrence & Candela, 2006), but has the advantage in terms\\nof statistical strength that we need not separately estimate a mapping in each direction. We showed\\nresults of our algorithm, Diffeomap, on a relatively small motion capture data set, a larger vowel\\ndata set, the USPS image data set, and finally the rather high dimensional data set derived from the\\ntext corpus of NIPS papers, with successes in all cases. Since our new approach performs well in\\npractice while being significantly different to all previous approaches to dimensionality reduction, it\\nhas the potential to lead to a significant new direction in the field.\\n\\nReferences\\nBilmes, J., & et.al. (2006). The Vocal Joystick. Proc. IEEE Intl. Conf. on Acoustic, Speech and Signal Processing. Toulouse, France.\\nCox, T., & Cox, M. (1994). Multidimensional scaling. London, UK: Chapman & Hall.\\nDeMers, D., & Cottrell, G. (1993). Non-linear dimensionality reduction. NIPS 5 (pp. 580?587). Morgan\\nKaufmann, San Mateo, CA.\\nDuda, R. O., Hart, P. E., & Stork, D. G. (2000). Pattern classification. New York: Wiley. 2nd Edition.\\nDupuis, P., & Grenander, U. (1998). Variational problems on flows of diffeomorphisms for image matching.\\nQuarterly of Applied Mathematics, LVI, 587?600.\\nHinton, G., & Roweis, S. (2003). Stochastic neighbor embedding. In S. T. S. Becker and K. Obermayer (Eds.),\\nAdvances in neural information processing systems 15, 833?840. Cambridge, MA: MIT Press.\\nHull, J. J. (1994). A database for handwritten text recognition research. IEEE Trans. Pattern Anal. Mach.\\nIntell., 16, 550?554.\\nJoshi, S. C., & Miller, M. I. (2000). Landmark matching via large deformation diffeomorphisms. IEEE Transactions on Image Processing, 9, 1357?1370.\\nKarac?ali, B., & Davatzikos, C. (2003). Topology preservation and regularity in estimated deformation fields.\\nInformation Processing in Medical Imaging (pp. 426?437).\\nLawrence, N. D. (2004). Gaussian process latent variable models for visualisation of high dimensional data. In\\nS. Thrun, L. Saul and B. Sch?olkopf (Eds.), Nips 16. Cambridge, MA: MIT Press.\\nLawrence, N. D., & Candela, J. Q. (2006). Local distance preservation in the GP-LVM through back constraints.\\nIn International conference on machine learning, 513?520. ACM.\\nSch?olkopf, B., Herbrich, R., & Smola, A. J. (2001). A generalized representer theorem. Proc. of the 14th\\nAnnual Conf. on Computational Learning Theory (pp. 416?426). London, UK: Springer-Verlag.\\nSong, L., Smola, A., Borgwardt, K., & Gretton, A. (2008). Colored maximum variance unfolding. In J. Platt,\\nD. Koller, Y. Singer and S. Roweis (Eds.), Nips 20, 1385?1392. Cambridge, MA: MIT Press.\\nTenenbaum, J. B., de Silva, V., & Langford, J. C. (2000). A global geometric framework for nonlinear dimensionality reduction. Science, 290, 2319?2323.\\nvan der Maaten, L. J. P., Postma, E., & van den Herik, H. (2008). Dimensionality reduction: A comparative\\nreview. In T. Ertl (Ed.), Submitted to neurocognition. Elsevier.\\nVenna, J. (2007). Dimensionality reduction for visual exploration of similarity structures. Doctoral dissertation,\\nHelsinki University of Technology.\\n\\n8\\n\\n\\f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 154
        }
      ],
      "source": [
        "df = df.iloc[:5000,:]\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "L2WGcxmDyenp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e35696e5-de18-488e-d604-464abe92671b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "AWOyWGdNygkN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a21375de-763f-4902-be3d-1d00911c6634"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "_NfiJmvlzETZ"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADAZBptoDlmC"
      },
      "source": [
        "Creating a list of custom stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "BwgmPYf7Dqt0"
      },
      "outputs": [],
      "source": [
        "new_words = [\"fig\",\"figure\",\"image\",\"sample\",\"using\",\"show\",\"result\",\"large\",\"also\",\"one\",\"two\",\"three\",\"four\",\"five\",\"seven\",\"eight\",\"nine\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "7rbb1iytE7zj"
      },
      "outputs": [],
      "source": [
        "stop_words = list(stop_words.union(new_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "yHyWjP8wFKzo"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "  if isinstance(text, float):\n",
        "    text = str(text)\n",
        "  text = text.lower()\n",
        "  text = re.sub(r'<.*?>',' ',text)\n",
        "  text = re.sub(r'[^a-zA-Z]',' ',text)\n",
        "  text = nltk.word_tokenize(text)\n",
        "  text = [word for word in text if word not in stop_words]\n",
        "  text = [word for word in text if len(word) >= 3]\n",
        "  stemming = PorterStemmer()\n",
        "  text = [stemming.stem(word) for word in text]\n",
        "  return ' '.join(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "oGzNBBk_GmHh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "7d803ec1-d4bb-4c8c-8865-988274b4ca72"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'learn understand python web languag'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 161
        }
      ],
      "source": [
        "preprocess_text(\"tHiS iS to learning and understanding 4545 %$# PyThoN <h1><p> web language</p></h1>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "_6sk-hQ9RZ-N"
      },
      "outputs": [],
      "source": [
        "docs = df['paper_text'].apply(lambda x: preprocess_text(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "3kayFFMftewI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a54cb2ca-9f91-473e-908e-f295fdc15dd0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<5000x5000 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 3183711 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vector = CountVectorizer(max_df = 0.95, max_features = 5000, ngram_range = (1,3))\n",
        "word_count_vectors = count_vector.fit_transform(docs)\n",
        "word_count_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "TuwmuIoa0-S5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "d8e09a6f-3804-4a07-b9d8-095d7c145dea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfTransformer()"
            ],
            "text/html": [
              "<style>#sk-container-id-6 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-6 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-6 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-6 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-6 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-6 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-6 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-6 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-6 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-6 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-6 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-6 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-6 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-6 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-6 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-6 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-6 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-6 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-6 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-6 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-6 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-6 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-6 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-6 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-6 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-6 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-6 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-6 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-6 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-6 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-6 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-6 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-6 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-6 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-6 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-6 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfTransformer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;TfidfTransformer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\">?<span>Documentation for TfidfTransformer</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfTransformer()</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_transformer = TfidfTransformer(use_idf=True,smooth_idf=True)\n",
        "tfidf_transformer = tfidf_transformer.fit(word_count_vectors)\n",
        "tfidf_transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "S4tMgX1z_R-C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad64e8cc-6017-4e34-8a59-c761a76ed999"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aaai\n",
            "ab\n",
            "abil\n",
            "abl\n",
            "absenc\n",
            "absolut\n",
            "absolut valu\n",
            "abstract paper\n",
            "abstract present\n",
            "acad\n",
            "acad sci\n",
            "academ\n",
            "academ press\n",
            "academi\n",
            "academi scienc\n",
            "acceler\n",
            "accept\n",
            "access\n",
            "accommod\n",
            "accomplish\n",
            "accord\n",
            "accordingli\n",
            "account\n",
            "accumul\n",
            "accur\n",
            "accur estim\n",
            "accuraci\n",
            "achiev\n",
            "acid\n",
            "acknowledg\n",
            "acknowledg author\n",
            "acknowledg research\n",
            "acknowledg support\n",
            "acknowledg thank\n",
            "acknowledg work\n",
            "acknowledg work support\n",
            "acl\n",
            "acm\n",
            "acoust\n",
            "acoust speech\n",
            "acoust speech signal\n",
            "acquir\n",
            "acquisit\n",
            "across\n",
            "across differ\n",
            "act\n",
            "action\n",
            "action pair\n",
            "action potenti\n",
            "action select\n",
            "action space\n",
            "action state\n",
            "action valu\n",
            "action valu function\n",
            "activ\n",
            "activ function\n",
            "activ learn\n",
            "activ neuron\n",
            "activ pattern\n",
            "activ set\n",
            "actor\n",
            "actor critic\n",
            "actual\n",
            "acycl\n",
            "ad\n",
            "adaboost\n",
            "adam\n",
            "adapt\n",
            "add\n",
            "addit\n",
            "addit model\n",
            "addit nois\n",
            "address\n",
            "address issu\n",
            "address problem\n",
            "adequ\n",
            "adjac\n",
            "adjac matrix\n",
            "adjust\n",
            "admiss\n",
            "admit\n",
            "admm\n",
            "adopt\n",
            "adult\n",
            "advanc\n",
            "advanc neural\n",
            "advanc neural inform\n",
            "advantag\n",
            "adversari\n",
            "advertis\n",
            "advic\n",
            "affect\n",
            "affer\n",
            "affin\n",
            "agarw\n",
            "age\n",
            "agent\n",
            "aggreg\n",
            "aggress\n",
            "agnost\n",
            "agre\n",
            "agreement\n",
            "ahead\n",
            "aid\n",
            "aij\n",
            "aim\n",
            "aistat\n",
            "alarm\n",
            "alex\n",
            "alexand\n",
            "alg\n",
            "algebra\n",
            "algorithm\n",
            "algorithm achiev\n",
            "algorithm algorithm\n",
            "algorithm appli\n",
            "algorithm approxim\n",
            "algorithm base\n",
            "algorithm call\n",
            "algorithm compar\n",
            "algorithm comput\n",
            "algorithm converg\n",
            "algorithm describ\n",
            "algorithm estim\n",
            "algorithm find\n",
            "algorithm first\n",
            "algorithm follow\n",
            "algorithm gener\n",
            "algorithm given\n",
            "algorithm guarante\n",
            "algorithm implement\n",
            "algorithm iter\n",
            "algorithm learn\n",
            "algorithm optim\n",
            "algorithm perform\n",
            "algorithm present\n",
            "algorithm propos\n",
            "algorithm provid\n",
            "algorithm requir\n",
            "algorithm run\n",
            "algorithm section\n",
            "algorithm solv\n",
            "algorithm train\n",
            "algorithm use\n",
            "algorithm work\n",
            "align\n",
            "alloc\n",
            "allow\n",
            "almost\n",
            "almost sure\n",
            "alon\n",
            "along\n",
            "alpha\n",
            "alphabet\n",
            "alreadi\n",
            "alter\n",
            "altern\n",
            "altern approach\n",
            "although\n",
            "alway\n",
            "amari\n",
            "ambigu\n",
            "america\n",
            "american\n",
            "american statist\n",
            "american statist associ\n",
            "amino\n",
            "among\n",
            "amount\n",
            "amount data\n",
            "amplifi\n",
            "amplitud\n",
            "anal\n",
            "analog\n",
            "analog vlsi\n",
            "analogu\n",
            "analys\n",
            "analysi\n",
            "analysi machin\n",
            "analysi machin intellig\n",
            "analyt\n",
            "analyz\n",
            "anatom\n",
            "ancestor\n",
            "anchor\n",
            "anderson\n",
            "andrew\n",
            "angl\n",
            "angular\n",
            "anim\n",
            "ann\n",
            "annal\n",
            "annal statist\n",
            "anneal\n",
            "annot\n",
            "annual\n",
            "annual confer\n",
            "anomali\n",
            "anonym\n",
            "anoth\n",
            "answer\n",
            "answer question\n",
            "anti\n",
            "apart\n",
            "appar\n",
            "appeal\n",
            "appear\n",
            "appendix\n",
            "appl\n",
            "appli\n",
            "appli algorithm\n",
            "appli mathemat\n",
            "appli method\n",
            "applic\n",
            "approach\n",
            "approach base\n",
            "approach learn\n",
            "approach problem\n",
            "approach use\n",
            "appropri\n",
            "approxim\n",
            "approxim algorithm\n",
            "approxim error\n",
            "approxim infer\n",
            "approxim method\n",
            "approxim optim\n",
            "approxim posterior\n",
            "approxim solut\n",
            "approxim valu\n",
            "april\n",
            "arbitrari\n",
            "arbitrarili\n",
            "arc\n",
            "architectur\n",
            "ard\n",
            "area\n",
            "arg\n",
            "arg max\n",
            "arg min\n",
            "argmax\n",
            "argmin\n",
            "argu\n",
            "argument\n",
            "ari\n",
            "aris\n",
            "arithmet\n",
            "arm\n",
            "arm bandit\n",
            "around\n",
            "arrang\n",
            "array\n",
            "arriv\n",
            "arrow\n",
            "art\n",
            "articl\n",
            "articul\n",
            "artifact\n",
            "artifici\n",
            "artifici intellig\n",
            "artifici intellig statist\n",
            "artifici neural\n",
            "artifici neural network\n",
            "arxiv\n",
            "arxiv preprint\n",
            "arxiv preprint arxiv\n",
            "ascent\n",
            "ask\n",
            "aspect\n",
            "assembl\n",
            "assess\n",
            "assign\n",
            "assist\n",
            "associ\n",
            "associ memori\n",
            "assum\n",
            "assum independ\n",
            "assumpt\n",
            "asymmetr\n",
            "asymptot\n",
            "asynchron\n",
            "atom\n",
            "attach\n",
            "attack\n",
            "attain\n",
            "attempt\n",
            "attent\n",
            "attract\n",
            "attractor\n",
            "attribut\n",
            "auc\n",
            "audio\n",
            "auditori\n",
            "auer\n",
            "augment\n",
            "august\n",
            "author\n",
            "auto\n",
            "autocorrel\n",
            "autoencod\n",
            "autom\n",
            "automat\n",
            "automata\n",
            "autonom\n",
            "auxiliari\n",
            "auxiliari variabl\n",
            "avail\n",
            "avail http\n",
            "avail http www\n",
            "averag\n",
            "averag error\n",
            "averag number\n",
            "averag precis\n",
            "averag reward\n",
            "averag run\n",
            "avg\n",
            "avoid\n",
            "awar\n",
            "award\n",
            "away\n",
            "axe\n",
            "axi\n",
            "axon\n",
            "bach\n",
            "back\n",
            "back propag\n",
            "background\n",
            "backpropag\n",
            "backward\n",
            "bad\n",
            "bag\n",
            "bag word\n",
            "balanc\n",
            "ball\n",
            "band\n",
            "bandit\n",
            "bandit problem\n",
            "bandwidth\n",
            "bank\n",
            "bar\n",
            "barrier\n",
            "bartlett\n",
            "barto\n",
            "base\n",
            "base algorithm\n",
            "base approach\n",
            "base classifi\n",
            "base kernel\n",
            "base learn\n",
            "base method\n",
            "base model\n",
            "base upon\n",
            "baselin\n",
            "basi\n",
            "basi function\n",
            "basi vector\n",
            "basic\n",
            "basic idea\n",
            "batch\n",
            "bay\n",
            "bay rule\n",
            "bayesian\n",
            "bayesian approach\n",
            "bayesian framework\n",
            "bayesian infer\n",
            "bayesian learn\n",
            "bayesian method\n",
            "bayesian model\n",
            "bayesian network\n",
            "bci\n",
            "bear\n",
            "beat\n",
            "becker\n",
            "becom\n",
            "begin\n",
            "behav\n",
            "behavior\n",
            "behaviour\n",
            "behind\n",
            "belief\n",
            "belief network\n",
            "belief propag\n",
            "belief state\n",
            "believ\n",
            "bell\n",
            "bellman\n",
            "belong\n",
            "ben\n",
            "benchmark\n",
            "benefici\n",
            "benefit\n",
            "bengio\n",
            "berkeley\n",
            "berkeley edu\n",
            "berlin\n",
            "bernoulli\n",
            "bernstein\n",
            "bertseka\n",
            "besid\n",
            "best\n",
            "best knowledg\n",
            "best perform\n",
            "best result\n",
            "beta\n",
            "beth\n",
            "better\n",
            "better perform\n",
            "better result\n",
            "beyond\n",
            "bfg\n",
            "bia\n",
            "bia varianc\n",
            "bialek\n",
            "bianchi\n",
            "bias\n",
            "bic\n",
            "bifurc\n",
            "big\n",
            "bigram\n",
            "bij\n",
            "bilinear\n",
            "bin\n",
            "binari\n",
            "binari classif\n",
            "binari classifi\n",
            "binari variabl\n",
            "bind\n",
            "binocular\n",
            "binomi\n",
            "bioinformat\n",
            "biolog\n",
            "biolog cybernet\n",
            "biometrika\n",
            "biophys\n",
            "bipartit\n",
            "bipartit graph\n",
            "bird\n",
            "bishop\n",
            "bit\n",
            "black\n",
            "blei\n",
            "blind\n",
            "block\n",
            "block diagon\n",
            "blood\n",
            "blue\n",
            "blum\n",
            "blur\n",
            "board\n",
            "bodi\n",
            "bold\n",
            "boltzmann\n",
            "boltzmann machin\n",
            "bond\n",
            "book\n",
            "boolean\n",
            "boolean function\n",
            "boost\n",
            "boost algorithm\n",
            "bootstrap\n",
            "border\n",
            "borrow\n",
            "boston\n",
            "bottleneck\n",
            "bottom\n",
            "bottom row\n",
            "bottou\n",
            "bound\n",
            "bound box\n",
            "bound gener\n",
            "bound hold\n",
            "bound log\n",
            "bound number\n",
            "bound theorem\n",
            "boundari\n",
            "bow\n",
            "box\n",
            "boyd\n",
            "bracket\n",
            "brain\n",
            "branch\n",
            "break\n",
            "breast\n",
            "bregman\n",
            "bregman diverg\n",
            "bridg\n",
            "brief\n",
            "briefli\n",
            "bright\n",
            "bring\n",
            "broad\n",
            "broken\n",
            "brown\n",
            "budget\n",
            "buffer\n",
            "build\n",
            "build block\n",
            "built\n",
            "bump\n",
            "burg\n",
            "burst\n",
            "cach\n",
            "calcium\n",
            "calcul\n",
            "calibr\n",
            "california\n",
            "california berkeley\n",
            "call\n",
            "caltech\n",
            "cambridg\n",
            "cambridg mit\n",
            "cambridg mit press\n",
            "cambridg univers\n",
            "cambridg univers press\n",
            "camera\n",
            "canada\n",
            "cancel\n",
            "cancer\n",
            "cand\n",
            "candid\n",
            "canon\n",
            "capabl\n",
            "capac\n",
            "capacitor\n",
            "caption\n",
            "captur\n",
            "car\n",
            "card\n",
            "cardin\n",
            "care\n",
            "carlo\n",
            "carlo method\n",
            "carnegi\n",
            "carnegi mellon\n",
            "carnegi mellon univers\n",
            "carri\n",
            "cart\n",
            "cascad\n",
            "case\n",
            "cast\n",
            "cat\n",
            "categor\n",
            "categori\n",
            "cation\n",
            "cauchi\n",
            "caus\n",
            "causal\n",
            "cca\n",
            "cdf\n",
            "cell\n",
            "center\n",
            "centr\n",
            "central\n",
            "centroid\n",
            "certain\n",
            "certainli\n",
            "cesa\n",
            "cesa bianchi\n",
            "chain\n",
            "chain mont\n",
            "chain mont carlo\n",
            "challeng\n",
            "chanc\n",
            "chang\n",
            "chang point\n",
            "chang time\n",
            "channel\n",
            "chaotic\n",
            "chapel\n",
            "chapman\n",
            "chapman hall\n",
            "chapter\n",
            "charact\n",
            "character\n",
            "characterist\n",
            "charg\n",
            "check\n",
            "chemic\n",
            "chen\n",
            "chicago\n",
            "child\n",
            "children\n",
            "chines\n",
            "chip\n",
            "choic\n",
            "choos\n",
            "choos action\n",
            "chose\n",
            "chosen\n",
            "christoph\n",
            "chunk\n",
            "cient\n",
            "cifar\n",
            "cij\n",
            "circl\n",
            "circuit\n",
            "circuitri\n",
            "circular\n",
            "citi\n",
            "claim\n",
            "clamp\n",
            "clariti\n",
            "class\n",
            "class class\n",
            "class classif\n",
            "class function\n",
            "class label\n",
            "class model\n",
            "class problem\n",
            "classi\n",
            "classic\n",
            "classif\n",
            "classif accuraci\n",
            "classif algorithm\n",
            "classif error\n",
            "classif perform\n",
            "classif problem\n",
            "classif regress\n",
            "classif task\n",
            "classifi\n",
            "classifi train\n",
            "classifi use\n",
            "claus\n",
            "clean\n",
            "clear\n",
            "clearli\n",
            "click\n",
            "clinic\n",
            "clip\n",
            "cliqu\n",
            "clock\n",
            "close\n",
            "close form\n",
            "close form solut\n",
            "close relat\n",
            "closer\n",
            "closest\n",
            "cluster\n",
            "cluster algorithm\n",
            "cluster cluster\n",
            "cluster data\n",
            "cluster method\n",
            "cluster problem\n",
            "clutter\n",
            "cmo\n",
            "cmu\n",
            "cmu edu\n",
            "cnn\n",
            "co\n",
            "coalesc\n",
            "coars\n",
            "code\n",
            "codebook\n",
            "codeword\n",
            "coeffici\n",
            "cognit\n",
            "cognit scienc\n",
            "cohen\n",
            "coher\n",
            "coin\n",
            "coincid\n",
            "collabor\n",
            "collabor filter\n",
            "collaps\n",
            "collect\n",
            "colleg\n",
            "collis\n",
            "color\n",
            "colt\n",
            "columbia\n",
            "column\n",
            "column vector\n",
            "com\n",
            "com abstract\n",
            "combin\n",
            "combinatori\n",
            "combinatori optim\n",
            "come\n",
            "command\n",
            "comment\n",
            "committe\n",
            "common\n",
            "commonli\n",
            "commonli use\n",
            "commun\n",
            "comp\n",
            "compact\n",
            "compar\n",
            "compar algorithm\n",
            "compar method\n",
            "compar model\n",
            "compar perform\n",
            "compar result\n",
            "comparison\n",
            "compat\n",
            "compens\n",
            "compet\n",
            "competit\n",
            "compil\n",
            "complement\n",
            "complementari\n",
            "complet\n",
            "complex\n",
            "complex cell\n",
            "complex model\n",
            "complic\n",
            "compon\n",
            "compon analysi\n",
            "compos\n",
            "composit\n",
            "compound\n",
            "comprehens\n",
            "compress\n",
            "compress sens\n",
            "compris\n",
            "comput approxim\n",
            "comput biolog\n",
            "comput complex\n",
            "comput cost\n",
            "comput effici\n",
            "comput expens\n",
            "comput gradient\n",
            "comput graphic\n",
            "comput learn\n",
            "comput learn theori\n",
            "comput linguist\n",
            "comput model\n",
            "comput neural\n",
            "comput neurosci\n",
            "comput optim\n",
            "comput posterior\n",
            "comput scienc\n",
            "comput scienc depart\n",
            "comput scienc univers\n",
            "comput system\n",
            "comput time\n",
            "comput vision\n",
            "comput vision pattern\n",
            "con\n",
            "concaten\n",
            "concav\n",
            "concentr\n",
            "concept\n",
            "conceptu\n",
            "concern\n",
            "conclud\n",
            "conclus\n",
            "conclus paper\n",
            "conclus present\n",
            "concret\n",
            "concurr\n",
            "condit\n",
            "condit densiti\n",
            "condit distribut\n",
            "condit hold\n",
            "condit independ\n",
            "condit probabl\n",
            "condit random\n",
            "condit random field\n",
            "conduct\n",
            "cone\n",
            "conf\n",
            "confer\n",
            "confer artifici\n",
            "confer artifici intellig\n",
            "confer comput\n",
            "confer comput vision\n",
            "confer machin\n",
            "confer machin learn\n",
            "confer neural\n",
            "confer uncertainti\n",
            "confer uncertainti artifici\n",
            "confid\n",
            "confid interv\n",
            "configur\n",
            "confirm\n",
            "conflict\n",
            "conform\n",
            "confus\n",
            "conjectur\n",
            "conjug\n",
            "conjug gradient\n",
            "conjunct\n",
            "connect\n",
            "connect compon\n",
            "connect weight\n",
            "connectionist\n",
            "consecut\n",
            "consensu\n",
            "consequ\n",
            "conserv\n",
            "consid\n",
            "consid case\n",
            "consid follow\n",
            "consid gener\n",
            "consid problem\n",
            "consid set\n",
            "consider\n",
            "consist\n",
            "consist estim\n",
            "const\n",
            "constant\n",
            "constant depend\n",
            "constant factor\n",
            "constitu\n",
            "constitut\n",
            "constrain\n",
            "constrain optim\n",
            "constraint\n",
            "construct\n",
            "consum\n",
            "contact\n",
            "contain\n",
            "contamin\n",
            "content\n",
            "context\n",
            "contextu\n",
            "continu\n",
            "continu function\n",
            "continu state\n",
            "continu time\n",
            "continu variabl\n",
            "contour\n",
            "contract\n",
            "contradict\n",
            "contrari\n",
            "contrast\n",
            "contribut\n",
            "contribut paper\n",
            "control\n",
            "control problem\n",
            "conv\n",
            "conveni\n",
            "convent\n",
            "converg\n",
            "converg optim\n",
            "converg properti\n",
            "converg rate\n",
            "convers\n",
            "convert\n",
            "convex\n",
            "convex combin\n",
            "convex function\n",
            "convex hull\n",
            "convex loss\n",
            "convex optim\n",
            "convex optim problem\n",
            "convex problem\n",
            "convex program\n",
            "convex relax\n",
            "convex set\n",
            "convey\n",
            "convolut\n",
            "convolut neural\n",
            "convolut neural network\n",
            "cooper\n",
            "coordin\n",
            "coordin descent\n",
            "cope\n",
            "copi\n",
            "copula\n",
            "core\n",
            "coreset\n",
            "corner\n",
            "corollari\n",
            "corpora\n",
            "corpu\n",
            "corr\n",
            "correct\n",
            "correctli\n",
            "correl\n",
            "correl coeffici\n",
            "correl matrix\n",
            "correspond\n",
            "correspond differ\n",
            "corrupt\n",
            "cortex\n",
            "cortic\n",
            "cortic neuron\n",
            "cosin\n",
            "cost\n",
            "cost function\n",
            "costli\n",
            "could\n",
            "could use\n",
            "count\n",
            "count number\n",
            "counter\n",
            "counterpart\n",
            "coupl\n",
            "cours\n",
            "cov\n",
            "covari\n",
            "covari function\n",
            "covari matric\n",
            "covari matrix\n",
            "cover\n",
            "cover number\n",
            "coverag\n",
            "cpu\n",
            "cpu time\n",
            "creat\n",
            "crf\n",
            "cristianini\n",
            "criteria\n",
            "criterion\n",
            "critic\n",
            "cross\n",
            "cross correl\n",
            "cross valid\n",
            "crp\n",
            "crucial\n",
            "csp\n",
            "cube\n",
            "cubic\n",
            "cue\n",
            "cumul\n",
            "cun\n",
            "current\n",
            "current state\n",
            "curs\n",
            "curs dimension\n",
            "curv\n",
            "curvatur\n",
            "custom\n",
            "cut\n",
            "cvpr\n",
            "cybernet\n",
            "cycl\n",
            "cyclic\n",
            "dag\n",
            "damp\n",
            "daniel\n",
            "dark\n",
            "darpa\n",
            "dasgupta\n",
            "dash\n",
            "dash line\n",
            "data\n",
            "data analysi\n",
            "data avail\n",
            "data cluster\n",
            "data collect\n",
            "data consist\n",
            "data data\n",
            "data depend\n",
            "data distribut\n",
            "data driven\n",
            "data experi\n",
            "data gener\n",
            "data given\n",
            "data matrix\n",
            "data mine\n",
            "data model\n",
            "data point\n",
            "data sampl\n",
            "data set\n",
            "data space\n",
            "data structur\n",
            "data train\n",
            "data use\n",
            "data vector\n",
            "databas\n",
            "datapoint\n",
            "dataset\n",
            "dataset consist\n",
            "dataset contain\n",
            "dataset use\n",
            "date\n",
            "davi\n",
            "david\n",
            "day\n",
            "dayan\n",
            "dbm\n",
            "dbn\n",
            "deal\n",
            "dec\n",
            "decad\n",
            "decay\n",
            "decemb\n",
            "decid\n",
            "decis\n",
            "decis boundari\n",
            "decis function\n",
            "decis make\n",
            "decis problem\n",
            "decis process\n",
            "decis rule\n",
            "decis tree\n",
            "decod\n",
            "decompos\n",
            "decomposit\n",
            "deconvolut\n",
            "decorrel\n",
            "decoupl\n",
            "decreas\n",
            "deep\n",
            "deep belief\n",
            "deep learn\n",
            "deep network\n",
            "def\n",
            "default\n",
            "defin\n",
            "defin follow\n",
            "defin set\n",
            "definit\n",
            "deform\n",
            "deg\n",
            "degener\n",
            "degrad\n",
            "degre\n",
            "degre freedom\n",
            "delay\n",
            "delet\n",
            "delta\n",
            "demand\n",
            "demonstr\n",
            "demonstr effect\n",
            "dendrit\n",
            "denois\n",
            "denomin\n",
            "denot\n",
            "denot expect\n",
            "denot number\n",
            "denot set\n",
            "denot vector\n",
            "dens\n",
            "densiti\n",
            "densiti estim\n",
            "densiti function\n",
            "depart\n",
            "depart comput\n",
            "depart comput scienc\n",
            "depart electr\n",
            "depart statist\n",
            "depend\n",
            "depend number\n",
            "depend structur\n",
            "depict\n",
            "depress\n",
            "dept\n",
            "dept comput\n",
            "dept comput scienc\n",
            "depth\n",
            "der\n",
            "deriv\n",
            "deriv respect\n",
            "descend\n",
            "descent\n",
            "descent algorithm\n",
            "descent method\n",
            "describ\n",
            "describ algorithm\n",
            "describ section\n",
            "descript\n",
            "descriptor\n",
            "design\n",
            "design matrix\n",
            "desir\n",
            "despit\n",
            "det\n",
            "detail\n",
            "detect\n",
            "detector\n",
            "determin\n",
            "determin whether\n",
            "determinist\n",
            "develop\n",
            "deviat\n",
            "devic\n",
            "devis\n",
            "diag\n",
            "diagnosi\n",
            "diagon\n",
            "diagon element\n",
            "diagon matrix\n",
            "diagram\n",
            "diamet\n",
            "dictionari\n",
            "diego\n",
            "dietterich\n",
            "differ algorithm\n",
            "differ approach\n",
            "differ class\n",
            "differ learn\n",
            "differ method\n",
            "differ model\n",
            "differ number\n",
            "differ set\n",
            "differ time\n",
            "differ type\n",
            "differ valu\n",
            "differ way\n",
            "differenti\n",
            "differenti equat\n",
            "differenti privaci\n",
            "differenti privat\n",
            "difficult\n",
            "difficulti\n",
            "diffus\n",
            "digit\n",
            "dij\n",
            "dim\n",
            "dimens\n",
            "dimens reduct\n",
            "dimension\n",
            "dimension data\n",
            "dimension featur\n",
            "dimension featur space\n",
            "dimension input\n",
            "dimension problem\n",
            "dimension reduct\n",
            "dimension space\n",
            "dimension subspac\n",
            "dimension vector\n",
            "diminish\n",
            "direct\n",
            "directli\n",
            "dirichlet\n",
            "dirichlet alloc\n",
            "dirichlet distribut\n",
            "dirichlet prior\n",
            "dirichlet process\n",
            "disadvantag\n",
            "disagr\n",
            "discard\n",
            "discontinu\n",
            "discount\n",
            "discount factor\n",
            "discov\n",
            "discoveri\n",
            "discrep\n",
            "discret\n",
            "discret time\n",
            "discrimin\n",
            "discrimin analysi\n",
            "discrimin train\n",
            "discuss\n",
            "discuss paper\n",
            "discuss section\n",
            "diseas\n",
            "disjoint\n",
            "disk\n",
            "dispar\n",
            "displac\n",
            "display\n",
            "dissimilar\n",
            "distanc\n",
            "distanc function\n",
            "distanc measur\n",
            "distanc metric\n",
            "distant\n",
            "distinct\n",
            "distinguish\n",
            "distort\n",
            "distractor\n",
            "distribut\n",
            "distribut accord\n",
            "distribut data\n",
            "distribut defin\n",
            "distribut function\n",
            "distribut gaussian\n",
            "distribut gener\n",
            "distribut given\n",
            "distribut mean\n",
            "distribut model\n",
            "distribut paramet\n",
            "distribut represent\n",
            "distribut set\n",
            "distribut use\n",
            "disturb\n",
            "diverg\n",
            "divers\n",
            "divid\n",
            "divis\n",
            "dna\n",
            "document\n",
            "dog\n",
            "domain\n",
            "domain adapt\n",
            "domin\n",
            "done\n",
            "dot\n",
            "dot line\n",
            "dot product\n",
            "doubl\n",
            "dpp\n",
            "dramat\n",
            "drastic\n",
            "draw\n",
            "drawback\n",
            "drawn\n",
            "drift\n",
            "drive\n",
            "driven\n",
            "drop\n",
            "dropout\n",
            "dual\n",
            "dual problem\n",
            "dual variabl\n",
            "dualiti\n",
            "due\n",
            "due fact\n",
            "due space\n",
            "durat\n",
            "dynam\n",
            "dynam model\n",
            "dynam program\n",
            "dynam system\n",
            "earli\n",
            "earlier\n",
            "eas\n",
            "easi\n",
            "easi see\n",
            "easier\n",
            "easili\n",
            "eccv\n",
            "econom\n",
            "ed\n",
            "edg\n",
            "edg weight\n",
            "edinburgh\n",
            "edit\n",
            "editor\n",
            "editor advanc\n",
            "editor advanc neural\n",
            "edu\n",
            "edu abstract\n",
            "educ\n",
            "eec\n",
            "eeg\n",
            "effect\n",
            "efficaci\n",
            "effici\n",
            "effici algorithm\n",
            "effici comput\n",
            "effici learn\n",
            "effort\n",
            "eigen\n",
            "eigenfunct\n",
            "eigenvalu\n",
            "eigenvector\n",
            "eij\n",
            "either\n",
            "elast\n",
            "electr\n",
            "electr engin\n",
            "electrod\n",
            "electron\n",
            "eleg\n",
            "element\n",
            "elementari\n",
            "elev\n",
            "elicit\n",
            "elig\n",
            "elimin\n",
            "ellipt\n",
            "els\n",
            "elsewher\n",
            "email\n",
            "embed\n",
            "emerg\n",
            "emiss\n",
            "emit\n",
            "emphas\n",
            "empir\n",
            "empir distribut\n",
            "empir estim\n",
            "empir evalu\n",
            "empir result\n",
            "empir risk\n",
            "empir risk minim\n",
            "empir studi\n",
            "employ\n",
            "empti\n",
            "enabl\n",
            "encod\n",
            "encount\n",
            "encourag\n",
            "end\n",
            "end end\n",
            "energi\n",
            "energi function\n",
            "enforc\n",
            "eng\n",
            "engin\n",
            "engin univers\n",
            "english\n",
            "enhanc\n",
            "enjoy\n",
            "enough\n",
            "ensembl\n",
            "ensur\n",
            "enter\n",
            "entir\n",
            "entiti\n",
            "entri\n",
            "entropi\n",
            "enumer\n",
            "envelop\n",
            "environ\n",
            "environment\n",
            "episod\n",
            "epoch\n",
            "eq\n",
            "eqn\n",
            "equal\n",
            "equal number\n",
            "equat\n",
            "equilibria\n",
            "equilibrium\n",
            "equip\n",
            "equival\n",
            "equival class\n",
            "erd\n",
            "ergod\n",
            "eric\n",
            "erm\n",
            "err\n",
            "error\n",
            "error bar\n",
            "error bound\n",
            "error correct\n",
            "error estim\n",
            "error function\n",
            "error measur\n",
            "error rate\n",
            "error term\n",
            "error train\n",
            "escap\n",
            "especi\n",
            "essenti\n",
            "establish\n",
            "estim\n",
            "estim base\n",
            "estim error\n",
            "estim function\n",
            "estim gener\n",
            "estim given\n",
            "estim mean\n",
            "estim method\n",
            "estim model\n",
            "estim obtain\n",
            "estim paramet\n",
            "estim probabl\n",
            "estim problem\n",
            "estim use\n",
            "estim valu\n",
            "etc\n",
            "euclidean\n",
            "euclidean distanc\n",
            "euclidean space\n",
            "european\n",
            "european confer\n",
            "evalu\n",
            "evalu perform\n",
            "even\n",
            "even though\n",
            "event\n",
            "eventu\n",
            "ever\n",
            "everi\n",
            "everi time\n",
            "everywher\n",
            "evid\n",
            "evok\n",
            "evolut\n",
            "evolutionari\n",
            "evolv\n",
            "exact\n",
            "exact infer\n",
            "exactli\n",
            "examin\n",
            "exampl\n",
            "exampl consid\n",
            "exampl train\n",
            "exampl use\n",
            "exce\n",
            "exceed\n",
            "excel\n",
            "except\n",
            "excess\n",
            "exchang\n",
            "excit\n",
            "excitatori\n",
            "excitatori inhibitori\n",
            "exclud\n",
            "exclus\n",
            "execut\n",
            "exemplar\n",
            "exhaust\n",
            "exhibit\n",
            "exist\n",
            "exist algorithm\n",
            "exist constant\n",
            "exist method\n",
            "exp\n",
            "exp exp\n",
            "exp exp exp\n",
            "exp log\n",
            "expand\n",
            "expans\n",
            "expect\n",
            "expect loss\n",
            "expect maxim\n",
            "expect number\n",
            "expect propag\n",
            "expect reward\n",
            "expect valu\n",
            "expens\n",
            "experi\n",
            "experi compar\n",
            "experi demonstr\n",
            "experi perform\n",
            "experi section\n",
            "experi use\n",
            "experiment\n",
            "experiment data\n",
            "experiment result\n",
            "expert\n",
            "explain\n",
            "explan\n",
            "explicit\n",
            "explicitli\n",
            "exploit\n",
            "explor\n",
            "expon\n",
            "exponenti\n",
            "exponenti famili\n",
            "exponenti number\n",
            "express\n",
            "express term\n",
            "extend\n",
            "extens\n",
            "extent\n",
            "extern\n",
            "extra\n",
            "extract\n",
            "extrapol\n",
            "extrem\n",
            "eye\n",
            "eye movement\n",
            "fabric\n",
            "face\n",
            "face detect\n",
            "face imag\n",
            "face recognit\n",
            "facial\n",
            "facilit\n",
            "fact\n",
            "factor\n",
            "factor analysi\n",
            "factor graph\n",
            "factor model\n",
            "factori\n",
            "fail\n",
            "failur\n",
            "fair\n",
            "fairli\n",
            "fall\n",
            "fals\n",
            "fals alarm\n",
            "fals posit\n",
            "fals posit rate\n",
            "famili\n",
            "famili distribut\n",
            "familiar\n",
            "fan\n",
            "far\n",
            "fashion\n",
            "fast\n",
            "faster\n",
            "fault\n",
            "favor\n",
            "feasibl\n",
            "featur\n",
            "featur base\n",
            "featur extract\n",
            "featur learn\n",
            "featur map\n",
            "featur model\n",
            "featur represent\n",
            "featur select\n",
            "featur set\n",
            "featur space\n",
            "featur use\n",
            "featur valu\n",
            "featur vector\n",
            "fed\n",
            "feed\n",
            "feed forward\n",
            "feedback\n",
            "feedforward\n",
            "fei\n",
            "fellowship\n",
            "femal\n",
            "fewer\n",
            "fiber\n",
            "field\n",
            "field approxim\n",
            "fifth\n",
            "figur\n",
            "file\n",
            "fill\n",
            "filter\n",
            "final\n",
            "financi\n",
            "find\n",
            "find best\n",
            "find good\n",
            "find optim\n",
            "fine\n",
            "fine tune\n",
            "finger\n",
            "finit\n",
            "finit dimension\n",
            "finit number\n",
            "finit set\n",
            "finit state\n",
            "fire\n",
            "fire neuron\n",
            "fire rate\n",
            "first\n",
            "first consid\n",
            "first experi\n",
            "first layer\n",
            "first order\n",
            "first second\n",
            "first set\n",
            "first stage\n",
            "first step\n",
            "first term\n",
            "firstli\n",
            "fisher\n",
            "fisher inform\n",
            "fit\n",
            "fit data\n",
            "fit model\n",
            "fix\n",
            "fix number\n",
            "fix point\n",
            "fix valu\n",
            "fixat\n",
            "flat\n",
            "flexibl\n",
            "fli\n",
            "flip\n",
            "float\n",
            "flow\n",
            "fluctuat\n",
            "fmri\n",
            "focu\n",
            "focus\n",
            "fold\n",
            "fold cross\n",
            "fold cross valid\n",
            "follow\n",
            "follow condit\n",
            "follow definit\n",
            "follow first\n",
            "follow form\n",
            "follow hold\n",
            "follow lemma\n",
            "follow optim\n",
            "follow proposit\n",
            "follow section\n",
            "follow step\n",
            "follow theorem\n",
            "food\n",
            "forc\n",
            "forecast\n",
            "foreground\n",
            "forest\n",
            "form\n",
            "form solut\n",
            "formal\n",
            "format\n",
            "former\n",
            "formul\n",
            "formula\n",
            "fortun\n",
            "forward\n",
            "forward backward\n",
            "found\n",
            "foundat\n",
            "fourier\n",
            "fourier transform\n",
            "fourth\n",
            "fraction\n",
            "fragment\n",
            "frame\n",
            "framework\n",
            "franc\n",
            "francisco\n",
            "frank\n",
            "free\n",
            "free energi\n",
            "free paramet\n",
            "freedom\n",
            "freeman\n",
            "frequenc\n",
            "frequent\n",
            "freund\n",
            "frey\n",
            "friedman\n",
            "friend\n",
            "frobeniu\n",
            "frobeniu norm\n",
            "front\n",
            "frontal\n",
            "fulfil\n",
            "full\n",
            "full rank\n",
            "fulli\n",
            "fulli connect\n",
            "function approxim\n",
            "function base\n",
            "function class\n",
            "function comput\n",
            "function convex\n",
            "function correspond\n",
            "function defin\n",
            "function depend\n",
            "function estim\n",
            "function evalu\n",
            "function follow\n",
            "function form\n",
            "function function\n",
            "function gener\n",
            "function given\n",
            "function input\n",
            "function learn\n",
            "function let\n",
            "function linear\n",
            "function log\n",
            "function map\n",
            "function minim\n",
            "function model\n",
            "function number\n",
            "function optim\n",
            "function paramet\n",
            "function repres\n",
            "function respect\n",
            "function satisfi\n",
            "function set\n",
            "function space\n",
            "function time\n",
            "function use\n",
            "function valu\n",
            "fund\n",
            "fundament\n",
            "furthermor\n",
            "fusion\n",
            "futur\n",
            "futur research\n",
            "futur work\n",
            "gabor\n",
            "gain\n",
            "game\n",
            "gamma\n",
            "ganglion\n",
            "ganglion cell\n",
            "gap\n",
            "gate\n",
            "gather\n",
            "gatsbi\n",
            "gauss\n",
            "gaussian\n",
            "gaussian approxim\n",
            "gaussian distribut\n",
            "gaussian kernel\n",
            "gaussian mixtur\n",
            "gaussian mixtur model\n",
            "gaussian model\n",
            "gaussian nois\n",
            "gaussian prior\n",
            "gaussian process\n",
            "gaussian process regress\n",
            "gaussian random\n",
            "gave\n",
            "gaze\n",
            "geman\n",
            "gene\n",
            "gene express\n",
            "gener algorithm\n",
            "gener approach\n",
            "gener assum\n",
            "gener bound\n",
            "gener case\n",
            "gener data\n",
            "gener distribut\n",
            "gener error\n",
            "gener framework\n",
            "gener function\n",
            "gener linear\n",
            "gener linear model\n",
            "gener method\n",
            "gener model\n",
            "gener perform\n",
            "gener problem\n",
            "gener process\n",
            "gener random\n",
            "gener sampl\n",
            "gener set\n",
            "generalis\n",
            "genet\n",
            "genom\n",
            "geodes\n",
            "geoffrey\n",
            "geometr\n",
            "geometri\n",
            "german\n",
            "germani\n",
            "get\n",
            "ghahramani\n",
            "ghz\n",
            "gibb\n",
            "gibb sampl\n",
            "gibb sampler\n",
            "give\n",
            "give rise\n",
            "given\n",
            "given data\n",
            "given equat\n",
            "given input\n",
            "given observ\n",
            "given set\n",
            "given train\n",
            "glass\n",
            "glm\n",
            "global\n",
            "global minimum\n",
            "global optim\n",
            "gmm\n",
            "go\n",
            "goal\n",
            "goal find\n",
            "goe\n",
            "gold\n",
            "good\n",
            "good approxim\n",
            "good perform\n",
            "good result\n",
            "googl\n",
            "gordon\n",
            "govern\n",
            "gp\n",
            "gradient\n",
            "gradient algorithm\n",
            "gradient ascent\n",
            "gradient base\n",
            "gradient descent\n",
            "gradient estim\n",
            "gradient method\n",
            "gradual\n",
            "gram\n",
            "gram matrix\n",
            "grammar\n",
            "grammat\n",
            "grant\n",
            "graph\n",
            "graph base\n",
            "graph cut\n",
            "graph laplacian\n",
            "graph node\n",
            "graph structur\n",
            "graphic\n",
            "graphic model\n",
            "grate\n",
            "gray\n",
            "great\n",
            "greater\n",
            "greatli\n",
            "greedi\n",
            "greedi algorithm\n",
            "green\n",
            "gretton\n",
            "grey\n",
            "grid\n",
            "griffith\n",
            "ground\n",
            "ground truth\n",
            "group\n",
            "group lasso\n",
            "grow\n",
            "growth\n",
            "guarante\n",
            "guarante converg\n",
            "guess\n",
            "guid\n",
            "half\n",
            "hall\n",
            "ham\n",
            "ham distanc\n",
            "han\n",
            "hand\n",
            "hand side\n",
            "handl\n",
            "handwritten\n",
            "handwritten digit\n",
            "happen\n",
            "hard\n",
            "harder\n",
            "hardwar\n",
            "harmon\n",
            "hash\n",
            "hash function\n",
            "hast\n",
            "hasti\n",
            "hasti tibshirani\n",
            "haussler\n",
            "hdp\n",
            "head\n",
            "heart\n",
            "heavi\n",
            "heavi tail\n",
            "heavili\n",
            "hebb\n",
            "hebbian\n",
            "hebbian learn\n",
            "hedg\n",
            "height\n",
            "held\n",
            "helicopt\n",
            "help\n",
            "help discuss\n",
            "henc\n",
            "hessian\n",
            "heterogen\n",
            "heurist\n",
            "hidden\n",
            "hidden layer\n",
            "hidden markov\n",
            "hidden markov model\n",
            "hidden node\n",
            "hidden state\n",
            "hidden unit\n",
            "hidden variabl\n",
            "hierarch\n",
            "hierarch cluster\n",
            "hierarch model\n",
            "hierarchi\n",
            "high\n",
            "high dimens\n",
            "high dimension\n",
            "high dimension data\n",
            "high dimension space\n",
            "high frequenc\n",
            "high level\n",
            "high order\n",
            "high probabl\n",
            "high resolut\n",
            "higher\n",
            "higher level\n",
            "higher order\n",
            "highest\n",
            "highli\n",
            "highlight\n",
            "hilbert\n",
            "hilbert space\n",
            "hill\n",
            "hing\n",
            "hing loss\n",
            "hinton\n",
            "hippocamp\n",
            "hippocampu\n",
            "histogram\n",
            "histor\n",
            "histori\n",
            "hit\n",
            "hmc\n",
            "hmm\n",
            "hoc\n",
            "hofmann\n",
            "hog\n",
            "hold\n",
            "homogen\n",
            "hope\n",
            "hopfield\n",
            "horizon\n",
            "horizont\n",
            "hour\n",
            "hous\n",
            "howard\n",
            "howev\n",
            "howev sinc\n",
            "html\n",
            "http\n",
            "http www\n",
            "huang\n",
            "huge\n",
            "hull\n",
            "human\n",
            "human subject\n",
            "hundr\n",
            "hybrid\n",
            "hyper\n",
            "hyper paramet\n",
            "hypercub\n",
            "hypergraph\n",
            "hyperparamet\n",
            "hyperplan\n",
            "hypothes\n",
            "hypothesi\n",
            "hypothesi class\n",
            "hypothesi space\n",
            "hypothesi test\n",
            "ibm\n",
            "ibp\n",
            "ic\n",
            "ica\n",
            "iccv\n",
            "icml\n",
            "icml page\n",
            "idea\n",
            "ideal\n",
            "ident\n",
            "ident distribut\n",
            "ident matrix\n",
            "identif\n",
            "identifi\n",
            "ieee\n",
            "ieee confer\n",
            "ieee intern\n",
            "ieee intern confer\n",
            "ieee tran\n",
            "ieee transact\n",
            "ieee transact inform\n",
            "ieee transact pattern\n",
            "iff\n",
            "ignor\n",
            "ii\n",
            "iid\n",
            "iii\n",
            "ill\n",
            "illumin\n",
            "illustr\n",
            "imag\n",
            "imagenet\n",
            "imagin\n",
            "imit\n",
            "immedi\n",
            "impact\n",
            "implement\n",
            "impli\n",
            "implic\n",
            "implicit\n",
            "implicitli\n",
            "import\n",
            "import role\n",
            "import sampl\n",
            "import weight\n",
            "importantli\n",
            "impos\n",
            "imposs\n",
            "impract\n",
            "impress\n",
            "improv\n",
            "improv accuraci\n",
            "improv gener\n",
            "improv perform\n",
            "impuls\n",
            "imput\n",
            "inact\n",
            "inc\n",
            "includ\n",
            "inclus\n",
            "incoher\n",
            "incom\n",
            "incomplet\n",
            "inconsist\n",
            "incorpor\n",
            "incorrect\n",
            "incorrectli\n",
            "increas\n",
            "increas number\n",
            "increasingli\n",
            "increment\n",
            "incur\n",
            "inde\n",
            "independ\n",
            "independ assumpt\n",
            "independ compon\n",
            "independ compon analysi\n",
            "independ given\n",
            "index\n",
            "index set\n",
            "indian\n",
            "indic\n",
            "indic function\n",
            "individu\n",
            "induc\n",
            "induct\n",
            "industri\n",
            "ineffici\n",
            "inequ\n",
            "inf\n",
            "infeas\n",
            "infer\n",
            "infer algorithm\n",
            "infer learn\n",
            "infer method\n",
            "infer model\n",
            "infer problem\n",
            "inferior\n",
            "infin\n",
            "infinit\n",
            "infinit dimension\n",
            "influenc\n",
            "info\n",
            "inform\n",
            "inform gain\n",
            "inform matrix\n",
            "inform process\n",
            "inform process system\n",
            "inform retriev\n",
            "inform theoret\n",
            "inform theori\n",
            "ing\n",
            "inher\n",
            "inhibit\n",
            "inhibitori\n",
            "initi\n",
            "initi condit\n",
            "initi set\n",
            "initi state\n",
            "initi valu\n",
            "initialis\n",
            "inject\n",
            "inner\n",
            "inner product\n",
            "innov\n",
            "input\n",
            "input data\n",
            "input dimens\n",
            "input featur\n",
            "input layer\n",
            "input neuron\n",
            "input output\n",
            "input pattern\n",
            "input signal\n",
            "input space\n",
            "input spike\n",
            "input unit\n",
            "input variabl\n",
            "input vector\n",
            "inria\n",
            "insensit\n",
            "insert\n",
            "insid\n",
            "insight\n",
            "inspect\n",
            "inspir\n",
            "instabl\n",
            "instanc\n",
            "instantan\n",
            "instanti\n",
            "instead\n",
            "institut\n",
            "institut technolog\n",
            "instruct\n",
            "instrument\n",
            "int\n",
            "int conf\n",
            "integ\n",
            "integr\n",
            "integr fire\n",
            "intel\n",
            "intellig\n",
            "intellig page\n",
            "intellig statist\n",
            "intellig system\n",
            "intend\n",
            "intens\n",
            "intent\n",
            "inter\n",
            "interact\n",
            "interconnect\n",
            "interest\n",
            "interestingli\n",
            "interfac\n",
            "interfer\n",
            "interior\n",
            "interior point\n",
            "interleav\n",
            "intermedi\n",
            "intern\n",
            "intern confer\n",
            "intern confer machin\n",
            "intern joint\n",
            "intern joint confer\n",
            "intern journal\n",
            "intern node\n",
            "intern represent\n",
            "internet\n",
            "interneuron\n",
            "interpol\n",
            "interpret\n",
            "intersect\n",
            "interv\n",
            "intra\n",
            "intract\n",
            "intrins\n",
            "introduc\n",
            "introduc new\n",
            "introduct\n",
            "intuit\n",
            "invari\n",
            "invers\n",
            "invers covari\n",
            "invers problem\n",
            "invert\n",
            "invest\n",
            "investig\n",
            "involv\n",
            "ion\n",
            "irregular\n",
            "irrelev\n",
            "ise\n",
            "ise model\n",
            "isi\n",
            "isol\n",
            "isomap\n",
            "isotrop\n",
            "israel\n",
            "issu\n",
            "item\n",
            "iter\n",
            "iter algorithm\n",
            "iter iter\n",
            "ith\n",
            "jaakkola\n",
            "jacob\n",
            "jacobian\n",
            "jain\n",
            "jame\n",
            "jan\n",
            "januari\n",
            "japan\n",
            "jensen\n",
            "jmlr\n",
            "joachim\n",
            "job\n",
            "john\n",
            "john wiley\n",
            "john wiley son\n",
            "johnson\n",
            "joint\n",
            "joint confer\n",
            "joint distribut\n",
            "joint probabl\n",
            "jointli\n",
            "jone\n",
            "jordan\n",
            "journal\n",
            "journal american\n",
            "journal american statist\n",
            "journal comput\n",
            "journal machin\n",
            "journal machin learn\n",
            "journal neurosci\n",
            "journal royal\n",
            "journal royal statist\n",
            "jth\n",
            "judg\n",
            "judgment\n",
            "juli\n",
            "jump\n",
            "junction\n",
            "junction tree\n",
            "june\n",
            "justif\n",
            "justifi\n",
            "kakad\n",
            "kalman\n",
            "kalman filter\n",
            "kaufmann\n",
            "kdd\n",
            "kde\n",
            "kearn\n",
            "keep\n",
            "kept\n",
            "kernel\n",
            "kernel base\n",
            "kernel densiti\n",
            "kernel densiti estim\n",
            "kernel function\n",
            "kernel hilbert\n",
            "kernel hilbert space\n",
            "kernel learn\n",
            "kernel machin\n",
            "kernel matrix\n",
            "kernel method\n",
            "kernel pca\n",
            "kernel use\n",
            "kernel width\n",
            "key\n",
            "khz\n",
            "kij\n",
            "kim\n",
            "kind\n",
            "kinemat\n",
            "kluwer\n",
            "knn\n",
            "know\n",
            "knowledg\n",
            "knowledg discoveri\n",
            "known\n",
            "koch\n",
            "kohonen\n",
            "koller\n",
            "kolmogorov\n",
            "kroneck\n",
            "kth\n",
            "kullback\n",
            "kullback leibler\n",
            "kullback leibler diverg\n",
            "kumar\n",
            "kwk\n",
            "kxi\n",
            "kxk\n",
            "lab\n",
            "label\n",
            "label data\n",
            "label exampl\n",
            "label point\n",
            "label set\n",
            "label train\n",
            "label unlabel\n",
            "laboratori\n",
            "lack\n",
            "lafferti\n",
            "lag\n",
            "lagrang\n",
            "lagrang multipli\n",
            "lagrangian\n",
            "landmark\n",
            "lang\n",
            "langford\n",
            "languag\n",
            "languag model\n",
            "languag process\n",
            "laplac\n",
            "laplacian\n",
            "lar\n",
            "larg\n",
            "larger\n",
            "larger number\n",
            "largest\n",
            "largest eigenvalu\n",
            "lasso\n",
            "last\n",
            "latenc\n",
            "latent\n",
            "latent dirichlet\n",
            "latent dirichlet alloc\n",
            "latent factor\n",
            "latent featur\n",
            "latent space\n",
            "latent variabl\n",
            "latent variabl model\n",
            "later\n",
            "latter\n",
            "lattic\n",
            "law\n",
            "lawrenc\n",
            "layer\n",
            "layer network\n",
            "layer perceptron\n",
            "layout\n",
            "lbp\n",
            "lda\n",
            "lead\n",
            "leaf\n",
            "leaf node\n",
            "learn\n",
            "learn algorithm\n",
            "learn applic\n",
            "learn approach\n",
            "learn classifi\n",
            "learn curv\n",
            "learn data\n",
            "learn featur\n",
            "learn framework\n",
            "learn function\n",
            "learn gener\n",
            "learn icml\n",
            "learn kernel\n",
            "learn learn\n",
            "learn linear\n",
            "learn machin\n",
            "learn method\n",
            "learn model\n",
            "learn multipl\n",
            "learn neural\n",
            "learn optim\n",
            "learn page\n",
            "learn paramet\n",
            "learn perform\n",
            "learn polici\n",
            "learn predict\n",
            "learn problem\n",
            "learn procedur\n",
            "learn proceed\n",
            "learn process\n",
            "learn rank\n",
            "learn rate\n",
            "learn research\n",
            "learn rule\n",
            "learn set\n",
            "learn spars\n",
            "learn structur\n",
            "learn system\n",
            "learn task\n",
            "learn techniqu\n",
            "learn theori\n",
            "learn time\n",
            "learn use\n",
            "learnabl\n",
            "learner\n",
            "learnt\n",
            "least\n",
            "least squar\n",
            "leav\n",
            "lectur\n",
            "lectur note\n",
            "lecun\n",
            "led\n",
            "lee\n",
            "leen\n",
            "left\n",
            "left hand\n",
            "left panel\n",
            "left right\n",
            "leg\n",
            "leibler\n",
            "leibler diverg\n",
            "lemma\n",
            "lemma lemma\n",
            "lemma let\n",
            "length\n",
            "less\n",
            "let\n",
            "let assum\n",
            "let consid\n",
            "let defin\n",
            "let denot\n",
            "let let\n",
            "let set\n",
            "letter\n",
            "level\n",
            "level featur\n",
            "level set\n",
            "leverag\n",
            "librari\n",
            "lie\n",
            "life\n",
            "lift\n",
            "light\n",
            "like\n",
            "like thank\n",
            "likelihood\n",
            "likelihood estim\n",
            "likelihood function\n",
            "likelihood model\n",
            "likelihood ratio\n",
            "likewis\n",
            "lim\n",
            "limit\n",
            "limit number\n",
            "lin\n",
            "line\n",
            "line learn\n",
            "line search\n",
            "linear\n",
            "linear classifi\n",
            "linear combin\n",
            "linear constraint\n",
            "linear discrimin\n",
            "linear dynam\n",
            "linear filter\n",
            "linear function\n",
            "linear gaussian\n",
            "linear kernel\n",
            "linear model\n",
            "linear oper\n",
            "linear program\n",
            "linear regress\n",
            "linear svm\n",
            "linear system\n",
            "linear threshold\n",
            "linear time\n",
            "linear transform\n",
            "linearli\n",
            "linguist\n",
            "link\n",
            "linkag\n",
            "lipschitz\n",
            "list\n",
            "listen\n",
            "literatur\n",
            "littl\n",
            "littman\n",
            "liu\n",
            "live\n",
            "lle\n",
            "lmnn\n",
            "lo\n",
            "load\n",
            "local\n",
            "local featur\n",
            "local linear\n",
            "local minima\n",
            "local minimum\n",
            "local model\n",
            "local optim\n",
            "local optima\n",
            "locat\n",
            "lock\n",
            "log\n",
            "log det\n",
            "log exp\n",
            "log likelihood\n",
            "log linear\n",
            "log log\n",
            "log log log\n",
            "log partit\n",
            "log partit function\n",
            "log probabl\n",
            "log scale\n",
            "logarithm\n",
            "logic\n",
            "logist\n",
            "logist regress\n",
            "london\n",
            "long\n",
            "long rang\n",
            "long term\n",
            "longer\n",
            "look\n",
            "loop\n",
            "loopi\n",
            "loopi belief\n",
            "loopi belief propag\n",
            "loos\n",
            "lose\n",
            "loss\n",
            "loss function\n",
            "loss gener\n",
            "lost\n",
            "lot\n",
            "low\n",
            "low dimension\n",
            "low level\n",
            "low rank\n",
            "low rank matrix\n",
            "lower\n",
            "lower bound\n",
            "lower dimension\n",
            "lowest\n",
            "lsh\n",
            "lsi\n",
            "lstd\n",
            "ltd\n",
            "ltp\n",
            "lugosi\n",
            "maass\n",
            "macaqu\n",
            "mach\n",
            "mach learn\n",
            "machin\n",
            "machin intellig\n",
            "machin learn\n",
            "machin learn icml\n",
            "machin learn page\n",
            "machin learn research\n",
            "machin svm\n",
            "mackay\n",
            "macro\n",
            "made\n",
            "magnet\n",
            "magnitud\n",
            "mahalanobi\n",
            "mail\n",
            "main\n",
            "main contribut\n",
            "main idea\n",
            "main result\n",
            "mainli\n",
            "maintain\n",
            "major\n",
            "major vote\n",
            "make\n",
            "make assumpt\n",
            "make predict\n",
            "make sens\n",
            "make use\n",
            "male\n",
            "malik\n",
            "man\n",
            "manag\n",
            "mani\n",
            "mani applic\n",
            "mani case\n",
            "mani differ\n",
            "manifold\n",
            "manifold learn\n",
            "manipul\n",
            "manner\n",
            "manual\n",
            "map\n",
            "map estim\n",
            "map infer\n",
            "mar\n",
            "march\n",
            "margin\n",
            "margin distribut\n",
            "margin likelihood\n",
            "margin probabl\n",
            "mark\n",
            "marker\n",
            "market\n",
            "markov\n",
            "markov chain\n",
            "markov chain mont\n",
            "markov decis\n",
            "markov decis process\n",
            "markov model\n",
            "markov network\n",
            "markov process\n",
            "markov random\n",
            "markov random field\n",
            "markovian\n",
            "martin\n",
            "mask\n",
            "mass\n",
            "massachusett\n",
            "massachusett institut\n",
            "massachusett institut technolog\n",
            "massiv\n",
            "master\n",
            "match\n",
            "match pursuit\n",
            "mateo\n",
            "materi\n",
            "math\n",
            "mathemat\n",
            "mathemat program\n",
            "matlab\n",
            "matric\n",
            "matrix\n",
            "matrix complet\n",
            "matrix defin\n",
            "matrix factor\n",
            "matrix invers\n",
            "matrix vector\n",
            "matrix whose\n",
            "matter\n",
            "max\n",
            "max log\n",
            "max margin\n",
            "max max\n",
            "max min\n",
            "max product\n",
            "maxi\n",
            "maxim\n",
            "maxim expect\n",
            "maxim margin\n",
            "maxima\n",
            "maximis\n",
            "maximum\n",
            "maximum entropi\n",
            "maximum likelihood\n",
            "maximum likelihood estim\n",
            "maximum margin\n",
            "maximum posteriori\n",
            "may\n",
            "may use\n",
            "mccallum\n",
            "mcclelland\n",
            "mcmc\n",
            "md\n",
            "mdl\n",
            "mdp\n",
            "mean\n",
            "mean algorithm\n",
            "mean cluster\n",
            "mean covari\n",
            "mean field\n",
            "mean field approxim\n",
            "mean gaussian\n",
            "mean squar\n",
            "mean squar error\n",
            "mean standard\n",
            "mean standard deviat\n",
            "mean valu\n",
            "mean varianc\n",
            "meaning\n",
            "measur\n",
            "measur perform\n",
            "mechan\n",
            "median\n",
            "medic\n",
            "medium\n",
            "meet\n",
            "meg\n",
            "mellon\n",
            "mellon univers\n",
            "member\n",
            "membership\n",
            "membran\n",
            "membran potenti\n",
            "memor\n",
            "memori\n",
            "mental\n",
            "mention\n",
            "mercer\n",
            "mere\n",
            "merg\n",
            "mesh\n",
            "messag\n",
            "messag pass\n",
            "messag pass algorithm\n",
            "met\n",
            "meta\n",
            "method\n",
            "method appli\n",
            "method base\n",
            "method compar\n",
            "method comput\n",
            "method describ\n",
            "method estim\n",
            "method find\n",
            "method gener\n",
            "method learn\n",
            "method perform\n",
            "method propos\n",
            "method requir\n",
            "method solv\n",
            "method use\n",
            "methodolog\n",
            "metric\n",
            "metric learn\n",
            "metric space\n",
            "metropoli\n",
            "metropoli hast\n",
            "michael\n",
            "michael jordan\n",
            "microarray\n",
            "microsoft\n",
            "microsoft research\n",
            "middl\n",
            "might\n",
            "mij\n",
            "mil\n",
            "mild\n",
            "miller\n",
            "million\n",
            "mimic\n",
            "min\n",
            "min log\n",
            "min max\n",
            "min min\n",
            "mind\n",
            "mine\n",
            "mini\n",
            "mini batch\n",
            "minim\n",
            "minim problem\n",
            "minima\n",
            "minimax\n",
            "minimis\n",
            "minimum\n",
            "minka\n",
            "minor\n",
            "minu\n",
            "minut\n",
            "mirror\n",
            "misclassif\n",
            "misclassifi\n",
            "mismatch\n",
            "miss\n",
            "miss data\n",
            "miss valu\n",
            "mistak\n",
            "mistak bound\n",
            "mit\n",
            "mit edu\n",
            "mit press\n",
            "mit press cambridg\n",
            "mix\n",
            "mixtur\n",
            "mixtur compon\n",
            "mixtur expert\n",
            "mixtur gaussian\n",
            "mixtur model\n",
            "mkl\n",
            "mle\n",
            "mln\n",
            "mlp\n",
            "mmd\n",
            "mnist\n",
            "mobil\n",
            "modal\n",
            "mode\n",
            "model\n",
            "model allow\n",
            "model appli\n",
            "model approach\n",
            "model assum\n",
            "model base\n",
            "model captur\n",
            "model class\n",
            "model combin\n",
            "model complex\n",
            "model comput\n",
            "model condit\n",
            "model consid\n",
            "model consist\n",
            "model data\n",
            "model defin\n",
            "model describ\n",
            "model differ\n",
            "model distribut\n",
            "model dynam\n",
            "model estim\n",
            "model first\n",
            "model fit\n",
            "model gaussian\n",
            "model gener\n",
            "model given\n",
            "model howev\n",
            "model human\n",
            "model includ\n",
            "model infer\n",
            "model learn\n",
            "model linear\n",
            "model make\n",
            "model may\n",
            "model model\n",
            "model natur\n",
            "model neural\n",
            "model neuron\n",
            "model object\n",
            "model observ\n",
            "model order\n",
            "model paramet\n",
            "model perform\n",
            "model predict\n",
            "model present\n",
            "model propos\n",
            "model provid\n",
            "model repres\n",
            "model result\n",
            "model section\n",
            "model select\n",
            "model set\n",
            "model shown\n",
            "model similar\n",
            "model structur\n",
            "model time\n",
            "model train\n",
            "model use\n",
            "moder\n",
            "modern\n",
            "modif\n",
            "modifi\n",
            "modul\n",
            "modular\n",
            "molecul\n",
            "molecular\n",
            "moment\n",
            "momentum\n",
            "monitor\n",
            "monkey\n",
            "monocular\n",
            "monoton\n",
            "mont\n",
            "mont carlo\n",
            "mont carlo method\n",
            "month\n",
            "moodi\n",
            "moor\n",
            "moreov\n",
            "morgan\n",
            "morgan kaufmann\n",
            "morpholog\n",
            "mostli\n",
            "motif\n",
            "motion\n",
            "motiv\n",
            "motor\n",
            "mous\n",
            "move\n",
            "movement\n",
            "movi\n",
            "mozer\n",
            "mrf\n",
            "mri\n",
            "mse\n",
            "msec\n",
            "mtl\n",
            "much\n",
            "much better\n",
            "much faster\n",
            "much larger\n",
            "much less\n",
            "much smaller\n",
            "multi\n",
            "multi arm\n",
            "multi arm bandit\n",
            "multi class\n",
            "multi label\n",
            "multi layer\n",
            "multi scale\n",
            "multi task\n",
            "multi task learn\n",
            "multi view\n",
            "multiclass\n",
            "multidimension\n",
            "multilay\n",
            "multimod\n",
            "multinomi\n",
            "multinomi distribut\n",
            "multipl\n",
            "multipl instanc\n",
            "multipl kernel\n",
            "multipl kernel learn\n",
            "multipli\n",
            "multiscal\n",
            "multitask\n",
            "multivari\n",
            "multivari gaussian\n",
            "muno\n",
            "murphi\n",
            "murray\n",
            "muscl\n",
            "music\n",
            "must\n",
            "mutat\n",
            "mutual\n",
            "mutual inform\n",
            "naiv\n",
            "naiv bay\n",
            "name\n",
            "narrow\n",
            "nash\n",
            "nash equilibrium\n",
            "nat\n",
            "nation\n",
            "nation scienc\n",
            "nation scienc foundat\n",
            "natur\n",
            "natur gradient\n",
            "natur imag\n",
            "natur languag\n",
            "natur languag process\n",
            "natur neurosci\n",
            "natur scene\n",
            "navig\n",
            "ndcg\n",
            "neal\n",
            "near\n",
            "near optim\n",
            "nearbi\n",
            "nearest\n",
            "nearest neighbor\n",
            "nearli\n",
            "necessari\n",
            "necessarili\n",
            "ned\n",
            "need\n",
            "need comput\n",
            "neg\n",
            "neg exampl\n",
            "neg log\n",
            "neg log likelihood\n",
            "neglect\n",
            "neglig\n",
            "neighbor\n",
            "neighborhood\n",
            "neighbour\n",
            "neither\n",
            "nerv\n",
            "nervou\n",
            "nervou system\n",
            "nest\n",
            "nesterov\n",
            "net\n",
            "network\n",
            "network architectur\n",
            "network comput\n",
            "network gener\n",
            "network hidden\n",
            "network input\n",
            "network learn\n",
            "network model\n",
            "network output\n",
            "network perform\n",
            "network structur\n",
            "network train\n",
            "network use\n",
            "neural\n",
            "neural activ\n",
            "neural code\n",
            "neural comput\n",
            "neural inform\n",
            "neural inform process\n",
            "neural net\n",
            "neural network\n",
            "neural network model\n",
            "neural popul\n",
            "neural respons\n",
            "neural system\n",
            "neurobiolog\n",
            "neuroimag\n",
            "neuromorph\n",
            "neuron\n",
            "neuron activ\n",
            "neuron fire\n",
            "neuron model\n",
            "neuron network\n",
            "neuron neuron\n",
            "neuron respons\n",
            "neuron spike\n",
            "neurophysiol\n",
            "neurophysiolog\n",
            "neurosci\n",
            "never\n",
            "nevertheless\n",
            "new\n",
            "new algorithm\n",
            "new approach\n",
            "new data\n",
            "new method\n",
            "new model\n",
            "new york\n",
            "new york usa\n",
            "news\n",
            "newsgroup\n",
            "newton\n",
            "newton method\n",
            "next\n",
            "next section\n",
            "next state\n",
            "nice\n",
            "nip\n",
            "nip page\n",
            "nmf\n",
            "node\n",
            "node node\n",
            "node tree\n",
            "nois\n",
            "nois free\n",
            "nois level\n",
            "nois model\n",
            "nois ratio\n",
            "nois varianc\n",
            "noiseless\n",
            "noisi\n",
            "nomin\n",
            "non\n",
            "non convex\n",
            "non gaussian\n",
            "non linear\n",
            "non neg\n",
            "non parametr\n",
            "non smooth\n",
            "non stationari\n",
            "non trivial\n",
            "non zero\n",
            "nonconvex\n",
            "none\n",
            "nonetheless\n",
            "nonlinear\n",
            "nonlinear function\n",
            "nonneg\n",
            "nonparametr\n",
            "nonparametr bayesian\n",
            "nonstationari\n",
            "nontrivi\n",
            "nonzero\n",
            "norm\n",
            "norm regular\n",
            "normal\n",
            "normal constant\n",
            "normal cut\n",
            "normal distribut\n",
            "normalis\n",
            "north\n",
            "notabl\n",
            "notat\n",
            "note\n",
            "noth\n",
            "notic\n",
            "notion\n",
            "noun\n",
            "novel\n",
            "novelti\n",
            "nsf\n",
            "nsf grant\n",
            "nuclear\n",
            "nuclear norm\n",
            "nucleu\n",
            "null\n",
            "number\n",
            "number class\n",
            "number cluster\n",
            "number compon\n",
            "number data\n",
            "number edg\n",
            "number exampl\n",
            "number featur\n",
            "number hidden\n",
            "number hidden unit\n",
            "number input\n",
            "number iter\n",
            "number label\n",
            "number node\n",
            "number non\n",
            "number observ\n",
            "number paramet\n",
            "number point\n",
            "number possibl\n",
            "number sampl\n",
            "number state\n",
            "number step\n",
            "number time\n",
            "number topic\n",
            "number train\n",
            "number train exampl\n",
            "number variabl\n",
            "numer\n",
            "numer experi\n",
            "nystr\n",
            "obey\n",
            "object\n",
            "object categori\n",
            "object class\n",
            "object detect\n",
            "object function\n",
            "object recognit\n",
            "object valu\n",
            "observ\n",
            "observ data\n",
            "observ model\n",
            "observ variabl\n",
            "obstacl\n",
            "obtain\n",
            "obtain follow\n",
            "obviou\n",
            "obvious\n",
            "occlud\n",
            "occlus\n",
            "occupi\n",
            "occur\n",
            "occurr\n",
            "ocular\n",
            "ocular domin\n",
            "odd\n",
            "odor\n",
            "offer\n",
            "offic\n",
            "offlin\n",
            "offset\n",
            "often\n",
            "often use\n",
            "ofth\n",
            "old\n",
            "olfactori\n",
            "olkopf\n",
            "olshausen\n",
            "omit\n",
            "omp\n",
            "one\n",
            "onlin\n",
            "onlin algorithm\n",
            "onlin learn\n",
            "onr\n",
            "onset\n",
            "onto\n",
            "open\n",
            "oper\n",
            "oper research\n",
            "opinion\n",
            "opper\n",
            "oppon\n",
            "oppos\n",
            "opposit\n",
            "opt\n",
            "optic\n",
            "optic flow\n",
            "optim\n",
            "optim action\n",
            "optim algorithm\n",
            "optim control\n",
            "optim method\n",
            "optim paramet\n",
            "optim perform\n",
            "optim polici\n",
            "optim problem\n",
            "optim solut\n",
            "optim strategi\n",
            "optim valu\n",
            "optima\n",
            "optimis\n",
            "optimist\n",
            "optimum\n",
            "option\n",
            "oracl\n",
            "order\n",
            "order magnitud\n",
            "order obtain\n",
            "order paramet\n",
            "order statist\n",
            "ordin\n",
            "ordinari\n",
            "org\n",
            "organ\n",
            "organ follow\n",
            "organ follow section\n",
            "orient\n",
            "origin\n",
            "origin data\n",
            "orthogon\n",
            "orthonorm\n",
            "oscil\n",
            "oscillatori\n",
            "other\n",
            "otherwis\n",
            "outcom\n",
            "outer\n",
            "outlier\n",
            "outlin\n",
            "outperform\n",
            "output\n",
            "output layer\n",
            "output neuron\n",
            "output node\n",
            "output space\n",
            "output unit\n",
            "outsid\n",
            "overal\n",
            "overcom\n",
            "overcomplet\n",
            "overfit\n",
            "overhead\n",
            "overlap\n",
            "overview\n",
            "oxford\n",
            "oxford univers\n",
            "oxford univers press\n",
            "pac\n",
            "pack\n",
            "packag\n",
            "packet\n",
            "page\n",
            "page acm\n",
            "page ieee\n",
            "page mit\n",
            "page mit press\n",
            "page springer\n",
            "pair\n",
            "pairwis\n",
            "pami\n",
            "panel\n",
            "panel show\n",
            "paper\n",
            "paper consid\n",
            "paper describ\n",
            "paper focu\n",
            "paper introduc\n",
            "paper organ\n",
            "paper organ follow\n",
            "paper present\n",
            "paper propos\n",
            "paper use\n",
            "paradigm\n",
            "parallel\n",
            "paramet\n",
            "paramet control\n",
            "paramet estim\n",
            "paramet learn\n",
            "paramet model\n",
            "paramet set\n",
            "paramet space\n",
            "paramet updat\n",
            "paramet use\n",
            "paramet valu\n",
            "paramet vector\n",
            "parameter\n",
            "parametr\n",
            "parametr model\n",
            "parent\n",
            "pari\n",
            "pariti\n",
            "park\n",
            "pars\n",
            "part\n",
            "parti\n",
            "partial\n",
            "partial deriv\n",
            "partial observ\n",
            "partial support\n",
            "particip\n",
            "particl\n",
            "particl filter\n",
            "particular\n",
            "particularli\n",
            "partit\n",
            "partit function\n",
            "partli\n",
            "parzen\n",
            "pascal\n",
            "pass\n",
            "pass algorithm\n",
            "pass filter\n",
            "passiv\n",
            "past\n",
            "patch\n",
            "path\n",
            "pathway\n",
            "patient\n",
            "pattern\n",
            "pattern analysi\n",
            "pattern analysi machin\n",
            "pattern classif\n",
            "pattern recognit\n",
            "paul\n",
            "pay\n",
            "payoff\n",
            "pca\n",
            "pcfg\n",
            "pdf\n",
            "pea\n",
            "peak\n",
            "pearl\n",
            "pearson\n",
            "pedestrian\n",
            "penal\n",
            "penalti\n",
            "penalti term\n",
            "peopl\n",
            "per\n",
            "per class\n",
            "per iter\n",
            "perceiv\n",
            "percent\n",
            "percentag\n",
            "percept\n",
            "perceptron\n",
            "perceptron algorithm\n",
            "perceptu\n",
            "pereira\n",
            "perfect\n",
            "perfectli\n",
            "perform\n",
            "perform algorithm\n",
            "perform best\n",
            "perform better\n",
            "perform compar\n",
            "perform differ\n",
            "perform experi\n",
            "perform improv\n",
            "perform measur\n",
            "perform method\n",
            "perform model\n",
            "perform poorli\n",
            "perform well\n",
            "perhap\n",
            "period\n",
            "permit\n",
            "permut\n",
            "perona\n",
            "perplex\n",
            "persist\n",
            "person\n",
            "perspect\n",
            "perturb\n",
            "peter\n",
            "phase\n",
            "phase transit\n",
            "phd\n",
            "phd thesi\n",
            "phenomena\n",
            "phenomenon\n",
            "phi\n",
            "phi rev\n",
            "phone\n",
            "phonem\n",
            "phonet\n",
            "phrase\n",
            "physic\n",
            "physiolog\n",
            "pick\n",
            "pictur\n",
            "piec\n",
            "piecewis\n",
            "piecewis linear\n",
            "pij\n",
            "pitch\n",
            "pittsburgh\n",
            "pivot\n",
            "pixel\n",
            "place\n",
            "plan\n",
            "planar\n",
            "planck\n",
            "plane\n",
            "plant\n",
            "plastic\n",
            "platt\n",
            "plausibl\n",
            "play\n",
            "play role\n",
            "player\n",
            "pleas\n",
            "plot\n",
            "plot show\n",
            "plu\n",
            "plug\n",
            "po\n",
            "poggio\n",
            "point\n",
            "point algorithm\n",
            "point estim\n",
            "point method\n",
            "point point\n",
            "point process\n",
            "point set\n",
            "point view\n",
            "pointwis\n",
            "poisson\n",
            "poisson distribut\n",
            "poisson process\n",
            "polar\n",
            "pole\n",
            "poli\n",
            "polici\n",
            "polici gradient\n",
            "polici iter\n",
            "polici search\n",
            "polynomi\n",
            "polynomi time\n",
            "polytop\n",
            "pomdp\n",
            "pool\n",
            "poor\n",
            "poorli\n",
            "popul\n",
            "popul code\n",
            "popular\n",
            "portfolio\n",
            "portion\n",
            "pose\n",
            "posit\n",
            "posit constant\n",
            "posit definit\n",
            "posit exampl\n",
            "posit neg\n",
            "posit rate\n",
            "posit semi\n",
            "posit semi definit\n",
            "posit semidefinit\n",
            "possess\n",
            "possibl\n",
            "possibl valu\n",
            "post\n",
            "post synapt\n",
            "posterior\n",
            "posterior distribut\n",
            "posterior mean\n",
            "posterior probabl\n",
            "posteriori\n",
            "postsynapt\n",
            "potenti\n",
            "potenti function\n",
            "power\n",
            "power law\n",
            "ppca\n",
            "practic\n",
            "practic applic\n",
            "pre\n",
            "pre process\n",
            "pre train\n",
            "preced\n",
            "precis\n",
            "precis matrix\n",
            "precis recal\n",
            "predefin\n",
            "predic\n",
            "predict\n",
            "predict accuraci\n",
            "predict distribut\n",
            "predict error\n",
            "predict label\n",
            "predict model\n",
            "predict perform\n",
            "predict problem\n",
            "predictor\n",
            "prefer\n",
            "prefix\n",
            "preliminari\n",
            "prepar\n",
            "preprint\n",
            "preprint arxiv\n",
            "preprocess\n",
            "presenc\n",
            "present\n",
            "present algorithm\n",
            "present new\n",
            "present novel\n",
            "present paper\n",
            "present result\n",
            "present section\n",
            "preserv\n",
            "press\n",
            "press cambridg\n",
            "pressur\n",
            "presum\n",
            "presynapt\n",
            "prevent\n",
            "previou\n",
            "previou approach\n",
            "previou section\n",
            "previou work\n",
            "previous\n",
            "price\n",
            "primal\n",
            "primari\n",
            "primari visual\n",
            "primari visual cortex\n",
            "primarili\n",
            "primat\n",
            "prime\n",
            "primit\n",
            "princeton\n",
            "princip\n",
            "princip compon\n",
            "princip compon analysi\n",
            "principl\n",
            "print\n",
            "prior\n",
            "prior distribut\n",
            "prior inform\n",
            "prior knowledg\n",
            "prior model\n",
            "prior probabl\n",
            "prior work\n",
            "priori\n",
            "prioriti\n",
            "privaci\n",
            "privat\n",
            "prob\n",
            "probabilist\n",
            "probabilist infer\n",
            "probabilist model\n",
            "probabl\n",
            "probabl densiti\n",
            "probabl densiti function\n",
            "probabl distribut\n",
            "probabl estim\n",
            "probabl given\n",
            "probabl least\n",
            "probabl mass\n",
            "probabl measur\n",
            "probabl model\n",
            "probabl observ\n",
            "probe\n",
            "problem\n",
            "problem comput\n",
            "problem consid\n",
            "problem estim\n",
            "problem find\n",
            "problem formul\n",
            "problem gener\n",
            "problem given\n",
            "problem involv\n",
            "problem learn\n",
            "problem min\n",
            "problem set\n",
            "problem solv\n",
            "problem use\n",
            "problemat\n",
            "proc\n",
            "proc ieee\n",
            "proc intern\n",
            "proce\n",
            "procedur\n",
            "proceed\n",
            "proceed annual\n",
            "proceed confer\n",
            "proceed ieee\n",
            "proceed intern\n",
            "proceed intern confer\n",
            "proceed nation\n",
            "process\n",
            "process model\n",
            "process prior\n",
            "process regress\n",
            "process system\n",
            "process system mit\n",
            "process system nip\n",
            "process system page\n",
            "process system volum\n",
            "processor\n",
            "produc\n",
            "product\n",
            "profil\n",
            "profit\n",
            "program\n",
            "programm\n",
            "progress\n",
            "prohibit\n",
            "project\n",
            "project onto\n",
            "promin\n",
            "promis\n",
            "promot\n",
            "proof\n",
            "proof lemma\n",
            "proof sketch\n",
            "proof theorem\n",
            "propag\n",
            "propag algorithm\n",
            "proper\n",
            "properli\n",
            "properti\n",
            "proport\n",
            "propos\n",
            "propos algorithm\n",
            "propos approach\n",
            "propos distribut\n",
            "propos method\n",
            "propos model\n",
            "propos new\n",
            "propos novel\n",
            "propos use\n",
            "proposit\n",
            "protein\n",
            "protocol\n",
            "prototyp\n",
            "provabl\n",
            "prove\n",
            "proven\n",
            "provid\n",
            "provid good\n",
            "proxim\n",
            "prune\n",
            "pseudo\n",
            "psycholog\n",
            "psycholog review\n",
            "psychophys\n",
            "public\n",
            "publicli\n",
            "publish\n",
            "pull\n",
            "puls\n",
            "pure\n",
            "purpos\n",
            "pursu\n",
            "pursuit\n",
            "push\n",
            "put\n",
            "pyramid\n",
            "quadrat\n",
            "quadrat program\n",
            "qualit\n",
            "qualiti\n",
            "quantifi\n",
            "quantil\n",
            "quantit\n",
            "quantiti\n",
            "quantiz\n",
            "quantum\n",
            "quasi\n",
            "queri\n",
            "question\n",
            "question whether\n",
            "queue\n",
            "quickli\n",
            "quit\n",
            "rademach\n",
            "rademach complex\n",
            "radial\n",
            "radial basi\n",
            "radial basi function\n",
            "radiu\n",
            "rais\n",
            "ram\n",
            "ran\n",
            "rand\n",
            "random\n",
            "random field\n",
            "random graph\n",
            "random initi\n",
            "random project\n",
            "random sampl\n",
            "random variabl\n",
            "random vector\n",
            "random walk\n",
            "randomli\n",
            "randomli chosen\n",
            "randomli gener\n",
            "randomli select\n",
            "rang\n",
            "rank\n",
            "rank approxim\n",
            "rank function\n",
            "rank matric\n",
            "rank matrix\n",
            "rao\n",
            "rapid\n",
            "rapidli\n",
            "rare\n",
            "rasmussen\n",
            "rat\n",
            "rate\n",
            "rate converg\n",
            "rather\n",
            "ratio\n",
            "ration\n",
            "raw\n",
            "ray\n",
            "rbf\n",
            "rbf kernel\n",
            "rbm\n",
            "re\n",
            "reach\n",
            "reaction\n",
            "read\n",
            "reader\n",
            "readili\n",
            "real\n",
            "real data\n",
            "real number\n",
            "real time\n",
            "real valu\n",
            "real world\n",
            "real world data\n",
            "real world problem\n",
            "realist\n",
            "realiz\n",
            "realli\n",
            "reason\n",
            "recal\n",
            "receiv\n",
            "receiv input\n",
            "recent\n",
            "recent propos\n",
            "recent work\n",
            "recent year\n",
            "recept\n",
            "recept field\n",
            "receptor\n",
            "recip\n",
            "reciproc\n",
            "recogn\n",
            "recognit\n",
            "recognit system\n",
            "recognit task\n",
            "recommend\n",
            "reconstruct\n",
            "reconstruct error\n",
            "record\n",
            "recov\n",
            "recoveri\n",
            "rectangl\n",
            "rectangular\n",
            "rectifi\n",
            "recurr\n",
            "recurr network\n",
            "recurr neural\n",
            "recurr neural network\n",
            "recurs\n",
            "red\n",
            "reduc\n",
            "reduc comput\n",
            "reduc number\n",
            "reduct\n",
            "redund\n",
            "refin\n",
            "reflect\n",
            "reformul\n",
            "refractori\n",
            "reg\n",
            "regard\n",
            "regardless\n",
            "regim\n",
            "region\n",
            "regist\n",
            "registr\n",
            "regress\n",
            "regress function\n",
            "regress model\n",
            "regress problem\n",
            "regressor\n",
            "regret\n",
            "regret bound\n",
            "regul\n",
            "regular\n",
            "regular paramet\n",
            "regular term\n",
            "regularis\n",
            "reinforc\n",
            "reinforc learn\n",
            "reject\n",
            "rel\n",
            "rel entropi\n",
            "rel error\n",
            "rel small\n",
            "relat\n",
            "relat data\n",
            "relat model\n",
            "relat work\n",
            "relationship\n",
            "relax\n",
            "releas\n",
            "relev\n",
            "reli\n",
            "reliabl\n",
            "remain\n",
            "remaind\n",
            "remark\n",
            "remov\n",
            "render\n",
            "rep\n",
            "repeat\n",
            "repeatedli\n",
            "repetit\n",
            "replac\n",
            "replic\n",
            "replica\n",
            "report\n",
            "report result\n",
            "repositori\n",
            "repres\n",
            "repres set\n",
            "represent\n",
            "reproduc\n",
            "reproduc kernel\n",
            "reproduc kernel hilbert\n",
            "request\n",
            "requir\n",
            "requir comput\n",
            "resampl\n",
            "rescal\n",
            "research\n",
            "research support\n",
            "resembl\n",
            "reservoir\n",
            "reset\n",
            "residu\n",
            "resist\n",
            "resolut\n",
            "resolv\n",
            "reson\n",
            "resort\n",
            "resourc\n",
            "resp\n",
            "respect\n",
            "respond\n",
            "respons\n",
            "rest\n",
            "rest paper\n",
            "restart\n",
            "restaur\n",
            "restor\n",
            "restrict\n",
            "restrict boltzmann\n",
            "restrict boltzmann machin\n",
            "result algorithm\n",
            "result averag\n",
            "result compar\n",
            "result demonstr\n",
            "result experi\n",
            "result indic\n",
            "result model\n",
            "result obtain\n",
            "result present\n",
            "result report\n",
            "result section\n",
            "result show\n",
            "result shown\n",
            "result suggest\n",
            "retain\n",
            "retin\n",
            "retina\n",
            "retriev\n",
            "return\n",
            "rev\n",
            "reveal\n",
            "revers\n",
            "review\n",
            "reward\n",
            "reward function\n",
            "reweight\n",
            "rewrit\n",
            "rewritten\n",
            "rhythm\n",
            "rich\n",
            "richard\n",
            "richer\n",
            "ridg\n",
            "ridg regress\n",
            "riemannian\n",
            "right\n",
            "right hand\n",
            "right hand side\n",
            "right panel\n",
            "rigid\n",
            "rigor\n",
            "rij\n",
            "ring\n",
            "rise\n",
            "risk\n",
            "risk bound\n",
            "risk minim\n",
            "rkh\n",
            "rmse\n",
            "rnn\n",
            "road\n",
            "robert\n",
            "robot\n",
            "robust\n",
            "roc\n",
            "roc curv\n",
            "roi\n",
            "role\n",
            "roll\n",
            "rollout\n",
            "room\n",
            "root\n",
            "root node\n",
            "rotat\n",
            "roth\n",
            "rough\n",
            "roughli\n",
            "round\n",
            "rout\n",
            "routin\n",
            "row\n",
            "row column\n",
            "rowei\n",
            "roy\n",
            "royal\n",
            "royal statist\n",
            "royal statist societi\n",
            "rst\n",
            "rubin\n",
            "rule\n",
            "rumelhart\n",
            "run\n",
            "run algorithm\n",
            "run time\n",
            "runtim\n",
            "russel\n",
            "saccad\n",
            "saddl\n",
            "saddl point\n",
            "safe\n",
            "said\n",
            "sake\n",
            "salakhutdinov\n",
            "salienc\n",
            "salient\n",
            "sampl\n",
            "sampl algorithm\n",
            "sampl distribut\n",
            "sampl drawn\n",
            "sampl method\n",
            "sampl use\n",
            "sampler\n",
            "san\n",
            "san diego\n",
            "san francisco\n",
            "san mateo\n",
            "sat\n",
            "satisfi\n",
            "satisfi condit\n",
            "satisfi follow\n",
            "satur\n",
            "saul\n",
            "save\n",
            "say\n",
            "scalabl\n",
            "scalar\n",
            "scale\n",
            "scale factor\n",
            "scale invari\n",
            "scale paramet\n",
            "scale problem\n",
            "scan\n",
            "scatter\n",
            "scenario\n",
            "scene\n",
            "sch\n",
            "sch olkopf\n",
            "schapir\n",
            "schedul\n",
            "schema\n",
            "schemat\n",
            "scheme\n",
            "schmidt\n",
            "school\n",
            "school comput\n",
            "school comput scienc\n",
            "schuurman\n",
            "schwartz\n",
            "sci\n",
            "scienc\n",
            "scienc depart\n",
            "scienc engin\n",
            "scienc foundat\n",
            "scienc univers\n",
            "scientif\n",
            "scope\n",
            "score\n",
            "score function\n",
            "scott\n",
            "screen\n",
            "sdp\n",
            "search\n",
            "search algorithm\n",
            "search engin\n",
            "search space\n",
            "sec\n",
            "second\n",
            "second layer\n",
            "second order\n",
            "second term\n",
            "secondari\n",
            "secondli\n",
            "section\n",
            "section describ\n",
            "section discuss\n",
            "section give\n",
            "section introduc\n",
            "section present\n",
            "section provid\n",
            "section section\n",
            "section use\n",
            "secur\n",
            "see\n",
            "see appendix\n",
            "see detail\n",
            "see exampl\n",
            "see section\n",
            "see tabl\n",
            "see text\n",
            "seed\n",
            "seeger\n",
            "seek\n",
            "seem\n",
            "seen\n",
            "segment\n",
            "sejnowski\n",
            "select\n",
            "select featur\n",
            "select method\n",
            "selector\n",
            "self\n",
            "self organ\n",
            "semant\n",
            "semi\n",
            "semi definit\n",
            "semi supervis\n",
            "semi supervis learn\n",
            "semidefinit\n",
            "semidefinit program\n",
            "send\n",
            "sens\n",
            "sensibl\n",
            "sensit\n",
            "sensor\n",
            "sensori\n",
            "sensori input\n",
            "sent\n",
            "sentenc\n",
            "sentiment\n",
            "separ\n",
            "septemb\n",
            "sequenc\n",
            "sequenti\n",
            "seri\n",
            "serial\n",
            "serv\n",
            "server\n",
            "servic\n",
            "session\n",
            "set algorithm\n",
            "set consist\n",
            "set contain\n",
            "set data\n",
            "set differ\n",
            "set exampl\n",
            "set experi\n",
            "set featur\n",
            "set function\n",
            "set gener\n",
            "set given\n",
            "set imag\n",
            "set input\n",
            "set label\n",
            "set learn\n",
            "set let\n",
            "set model\n",
            "set node\n",
            "set number\n",
            "set object\n",
            "set observ\n",
            "set paramet\n",
            "set point\n",
            "set possibl\n",
            "set random\n",
            "set result\n",
            "set sampl\n",
            "set set\n",
            "set size\n",
            "set state\n",
            "set test\n",
            "set train\n",
            "set use\n",
            "set valu\n",
            "set variabl\n",
            "set weight\n",
            "set zero\n",
            "settl\n",
            "setup\n",
            "seung\n",
            "sever\n",
            "sgd\n",
            "sgn\n",
            "shade\n",
            "shadow\n",
            "shalev\n",
            "shalev shwartz\n",
            "shall\n",
            "shallow\n",
            "shannon\n",
            "shape\n",
            "share\n",
            "sharp\n",
            "shatter\n",
            "shaw\n",
            "shaw taylor\n",
            "shi\n",
            "shift\n",
            "short\n",
            "short term\n",
            "shorter\n",
            "shortest\n",
            "shortest path\n",
            "shot\n",
            "show\n",
            "show averag\n",
            "show exampl\n",
            "show result\n",
            "shown\n",
            "shown tabl\n",
            "shrink\n",
            "shrinkag\n",
            "shwartz\n",
            "siam\n",
            "siam journal\n",
            "side\n",
            "side inform\n",
            "sift\n",
            "sigir\n",
            "sigmoid\n",
            "sigmoid function\n",
            "sign\n",
            "signal\n",
            "signal nois\n",
            "signal nois ratio\n",
            "signal process\n",
            "signatur\n",
            "signific\n",
            "signific improv\n",
            "significantli\n",
            "significantli better\n",
            "significantli improv\n",
            "significantli outperform\n",
            "sij\n",
            "silicon\n",
            "similar\n",
            "similar function\n",
            "similar matrix\n",
            "similar measur\n",
            "similar result\n",
            "similarli\n",
            "simon\n",
            "simoncelli\n",
            "simpl\n",
            "simpl cell\n",
            "simpl exampl\n",
            "simpl model\n",
            "simpler\n",
            "simplest\n",
            "simplex\n",
            "simpli\n",
            "simplic\n",
            "simplif\n",
            "simplifi\n",
            "simul\n",
            "simul data\n",
            "simul result\n",
            "simultan\n",
            "sin\n",
            "sinc\n",
            "singer\n",
            "singh\n",
            "singl\n",
            "singl layer\n",
            "singl neuron\n",
            "singl trial\n",
            "singular\n",
            "singular valu\n",
            "singular valu decomposit\n",
            "sinusoid\n",
            "sit\n",
            "site\n",
            "situat\n",
            "six\n",
            "size\n",
            "size train\n",
            "sketch\n",
            "skew\n",
            "skill\n",
            "skip\n",
            "slack\n",
            "sleep\n",
            "slice\n",
            "slight\n",
            "slightli\n",
            "slightli better\n",
            "slope\n",
            "slow\n",
            "slower\n",
            "slowli\n",
            "small\n",
            "small number\n",
            "small set\n",
            "small valu\n",
            "smaller\n",
            "smallest\n",
            "smc\n",
            "smith\n",
            "smola\n",
            "smooth\n",
            "smooth function\n",
            "smoother\n",
            "smoothli\n",
            "snapshot\n",
            "snr\n",
            "soc\n",
            "social\n",
            "social network\n",
            "societi\n",
            "societi seri\n",
            "soft\n",
            "softmax\n",
            "softwar\n",
            "sole\n",
            "solid\n",
            "solid line\n",
            "solla\n",
            "solut\n",
            "solut obtain\n",
            "solut problem\n",
            "solv\n",
            "solv follow\n",
            "solv optim\n",
            "solv problem\n",
            "solver\n",
            "som\n",
            "someth\n",
            "sometim\n",
            "somewhat\n",
            "son\n",
            "song\n",
            "soon\n",
            "sophist\n",
            "sort\n",
            "sound\n",
            "sourc\n",
            "sourc separ\n",
            "space\n",
            "space limit\n",
            "space model\n",
            "space time\n",
            "spam\n",
            "span\n",
            "span tree\n",
            "spars\n",
            "spars code\n",
            "spars represent\n",
            "spars solut\n",
            "sparsiti\n",
            "spatial\n",
            "spatial frequenc\n",
            "spatio\n",
            "spatio tempor\n",
            "spatiotempor\n",
            "speak\n",
            "speaker\n",
            "speci\n",
            "special\n",
            "special case\n",
            "specif\n",
            "specifi\n",
            "spectra\n",
            "spectral\n",
            "spectral cluster\n",
            "spectrogram\n",
            "spectrum\n",
            "speech\n",
            "speech recognit\n",
            "speech signal\n",
            "speed\n",
            "speedup\n",
            "sphere\n",
            "spheric\n",
            "spike\n",
            "spike count\n",
            "spike neuron\n",
            "spike rate\n",
            "spike time\n",
            "spike train\n",
            "spin\n",
            "spirit\n",
            "spline\n",
            "split\n",
            "spoken\n",
            "spontan\n",
            "spread\n",
            "springer\n",
            "springer verlag\n",
            "spuriou\n",
            "squar\n",
            "squar error\n",
            "squar loss\n",
            "squar root\n",
            "srebro\n",
            "srm\n",
            "stabil\n",
            "stabl\n",
            "stack\n",
            "stage\n",
            "stand\n",
            "standard\n",
            "standard deviat\n",
            "standard error\n",
            "stanford\n",
            "stanford edu\n",
            "stanford univers\n",
            "star\n",
            "start\n",
            "start point\n",
            "start state\n",
            "stat\n",
            "state\n",
            "state action\n",
            "state action pair\n",
            "state art\n",
            "state distribut\n",
            "state given\n",
            "state sequenc\n",
            "state space\n",
            "state space model\n",
            "state state\n",
            "state time\n",
            "state transit\n",
            "state variabl\n",
            "state vector\n",
            "statement\n",
            "static\n",
            "stationari\n",
            "stationari distribut\n",
            "stationari point\n",
            "statist\n",
            "statist associ\n",
            "statist comput\n",
            "statist estim\n",
            "statist learn\n",
            "statist learn theori\n",
            "statist mechan\n",
            "statist model\n",
            "statist properti\n",
            "statist signific\n",
            "statist societi\n",
            "statist societi seri\n",
            "stay\n",
            "std\n",
            "stdp\n",
            "steadi\n",
            "steadi state\n",
            "steepest\n",
            "stem\n",
            "step\n",
            "step algorithm\n",
            "step comput\n",
            "step size\n",
            "step step\n",
            "stephen\n",
            "stereo\n",
            "stick\n",
            "stick break\n",
            "still\n",
            "stimul\n",
            "stimuli\n",
            "stimulu\n",
            "stochast\n",
            "stochast approxim\n",
            "stochast gradient\n",
            "stochast gradient descent\n",
            "stochast optim\n",
            "stochast process\n",
            "stock\n",
            "stone\n",
            "stop\n",
            "storag\n",
            "store\n",
            "straight\n",
            "straightforward\n",
            "strategi\n",
            "stream\n",
            "street\n",
            "strength\n",
            "stress\n",
            "strict\n",
            "strictli\n",
            "strike\n",
            "string\n",
            "stroke\n",
            "strong\n",
            "strong convex\n",
            "stronger\n",
            "strongli\n",
            "strongli convex\n",
            "structur\n",
            "structur data\n",
            "structur learn\n",
            "structur model\n",
            "structur output\n",
            "structur predict\n",
            "student\n",
            "studi\n",
            "style\n",
            "sub\n",
            "sub gaussian\n",
            "sub optim\n",
            "subgradi\n",
            "subgraph\n",
            "subject\n",
            "submit\n",
            "submodular\n",
            "submodular function\n",
            "suboptim\n",
            "subproblem\n",
            "subsampl\n",
            "subscript\n",
            "subsect\n",
            "subsequ\n",
            "subset\n",
            "subspac\n",
            "substanti\n",
            "substitut\n",
            "subtract\n",
            "subtre\n",
            "succeed\n",
            "success\n",
            "suffer\n",
            "suffic\n",
            "suffici\n",
            "suffici condit\n",
            "suffici statist\n",
            "suffix\n",
            "suggest\n",
            "suit\n",
            "suitabl\n",
            "sum\n",
            "sum product\n",
            "sum squar\n",
            "summar\n",
            "summari\n",
            "summat\n",
            "sun\n",
            "sup\n",
            "super\n",
            "superior\n",
            "superposit\n",
            "supervis\n",
            "supervis learn\n",
            "supp\n",
            "supplement\n",
            "supplementari\n",
            "supplementari materi\n",
            "suppli\n",
            "support\n",
            "support nsf\n",
            "support part\n",
            "support vector\n",
            "support vector machin\n",
            "suppos\n",
            "suppress\n",
            "sure\n",
            "surfac\n",
            "surpris\n",
            "surprisingli\n",
            "surrog\n",
            "surrog loss\n",
            "surround\n",
            "survey\n",
            "surviv\n",
            "sutton\n",
            "svd\n",
            "svm\n",
            "svm classifi\n",
            "svr\n",
            "swap\n",
            "sweep\n",
            "switch\n",
            "syllabl\n",
            "symbol\n",
            "symmetr\n",
            "symmetri\n",
            "symposium\n",
            "synaps\n",
            "synapt\n",
            "synapt input\n",
            "synapt plastic\n",
            "synapt weight\n",
            "synchron\n",
            "synchroni\n",
            "syntact\n",
            "synthes\n",
            "synthesi\n",
            "synthet\n",
            "synthet data\n",
            "synthet real\n",
            "system\n",
            "system mit\n",
            "system mit press\n",
            "system nip\n",
            "system page\n",
            "system page mit\n",
            "system use\n",
            "system volum\n",
            "systemat\n",
            "tabl\n",
            "tabl comparison\n",
            "tabl show\n",
            "tackl\n",
            "tag\n",
            "tail\n",
            "take\n",
            "take account\n",
            "take advantag\n",
            "take form\n",
            "take place\n",
            "take valu\n",
            "taken\n",
            "talk\n",
            "tangent\n",
            "tanh\n",
            "tap\n",
            "target\n",
            "target distribut\n",
            "target function\n",
            "task\n",
            "task learn\n",
            "taskar\n",
            "taxonomi\n",
            "taylor\n",
            "teach\n",
            "teacher\n",
            "team\n",
            "tech\n",
            "technic\n",
            "technic report\n",
            "techniqu\n",
            "techniqu use\n",
            "technolog\n",
            "teh\n",
            "tell\n",
            "temperatur\n",
            "templat\n",
            "tempor\n",
            "tempor differ\n",
            "tempor differ learn\n",
            "ten\n",
            "tend\n",
            "tendenc\n",
            "tenenbaum\n",
            "tensor\n",
            "term\n",
            "term depend\n",
            "termin\n",
            "tesauro\n",
            "test\n",
            "test data\n",
            "test error\n",
            "test exampl\n",
            "test imag\n",
            "test perform\n",
            "test point\n",
            "test set\n",
            "test statist\n",
            "test time\n",
            "texa\n",
            "text\n",
            "text classif\n",
            "textur\n",
            "thank\n",
            "theorem\n",
            "theorem assum\n",
            "theorem follow\n",
            "theorem given\n",
            "theorem let\n",
            "theorem show\n",
            "theorem theorem\n",
            "theoret\n",
            "theoret analysi\n",
            "theoret guarante\n",
            "theoret result\n",
            "theori\n",
            "therebi\n",
            "therefor\n",
            "therein\n",
            "thesi\n",
            "thick\n",
            "thin\n",
            "thing\n",
            "think\n",
            "third\n",
            "thm\n",
            "thoma\n",
            "thompson\n",
            "though\n",
            "thought\n",
            "thousand\n",
            "thread\n",
            "threshold\n",
            "threshold function\n",
            "throughout\n",
            "throughout paper\n",
            "thrun\n",
            "thu\n",
            "tibshirani\n",
            "tie\n",
            "tight\n",
            "tighter\n",
            "tile\n",
            "tilt\n",
            "time\n",
            "time algorithm\n",
            "time complex\n",
            "time comput\n",
            "time constant\n",
            "time cours\n",
            "time delay\n",
            "time depend\n",
            "time differ\n",
            "time frequenc\n",
            "time interv\n",
            "time point\n",
            "time requir\n",
            "time scale\n",
            "time sec\n",
            "time second\n",
            "time seri\n",
            "time step\n",
            "time time\n",
            "time vari\n",
            "timestep\n",
            "tion\n",
            "tip\n",
            "tishbi\n",
            "tissu\n",
            "togeth\n",
            "token\n",
            "toler\n",
            "tone\n",
            "took\n",
            "tool\n",
            "top\n",
            "top bottom\n",
            "top row\n",
            "topic\n",
            "topic model\n",
            "topograph\n",
            "topolog\n",
            "toronto\n",
            "torralba\n",
            "total\n",
            "total number\n",
            "total variat\n",
            "touretzki\n",
            "toward\n",
            "toy\n",
            "trace\n",
            "trace norm\n",
            "track\n",
            "tracker\n",
            "tractabl\n",
            "trade\n",
            "tradeoff\n",
            "tradit\n",
            "traffic\n",
            "train\n",
            "train algorithm\n",
            "train classifi\n",
            "train data\n",
            "train error\n",
            "train exampl\n",
            "train imag\n",
            "train instanc\n",
            "train model\n",
            "train network\n",
            "train pattern\n",
            "train point\n",
            "train procedur\n",
            "train sampl\n",
            "train set\n",
            "train set size\n",
            "train test\n",
            "train test set\n",
            "train time\n",
            "trajectori\n",
            "tran\n",
            "transact\n",
            "transact inform\n",
            "transact inform theori\n",
            "transact pattern\n",
            "transact pattern analysi\n",
            "transcript\n",
            "transduct\n",
            "transfer\n",
            "transfer function\n",
            "transfer learn\n",
            "transform\n",
            "transient\n",
            "transistor\n",
            "transit\n",
            "transit function\n",
            "transit matrix\n",
            "transit probabl\n",
            "translat\n",
            "translat invari\n",
            "transmiss\n",
            "transmit\n",
            "transpar\n",
            "transport\n",
            "travel\n",
            "travers\n",
            "treat\n",
            "treatment\n",
            "tree\n",
            "tree algorithm\n",
            "tree base\n",
            "tree structur\n",
            "treewidth\n",
            "trend\n",
            "tresp\n",
            "tri\n",
            "trial\n",
            "trial trial\n",
            "triangl\n",
            "triangul\n",
            "trick\n",
            "trigger\n",
            "tripl\n",
            "triplet\n",
            "trivial\n",
            "true\n",
            "true label\n",
            "true model\n",
            "true posit\n",
            "true valu\n",
            "truli\n",
            "truncat\n",
            "truth\n",
            "tsitsikli\n",
            "tsybakov\n",
            "tune\n",
            "tune curv\n",
            "tune paramet\n",
            "tupl\n",
            "turn\n",
            "tutori\n",
            "twenti\n",
            "twice\n",
            "type\n",
            "type error\n",
            "typic\n",
            "uai\n",
            "ucb\n",
            "uci\n",
            "ucl\n",
            "uller\n",
            "ultim\n",
            "unabl\n",
            "unari\n",
            "unbias\n",
            "unbias estim\n",
            "unbound\n",
            "uncertain\n",
            "uncertainti\n",
            "uncertainti artifici\n",
            "uncertainti artifici intellig\n",
            "unchang\n",
            "unclear\n",
            "unconstrain\n",
            "uncorrel\n",
            "underli\n",
            "understand\n",
            "understood\n",
            "undirect\n",
            "undirect graph\n",
            "unfortun\n",
            "uni\n",
            "unifi\n",
            "uniform\n",
            "uniform distribut\n",
            "uniformli\n",
            "uniformli distribut\n",
            "uniformli random\n",
            "unimod\n",
            "union\n",
            "uniqu\n",
            "unit\n",
            "unit activ\n",
            "unit varianc\n",
            "uniti\n",
            "univ\n",
            "univari\n",
            "univers\n",
            "univers california\n",
            "univers california berkeley\n",
            "univers pittsburgh\n",
            "univers press\n",
            "univers toronto\n",
            "universit\n",
            "unknown\n",
            "unlabel\n",
            "unlabel data\n",
            "unlabel exampl\n",
            "unless\n",
            "unlik\n",
            "unnorm\n",
            "unobserv\n",
            "unseen\n",
            "unstabl\n",
            "unsupervis\n",
            "unsupervis learn\n",
            "updat\n",
            "updat equat\n",
            "updat paramet\n",
            "updat rule\n",
            "upon\n",
            "upper\n",
            "upper bound\n",
            "upper lower\n",
            "upper lower bound\n",
            "usa\n",
            "usag\n",
            "use algorithm\n",
            "use approxim\n",
            "use comput\n",
            "use construct\n",
            "use data\n",
            "use denot\n",
            "use differ\n",
            "use estim\n",
            "use experi\n",
            "use fact\n",
            "use featur\n",
            "use follow\n",
            "use gaussian\n",
            "use gener\n",
            "use inform\n",
            "use kernel\n",
            "use learn\n",
            "use linear\n",
            "use method\n",
            "use model\n",
            "use predict\n",
            "use set\n",
            "use simpl\n",
            "use standard\n",
            "use test\n",
            "use train\n",
            "user\n",
            "usp\n",
            "usual\n",
            "util\n",
            "utter\n",
            "valid\n",
            "valid set\n",
            "valu\n",
            "valu decomposit\n",
            "valu function\n",
            "valu iter\n",
            "valu paramet\n",
            "valu use\n",
            "valuabl\n",
            "van\n",
            "vanish\n",
            "vapnik\n",
            "var\n",
            "vari\n",
            "vari number\n",
            "variabl\n",
            "variabl given\n",
            "variabl model\n",
            "variabl select\n",
            "variabl take\n",
            "varianc\n",
            "varianc estim\n",
            "variant\n",
            "variat\n",
            "variat approxim\n",
            "variat bayesian\n",
            "variat distribut\n",
            "variat infer\n",
            "variat method\n",
            "variat paramet\n",
            "varieti\n",
            "variou\n",
            "vec\n",
            "vector\n",
            "vector machin\n",
            "vector machin svm\n",
            "vector quantiz\n",
            "vector space\n",
            "vehicl\n",
            "veloc\n",
            "verb\n",
            "verif\n",
            "verifi\n",
            "verlag\n",
            "versa\n",
            "version\n",
            "version paper\n",
            "versu\n",
            "vertex\n",
            "vertic\n",
            "via\n",
            "vice\n",
            "vice versa\n",
            "video\n",
            "video sequenc\n",
            "view\n",
            "viewpoint\n",
            "violat\n",
            "virtual\n",
            "visibl\n",
            "visibl unit\n",
            "vision\n",
            "vision pattern\n",
            "vision pattern recognit\n",
            "vision research\n",
            "visit\n",
            "visual\n",
            "visual cortex\n",
            "visual system\n",
            "viterbi\n",
            "vlsi\n",
            "voc\n",
            "vocabulari\n",
            "voic\n",
            "vol\n",
            "voltag\n",
            "volum\n",
            "volum page\n",
            "von\n",
            "vote\n",
            "vowel\n",
            "voxel\n",
            "wainwright\n",
            "wait\n",
            "walk\n",
            "wall\n",
            "wang\n",
            "want\n",
            "warmuth\n",
            "warp\n",
            "washington\n",
            "water\n",
            "wave\n",
            "waveform\n",
            "wavelet\n",
            "way\n",
            "weak\n",
            "weak classifi\n",
            "weak learner\n",
            "weaker\n",
            "weakli\n",
            "web\n",
            "web page\n",
            "weight\n",
            "weight chang\n",
            "weight decay\n",
            "weight function\n",
            "weight matrix\n",
            "weight sum\n",
            "weight updat\n",
            "weight vector\n",
            "weiss\n",
            "well\n",
            "well defin\n",
            "well known\n",
            "well suit\n",
            "weston\n",
            "whenev\n",
            "wherea\n",
            "whether\n",
            "white\n",
            "white nois\n",
            "whiten\n",
            "whole\n",
            "whose\n",
            "wide\n",
            "wide rang\n",
            "wide use\n",
            "wide varieti\n",
            "width\n",
            "wiener\n",
            "wij\n",
            "wiley\n",
            "wiley son\n",
            "william\n",
            "williamson\n",
            "willski\n",
            "wilson\n",
            "win\n",
            "window\n",
            "winner\n",
            "winner take\n",
            "wire\n",
            "wise\n",
            "wish\n",
            "wishart\n",
            "wit\n",
            "within\n",
            "without\n",
            "without loss\n",
            "without loss gener\n",
            "wolf\n",
            "word\n",
            "work\n",
            "work support\n",
            "work support part\n",
            "work well\n",
            "worker\n",
            "workshop\n",
            "world\n",
            "world data\n",
            "world problem\n",
            "wors\n",
            "worst\n",
            "worst case\n",
            "worth\n",
            "would\n",
            "would expect\n",
            "would like\n",
            "would like thank\n",
            "would requir\n",
            "wright\n",
            "write\n",
            "written\n",
            "wrong\n",
            "wta\n",
            "www\n",
            "xij\n",
            "xing\n",
            "xit\n",
            "xor\n",
            "xti\n",
            "yahoo\n",
            "yang\n",
            "ye\n",
            "year\n",
            "yet\n",
            "yield\n",
            "yij\n",
            "york\n",
            "york usa\n",
            "young\n",
            "yuill\n",
            "zemel\n",
            "zero\n",
            "zero entri\n",
            "zero mean\n",
            "zero mean gaussian\n",
            "zhang\n",
            "zhou\n",
            "zhu\n",
            "zij\n",
            "zisserman\n"
          ]
        }
      ],
      "source": [
        "feature_names = count_vector.get_feature_names_out()\n",
        "for feature in feature_names:\n",
        "  print(feature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "kamZop4R3_p-"
      },
      "outputs": [],
      "source": [
        "def get_keywords(idx,docs,topN=10):\n",
        "  docs_word_count = tfidf_transformer.transform(count_vector.transform([docs[idx]]))\n",
        "  #build sparse matrix\n",
        "  docs_word_count = docs_word_count.tocoo()\n",
        "  tuples = zip(docs_word_count.col,docs_word_count.data)\n",
        "  sorted_items = sorted(tuples,key = lambda x: (x[1],x[0]),reverse=True)\n",
        "\n",
        "  #getting the top N keywords\n",
        "  sorted_items = sorted_items[:topN]\n",
        "  score_vals = []\n",
        "  feature_vals = []\n",
        "  for idx, score in sorted_items:\n",
        "    score_vals.append(round(score,3))\n",
        "    feature_vals.append(feature_names[idx])\n",
        "\n",
        "  #we get the final result here\n",
        "  results = {}\n",
        "  for idx in range(len(feature_vals)):\n",
        "    results[feature_vals[idx]] = score_vals[idx]\n",
        "  return results\n",
        "\n",
        "def print_keywords(idx,keywords,idf):\n",
        "  print(\"\\n--------Title-------\")\n",
        "  print(df['title'][idx])\n",
        "  print(\"\\n--------Abstract-------\")\n",
        "  print(df['abstract'][idx])\n",
        "  print(\"\\n--------Keywords-------\")\n",
        "  for k in keywords:\n",
        "    print(k,keywords[k])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "yGJrEnrrqKe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "9a9ee847-01f2-4bdb-8825-21eae3871510"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Multilinear Dynamical Systems\\nfor Tensor Time Series\\nMark Rogers\\nLei Li\\nStuart Russell\\nEECS Department, University of California, Berkeley\\nmarkrogersjr@berkeley.edu, {leili,russell}@cs.berkeley.edu\\n\\nAbstract\\nData in the sciences frequently occur as sequences of multidimensional arrays\\ncalled tensors. How can hidden, evolving trends in such data be extracted while\\npreserving the tensor structure? The model that is traditionally used is the linear\\ndynamical system (LDS) with Gaussian noise, which treats the latent state and\\nobservation at each time slice as a vector. We present the multilinear dynamical\\nsystem (MLDS) for modeling tensor time series and an expectation?maximization\\n(EM) algorithm to estimate the parameters. The MLDS models each tensor observation in the time series as the multilinear projection of the corresponding member\\nof a sequence of latent tensors. The latent tensors are again evolving with respect\\nto a multilinear projection. Compared to the LDS with an equal number of parameters, the MLDS achieves higher prediction accuracy and marginal likelihood for\\nboth artificial and real datasets.\\n\\n1\\n\\nIntroduction\\n\\nA tenet of mathematical modeling is to faithfully match the structural properties of the data; yet, on\\noccasion, the available tools are inadequate to perform the task. This scenario is especially common\\nwhen the data are tensors, i.e., multidimensional arrays: vector and matrix models are fitted to them\\nwithout justification. This is, perhaps, due to the lack of an agreed-upon tensor model. There are\\nmany examples that seem to require such a model: The spatiotemporal grid of atmospheric data in\\nclimate modeling is a time series of n ? m ? l tensors, where n, m and l are the numbers of latitude,\\nlongitude, and elevation grid points. If k measurements?e.g., temperature, humidity, and wind\\nspeed for k=3?are made, then a time series of n ? m ? l ? k tensors is constructed. The daily high,\\nlow, opening, closing, adjusted closing, and volume of the stock prices of n multiple companies\\ncomprise a time series of 6 ? n tensors. A grayscale video sequence is a two-dimensional tensor\\ntime series because each frame is a two-dimensional array of pixels.\\nSeveral queries can be made when one is presented with a tensor time series. As with any time\\nseries, a forecast of future data may be requested. For climate data, successful prediction may\\nspell out whether the overall ocean temperatures will increase. Prediction of stock prices may not\\nonly inform investors but also help to stabilize the economy and prevent market collapse. The\\nrelationships between particular subsets of tensor elements could be of significance. How does the\\ntemperature of the ocean at 8? N, 165? E affect the temperature at 5? S, 125? W? For stock price data,\\none may investigate how the stock prices of electric car companies affect those of oil companies.\\nFor a video sequence, one might expect adjacent pixels to be more correlated than those far away\\nfrom each other. Another way to describe the relationships among tensor elements is in terms of\\ntheir covariances. Equipped with a tabulation of the covariances, one may read off how a given\\ntensor element affects others. Later in this paper, we will define a tensor time series model and a\\ncovariance tensor that permits the modeling of general noise relationships among tensor elements.\\nMore formally, a tensor X ? RI1 ?????IM is a multidimensional array with elements that can each be\\nindexed by a vector of positive integers. That is, every element Xi1 ???iM ? R is uniquely addressed\\n1\\n\\n\\x0cby a vector (i1 , ? ? ? , iM ) such that 1 ? im ? Im for all m. Each of the M dimensions of X is called\\na mode and represents a particular component of the data. The simplest tensors are vectors and\\nmatrices: vectors are tensors with only a single mode, while matrices are tensors with two modes.\\nWe will consider the tensor time series, which is an ordered, finite collection of tensors that all share\\nthe same dimensionality. In practice, each member of an observed tensor time series reflects the\\nstate of a dynamical system that is measured at discrete epochs.\\nWe propose a novel model for tensor time series: the multilinear dynamical system (MLDS). The\\nMLDS explicitly incorporates the dynamics, noise, and tensor structure of the data by juxtaposing\\nconcepts in probabilistic graphical models and multilinear algebra. Specifically, the MLDS generalizes the states of the linear dynamical system (LDS) to tensors via a probabilistic variant of the\\nTucker decomposition. The LDS tracks latent vector states and observed vector sequences; this\\npermits forecasting, estimation of latent states, and modeling of noise but only for vector objects.\\nMeanwhile, the Tucker decomposition of a single tensor computes a latent ?core? tensor but has\\nno dynamics or noise capabilities. Thus, the MLDS achieves the best of both worlds by uniting\\nthe two models in a common framework. We show that the MLDS, in fact, generalizes LDS and\\nother well-known vector models to tensors of arbitrary dimensionality. In our experiments on both\\nsynthetic and real data, we demonstrate that the MLDS outperforms the LDS with an equal number\\nof parameters.\\n\\n2\\n\\nTensor algebra\\n\\nLet N be the set of all positive integers and R be the set of all real numbers. Given I ? NM ,\\nwhere M ? N, we assemble a tensor-product space RI1 ?????IM , which will sometimes be written\\nas RI = R(I1 ,...,IM ) for shorthand. Then a tensor X ? RI1 ?????IM is an element of a tensor-product\\nspace. A tensor X may be referenced by either a full vector (i1 , . . . , iM ) or a by subvector, using\\nthe ? symbol to indicate coordinates that are not fixed. For example, let X ? RI1 ?I2 ?I3 . Then\\nXi1 i2 i3 is a scalar, X?i2 i3 ? RI1 is the vector obtained by setting the second and third coordinates\\nto i2 and i3 , and X??i3 ? RI1 ?I2 is the matrix obtained by setting the third coordinate to i3 . The\\nconcatenation of two M -dimensional vectors I = (I1 , . . . , IM ) and J = (J1 , . . . , JM ) is given by\\nIJ = (I1 , . . . , IM , J1 , . . . , JM ), a vector with 2M entries.\\nLet X ? RI1 ?????IM , M ? N. The vectorization vec(X) ? RI1 ???IM is obtained by shaping the\\ntensor into a vector. In particular, the elements of vec(X) are given by vec(X)k = Xi1 ???iM , where\\nPM Qm?1\\nk = 1 + m=1 n=1 In (im ? 1). For example, if X ? R2?3?2 is given by\\n\\x12\\n\\x13\\n\\x12\\n\\x13\\n1 3 5\\n7 9 11\\nX??1 =\\n, X??2 =\\n,\\n2 4 6\\n8 10 12\\nT\\n\\nthen vec(X) = (1 2 3 4 5 6 7 8 9 10 11 12) .\\nLet I, J ? NM , M ? N. The matricization mat(A) ? RI1 ???IM ?J1 ???JM of a tensor A ? RIJ\\nPM Qm?1\\nis given by mat(A)kl = Ai1 ???iM j1 ???jM , where k = 1 + m=1 n=1 In (im ? 1) and l = 1 +\\nPM Qm?1\\nm=1\\nn=1 Jn (jm ? 1). The matricization ?flattens? a tensor into a matrix. For example, define\\nA ? R2?2?2?2 by\\n\\x12\\n\\x13\\n\\x12\\n\\x13\\n\\x12\\n\\x13\\n\\x12\\n\\x13\\n1 3\\n5 7\\n9 11\\n13 15\\nA??11 =\\n, A??21 =\\n, A??12 =\\n, A??22 =\\n.\\n2 4\\n6 8\\n10 12\\n14 16\\n?\\n?\\n1 5 9 13\\n? 2 6 10 14 ?\\nThen we have mat(A) = ?\\n.\\n3 7 11 15 ?\\n4 8 12 16\\nThe vec and mat operators put tensors in bijective correspondence with vectors and matrices. To\\ndefine the inverse of each of these operators, a reference must be made to the dimensionality of the\\noriginal tensor. In other words, given X ? RI and A ? RIJ , where I, J ? NM , M ? N, we have\\n?1\\nX = vec?1\\nI (vec(X)) and A = matIJ (mat(A)).\\nLet I, J ? NM , M ? N. The factorization of a tensor A ? RIJ is given by Ai1 ???iM j1 ???jM =\\nQM\\n(m)\\n(m)\\n? RIm ?Jm for all m. The factorization exponentially reduces the\\nm=1 Aim jm , where A\\n2\\n\\n\\x0cQM\\nPM\\nnumber of parameters needed to express A from m=1 Im Jm to m=1 Im Jm . In matrix form, we\\nhave mat(A) = A(M ) ? A(M ?1) ? ? ? ? ? A(1) , where ? is the Kronecker matrix product [1]. Note\\nthat tensors in RIJ are not factorizable in general [2].\\nIJ\\nJ\\nM\\nThe product A ~ X of\\nPtwo tensors A ? R and X ? R , where I, J ? N , M ? N, is given\\nby (A ~ X)i1 ???iM = j1 ???jM Ai1 ???iM j1 ???jM Xj1 ???jM . The tensor A is called a multilinear operator\\nwhen it appears in a tensor product as above. The product is only defined if the dimensionalities of\\nthe last M modes of A match the dimensionalities of X. Note that this tensor product generalizes\\nthe standard matrix-vector product in the case M = 1.\\n\\nWe shall primarily work with tensors in their vector and matrix representations. Hence, we appeal\\nto the following\\nLemma 1. Let I, J ? NM , M ? N, A ? RIJ , X ? RJ . Then\\nvec(A ~ X) = mat(A) vec(X) .\\nFurthermore, if A is factorizable with matrices A , then\\nh\\ni\\nvec(A ~ X) = A(M ) ? ? ? ? ? A(1) vec(X) .\\n\\n(1)\\n\\n(m)\\n\\n(2)\\n\\nPM Qm?1\\nPM Qm?1\\nProof. Let k = 1 + m=1 n=1 In (im ? 1) and l = 1 + m=1 n=1 Jn (jm ? 1) for some\\n(j1 , . . . , jM ). We have\\nX\\nX\\nvec(A ~ X)k =\\nAi1 ???iM j1 ???jM Xj1 ???jM =\\nmat(A)kl vec(X)l = (mat(A) vec(X))k ,\\nj1 ???jM\\n\\nl\\n\\nwhich holds for all 1 ? im ? Im , 1 ? m ? M . Thus, (1) holds. To prove (2), we express mat(A)\\nas the Kronecker product of M matrices A(1) , . . . , A(M ) .\\nThe Tucker decomposition can be expressed using the product ~ defined above.\\nThe\\nTucker decomposition models a given tensor X ? RI1 ?????IM as the result of a multilinear transformation that is applied to a latent core tensor Z ? RJ1 ?????JM : X = A ~ Z.\\nThe multilinear operator A is a factorizable tensor such that\\nA(3) mat(A) = A(M ) ?A(M ?1) ?? ? ??A(1) ,. where A(1) , . . . , A(M )\\nare projection matrices (Figure 1). The canonical decomposi=\\ntion/parallel factors (CP) decomposition is a special case of the\\n(1)\\n(2)\\nX\\nZ A Tucker decomposition in which Z is ?superdiagonal?, i.e., J1 =\\nA\\nFigure 1: The Tucker decomposi- ? ? ? = JM = R and only the Zj1 ???jM such that j1 = ? ? ? = jM\\ntion of a third-order tensor X.\\ncan be nonzero. The CP decomposition expresses X as a sum\\nPR\\n(m)\\n(M )\\n(1)\\nX = r=1 ur ? ? ? ? ? ur , where ur ? RIm for all m and r and ? denotes the tensor outer\\nproduct [3].\\nTo illustrate, consider the case M = 2 and let X = A~Z, where X ? Rn?m and Z ? Rp?q . Then\\nX = AZB T , where mat(A) = B ? A. If p ? n and q ? m, then Z is a dimensionality-reduced\\nversion of X: the matrix A increases the number of rows of Z from p to n via left-multiplication,\\nwhile the matrix B increases the number of columns of Z from q to m via right-multiplication. To\\nreconstruct X, we simply apply A ~ Z. See Figure 1 for an illustration of the case M = 3.\\n\\n3\\n\\nRandom tensors\\n\\nGiven I ? NM , M ? N, we define a random tensor X ? RI1 ?????IM as follows. Suppose vec(X)\\nis normally distributed with expectation vec(U) and positive-definite covariance mat(S), where U ?\\nRI and S ? RII . Then we say that X has the normal distribution with expectation U ? RI and\\ncovariance S ? RII and write X ? N (U, S). The definition of the normal distribution on tensors\\ncan thus be restated more succinctly as\\nX ? N (U, S) ?? vec(X) ? N (vec U, mat S) .\\n\\n(3)\\n\\nOur formulation extends the normal distribution defined in [4], which is restricted to symmetric,\\nsecond-order tensors.\\n3\\n\\n\\x0cWe will make use of an important special case of the normal distribution defined on tensors: the\\nmultilinear Gaussian distribution. Let I, J ? NM , M ? N, and suppose X ? RI and Z ? RJ are\\njointly distributed as\\nZ ? N (U, G) and X | Z ? N (C ~ Z, S) ,\\n(4)\\nwhere C ? RIJ . The marginal distribution of X and the posterior distribution of Z given X are given\\nby the following result.\\nLemma 2. Let I, J ? NM , M ? N, and suppose the joint distribution of random tensors X ? RI\\nand Z ? RJ is given by (4). Then the marginal distribution of X is\\n\\x01\\nX ? N C ~ U, C ~ G ~ CT + S ,\\n(5)\\nwhere CT ? RJI and CTj1 ???jM i1 ???iM = Ci1 ???iM j1 ???jM . The conditional distribution of Z given X is\\n\\x10\\n\\x11\\n? G\\n? ,\\nZ | X ? N U,\\n(6)\\n? = vec?1 (? + W (vec(X) ? mat(C) ?)), G\\n? = mat?1 (? ? W mat(C) ?), ? = vec(U),\\nwhere U\\nJ\\nJJ\\nh\\ni?1\\nT\\nT\\n? = mat(G), ? = mat(S), and W = ?mat(C) mat(C) ?mat(C) + ?\\n.\\nProof. Lemma 1, (3), and (4) imply that the vectorizations of Z and X given Z follow vec(Z) ?\\nN (?, ?) and vec(X) | vec(Z) ? N (mat(C) vec(Z) , ?). By the properties of the multivariate\\nnormal distribution, the marginal distribution of vec(X) and the conditional distribution of vec(Z)\\nT\\ngiven vec(X) are vec(X) ? N (mat(C) vec(U), mat(C) ?mat(C) + ?) and vec(Z) | vec(X) ?\\nT\\n? mat(G)).\\n?\\nN (vec(U),\\nThe associativity of ~ implies that mat(C ~ G ~ CT ) = mat(C) ?mat(C) .\\nFinally, we apply Lemma 1 once more to obtain (5) and (6).\\n\\n4\\n\\nMultilinear dynamical system\\n\\nThe aim is to develop a model of a tensor time series X1 , . . . , XN that takes into account tensor\\nstructure. In defining the MLDS, we build upon the results of previous sections by treating each\\nXn as a random tensor and relating the model components with multilinear transformations. When\\nthe MLDS components are vectorized and matricized, an LDS with factorized transition and projection matrices is revealed. Hence, the strategy for fitting the MLDS is to vectorize each Xn , run the\\nexpectation-maximization (EM) algorithm of the LDS for all components but the matricized transition and projection tensors?which are learned via an alternative gradient method?and finally convert\\nall model components back to tensor form.\\n4.1\\n\\nDefinition\\n\\nLet I, J ? NM , M ? N. The MLDS model consists of a sequence Z1 , . . . , ZN of latent tensors,\\nwhere Zn ? RJ1 ?????JM for all n. Each latent tensor Zn emits an observation Xn ? RI1 ?????IM .\\nThe system is initialized by a latent tensor Z1 distributed as\\nZ1 ? N (U0 , Q0 ) .\\n\\n(7)\\n\\nGiven Zn , 1 ? n ? N ? 1, we generate Zn+1 according to the conditional distribution\\nZn+1 | Zn ? N (A ~ Zn , Q) ,\\n\\n(8)\\n\\nwhere Q is the conditional covariance shared by all Zn , 2 ? n ? N , and A is the transition tensor\\nwhich describes the dynamics of the evolving sequence Z1 , . . . , ZN . The transition tensor A is\\nfactorized into M matrices A(m) , each of which acts on a mode of Zn . In matrix form, we have\\nmat(A) = A(M ) ? ? ? ? ? A(1) . To each Zn there corresponds an observation Xn generated by\\nXn | Zn ? N (C ~ Zn , R) ,\\n\\n4\\n\\n(9)\\n\\n\\x0cZ1 Z2\\nX1 X2\\n\\nZn\\n...\\n\\nZn+1\\n\\n...\\n\\nXn+1\\n\\nZN\\nXN\\n\\nwhere R is the covariance shared by all Xn and C is the projection tensor which multilinearly transforms the latent tensor Zn .\\nLike the transition tensor A, the projection tensor C is factorizable, i.e., mat(C) = C (M ) ? ? ? ? ? C (1) . See Figure 2 for an\\nillustration of the MLDS.\\n\\nBy vectorizing each Xn and Zn , the MLDS becomes an LDS\\nwith factorized transition and projection matrices mat(A) and\\nmat(C). For the LDS, the transition and projection operators are\\nnot factorizable in general [2]. The factorizations of A and C\\nfor the MLDS not only allow for a generalized dimensionality\\nreduction of tensors but exponentially reduce the number of parameters of the transition and projecQM\\nQM\\n2\\ntion operators from |ALDS | + |C LDS | = m=1 Jm\\n+ m=1 Im Jm down to |AMLDS | + |CMLDS | =\\nPM\\nP\\nM\\n2\\nm=1 Jm +\\nm=1 Im Jm .\\n\\nXn\\nFigure 2: Schematic of the MLDS\\nwith three modes.\\n\\n4.2\\n\\nParameter estimation\\n\\nGiven a sequence of observations X1 , . . . , XN , we wish to fit the MLDS model by estimating\\n? = (U0 , Q0 , Q, A, R, C). Because the MLDS model contains latent variables Zn , we cannot directly maximize the likelihood of the data with respect to ?. The EM algorithm circumvents this\\ndifficulty by iteratively updating (E(Z1 ), . . . , E(ZN )) and ? in an alternating manner until the expected, complete likelihood of the data converges [5]. The normal distribution of tensors (3) will\\nfacilitate matrix and vector computations rather than compel us to work directly with tensors. In\\nparticular, we can express the complete likelihood of the MLDS model as\\nL (? | Z1 , X1 , . . . , ZN , XN ) = L (vec ? | vec Z1 , vec X1 , . . . , vec ZN , vec XN ) ,\\n(10)\\nwhere vec ? = (vec U0 , mat Q0 , mat Q, mat A, mat R, mat C). It follows that the vectorized MLDS\\nis an LDS that inherits the Kalman filter updates for the E-step and the M-step for all parameters\\nexcept mat A and mat C. See [6] for the EM algorithm of the LDS.\\nBecause A and C are factorizable, an alternative to the standard LDS updates is required. We\\nlocally maximize the expected, complete log-likelihood\\nby computing the gradient with respect to\\nP\\nthe vector v = [vec C (1)T ? ? ? vec C (M )T ]T ? R m Im Jm , which is obtained by concatenating the\\nvectorizations of the projection matrices C (m) . The expected, complete log-likelihood (with terms\\nconstant with respect to C deleted) can be written as\\nn\\nh\\nio\\nT\\nl(v) =tr ?mat(C) ?mat(C) ? 2?T ,\\n(11)\\n? ?1 , ? = PN E(vec Zn vec ZT ), and ? = PN vec (Xn )E(vec Zn )T . Now\\nwhere ? = mat(R)\\nn\\nn=1\\nn=1\\n(m)\\nlet k correspond to some Cij and let ?ij ? RIm ?Jm be the indicator matrix that is one at the\\nP\\n(i, j)th entry and zero elsewhere. The gradient ?l(v) ? R m Im Jm is given elementwise by\\nn\\nh\\nio\\nT\\n?l(v)k = 2tr ??vk mat(C) ?mat(C) ? ?T ,\\n(12)\\nwhere ?vk mat(C) = C (M ) ? ? ? ? ? ?ij ? ? ? ? ? C (1) [1]. If m = M , then we can exploit\\nQ the sparsity\\nof ?vk mat(C) by computing the trace of the product of two submatrices each with n6=M In rows\\nQ\\nand n6=M Jn columns:\\n\\x1ah\\n\\x1b\\niT\\n?l(v)k = 2tr C (M ?1) ? ? ? ? ? C (1) ?ij ,\\n(13)\\nQ\\nwhere ?ij is the submatrix of ? [mat(C) ? ? ?] with row indices (1, . . . , n6=M In ) shifted by\\nQ\\nQ\\nQ\\nn6=M In (i ? 1) and column indices (1, . . . ,\\nn6=M Jn ) shifted by\\nn6=M Jn (j ? 1). If m 6= M ,\\nthen the ordering of the modes can be replaced by 1, . . . , m ? 1, m + 1, . . . , M, m and the rows and\\ncolumns of ? [mat(C) ? ? ?] can be permuted accordingly. In other words, the original tensors Xn\\nare ?rotated? so that the mth mode becomes the M th mode.\\nThe M-step for A can be computed in a manner analogous to that of C by replacing I by J, replacing\\n?1\\nmat(C) by\\nmat(A), and substituting\\nv = [vec(Ah(1) )T ? ? ? vec(A(M ) )T i]T , ? = mat(Q) , ? =\\ni\\nPN ?1 h\\nPN ?1\\nT\\nT\\ninto (11).\\nn=1 E vec(Zn ) vec(Zn ) , and ? =\\nn=1 E vec(Zn+1 ) vec(Zn )\\n5\\n\\n\\x0c4.3\\n\\nSpecial cases of the MLDS and their relationships to existing models\\n\\nIt is clear that the MLDS is exactly an LDS in the case M = 1. Certain constraints on the MLDS\\nalso lead to generalizations of factor analysis, probabilistic principal components analysis (PPCA),\\nthe CP decomposition, and the matrix factorization model of collaborative filtering (MF). Let p =\\nQM\\nQM\\nm=1 Jm . If A = 0, U0 = 0, and Q0 = Q, then the Xn of the MLDS become\\nm=1 Im and q =\\nindependent and identically distributed draws from the multilinear Gaussian distribution. Setting\\nmat(Q) = Idq and mat(R) to a diagonal matrix results in a model that reduces to factor analysis\\nin the case M = 1. A further constraint on R, mat(R) = ?2 Idp , yields a multilinear extension of\\nPPCA. Removing the constraints on R and forcing mat(Zn ) = Idq for all n results in a probabilistic\\nCP decomposition in which the tensor elements have general covariances. Finally, the constraint\\nM = 2 yields a probabilistic MF.\\n\\n5\\n\\nExperimental results\\n\\nTo determine how well the MLDS could model tensor time series, the fits of the MLDS were compared to those of the LDS for both synthetic and real data. To avoid unnecessary complexity and\\nhighlight the difference between the two models?namely, how the transition and projection operators are defined?the noises in the models are isotropic. The MLDS parameters are initialized so\\nthat U0 is drawn from the standard normal distribution, the matricizations of the covariance tensors are identity matrices, and the columns of each A(m) and C (m) are the first Jm eigenvectors of\\nsingular-value-decomposed matrices with entries drawn from the standard normal distribution. The\\nLDS parameters are initialized in the same way by setting M = 1.\\nThe prediction error and convergence in likelihood were measured for each dataset. For the\\nsynthetic dataset, model complexity was also measured. The prediction error \\x0fM\\nn of a given\\nth\\nmodel \\x0c\\x0cM for the \\x0cn\\nmember\\nof\\na\\ntensor\\ntime\\nseries\\nX\\n,\\n.\\n.\\n.\\n,\\nX\\nis\\nthe\\nrelative\\nEuclidean\\ndis1\\nN\\n\\x0c\\n\\x0c\\x0c / ||Xn ||, where ||?|| = ||vec(?)|| . Each estimate XM\\n=\\nis given by XM\\ntance \\x0c\\x0cXn ? XM\\nn\\nn\\nn\\x01\\n2\\n\\x02 M \\x03\\n\\x01\\n\\x02 M \\x03\\x01\\x01\\nM\\nM n\\nis\\nthe\\nestimate\\nof\\nthe\\nlatent\\nstate\\n,\\nwhere\\nE\\nZ\\nvec?1\\nmat\\nC\\nmat\\nA\\nvec\\nE\\nZ\\nNtrain\\nNtrain\\nI\\nof the last member of the training sequence. The convergence in likelihood of each model is determined by monitoring the marginal likelihood as the number of EM iterations increases. Each model\\nis allowed to run until the difference between consecutive log-likelihood values is less than 0.1%\\nof the latter value. Lastly, the model complexity is determined by observing how the likelihood\\nand prediction error of each model vary as the model size |?M | increases. Aside from the model\\ncomplexity experiment, the LDS latent dimensionality is always set to the smallest value such that\\nthe number of parameters of the LDS is greater than or equal to that of the MLDS.\\n5.1\\n\\nResults for synthetic data\\n\\nThe synthetic dataset is an MLDS with dimensions I = (7, 11), J = (3, 5), and N = 1100 and\\nparameters initialized as described in the first paragraph of this section. For the prediction error and\\nconvergence analyses, the latent dimensionality of the MLDS for fitting was set to J = (3, 5) as\\nwell. Each model was trained on the first 1000 elements and tested on the last 100 elements of the\\nsequence. The results are shown in Figure 3. According to Figure 3(a), the prediction error of MLDS\\nmatches that of the true model and is below that of the LDS. Furthermore, the MLDS converges to\\nthe likelihood of the true model, which is greater than that of the LDS (see Figure 3(b)). As for\\nmodel complexity, the model size needed for the MLDS to match the likelihood and prediction error\\nof the true model is much smaller than that of the LDS (see Figure 3(c) and (d)).\\n5.2\\n\\nResults for real data\\n\\nWe consider the following datasets:\\nSST: A 5-by-6 grid of sea-surface temperatures from 5? N, 180? W to 5? S, 110? W recorded hourly\\nfrom 7:00PM on 4/26/94 to 3:00AM on 7/19/94, yielding 2000 epochs [7].\\nTesla: Opening, closing, high, low, and volume of the stock prices of 12 car and oil companies\\n(e.g., Tesla Motors Inc.), from 6/29/10 to 5/10/13 (724 epochs).\\nNASDAQ-100: Opening, closing, adjusted-closing, high, low, and volume for 20 randomlychosen NASDAQ-100 companies, from 1/1/05 to 12/31/09 (1259 epochs).\\n6\\n\\n\\x0c5\\n\\n6\\n\\n1020\\n\\n1060\\nTime slice\\n\\n?3\\n0\\n\\n5\\n10\\n15\\n20\\nNumber of EM iterations\\n\\n100\\nLDS\\nMLDS\\ntrue\\n\\n50\\n\\n0\\n0\\n\\n1000\\n2000\\nNumber of parameters\\n\\n1000\\n2000\\nNumber of parameters\\n\\n(c)\\n(d)\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\x0cXn ? XM\\n\\x0c\\x0c / ||Xn || is shown as a function of\\nFigure 3: Results for synthetic data. Prediction error \\x0fM\\nn =\\nn\\nthe time slice n in (a), convergence of marginal log-likelihood is shown in (b), marginal log-likelihood as a\\nP train +Ntest M\\nfunction of model size is shown in (c), and cumulative prediction error N\\nn=Ntrain+1 \\x0fn as a function of model\\nsize is shown in (d) for LDS, MLDS, and the true model.\\n\\n20\\n\\n0.8\\n\\n0.8\\n\\n0.6\\n0.4\\n0.2\\n\\n1850 1900 1950 2000\\nTime slice\\n\\n(b) Tesla\\n\\n4\\n\\n?8\\n\\n5\\n10 15 20 25\\nNumber of EM iterations\\n\\n(e) SST\\n\\nLog?likelihood\\n\\nLog?likelihood\\n\\nLDS\\nMLDS\\n\\n0\\n\\n1230\\n1250\\nTime slice\\n\\nLDS\\nMLDS\\n\\n?6\\n\\n?3\\n\\n10\\n20\\n30\\n40\\nNumber of EM iterations\\n\\n(f) Tesla\\n\\n5\\n\\n?0.5 x 10\\n\\n?1\\n?2\\n\\n1050 1100 1150\\nTime slice\\n\\n(d) Video\\n\\n0 x 10\\n\\n?4\\n\\n?8\\n\\n1210\\n\\n50\\n\\n5\\n\\n?2 x 10\\n\\n?6\\n\\nLDS\\nMLDS\\n\\n100\\n\\n(c) NASDAQ-100\\n\\n4\\n\\n?4 x 10\\n\\nLDS\\nMLDS\\n\\n0.6\\n\\n0.2\\n\\n705 710 715 720\\nTime slice\\n\\n(a) SST\\n\\n150\\n\\n0.4\\n\\nLDS\\nMLDS\\n\\nLog?likelihood\\n\\n0\\n\\n1\\n\\nError\\n\\n40\\n\\n1\\n\\nError\\n\\nLDS\\nMLDS\\n\\nError\\n\\nError\\n\\n60\\n\\n(b)\\n\\nLDS\\nMLDS\\n20\\n40\\n60\\nNumber of EM iterations\\n\\n(g) NASDAQ-100\\n\\nLog?likelihood\\n\\n(a)\\n\\nLDS\\nMLDS\\ntrue\\n\\n?2\\n\\nCumulative error\\n\\nLDS\\nMLDS\\ntrue\\n\\n?2\\n\\n?4\\n\\n1100\\n\\nLog?likelihood\\n\\nLog?likelihood\\n\\nError\\n\\nLDS\\nMLDS\\ntrue\\n\\n0.5\\n\\n0\\n\\n?1 x 10\\n\\n0 x 10\\n\\n1\\n\\n?1\\nLDS\\nMLDS\\n?1.5\\n\\n20\\n40\\n60\\nNumber of EM iterations\\n\\n(h) Video\\n\\nFigure 4: Results for LDS and MLDS applied to real data. The first row corresponds to prediction error \\x0fM\\nn\\nas a function of the time slice n, while the second corresponds to convergence in log-likelihood. Sea-surface\\ntemperature, Tesla, NASDAQ-100, and Video results are given by the respective columns.\\n\\nVideo: 1171 grayscale frames of ocean surf during low tide. This dataset was chosen because it\\nrecords a quasiperiodic natural scene.\\nFor each dataset, MLDS achieved higher prediction accuracy and likelihood than LDS. For the SST\\ndataset, each model was trained on the first 1800 epochs; occlusions were filled in using linear\\ninterpolation and refined with an extra step during the learning that replaced the estimates of the\\noccluded values by the conditional expectation given all the training data. For results when the\\nMLDS dimensionality is set to (3, 3), see Figure 4(a) and (e). For the Tesla dataset, each time series\\n((X1 )ij , . . . , (XN )ij ) were normalized prior to learning by subtracting by the mean and dividing by\\nthe standard deviation. Each model was trained on the first 700 epochs. See Figure 4(b) and (f) for\\nresults when the MLDS dimensionality is set to (5, 2). For the NASDAQ-100 dataset, each model\\nwas trained on the first 1200 epochs. The data were normalized in the same way as with the Tesla\\ndataset. For results when the MLDS dimensionality is set to (10, 3), see Figure 4(c) and (g). For the\\nVideo dataset, a 100-by-100 patch was selected, spatially downsampled to a 10-by-10 patch for each\\nframe, and normalized as before. Each model was trained on the first 1000 frames. See Figure 4(d)\\nand (h) for results when the MLDS dimensionality is set to (5, 5).\\n\\n6\\n\\nRelated work\\n\\nSeveral existing models can be fitted to tensor time series. If each tensor is ?vectorized?, i.e., reexpressed as a vector so that each element is indexed by a single positive integer, then an LDS can be\\napplied [8, 6]. An obvious limitation of the LDS for modeling tensor time series is that the tensor\\nstructure is not preserved. Thus, it is less clear how the latent vector space of the LDS relates to the\\nvarious tensor modes. Further, one cannot postulate a latent dimension for each mode as with the\\nMLDS. The net result, as we have shown, is that the LDS requires more parameters than the MLDS\\nto model a given system (assuming it does have tensor structure).\\n7\\n\\n\\x0cDynamic tensor analysis (DTA) and Bayesian probabilistic tensor factorization (BPTF) are explicit\\nmodels of tensor time series [9, 10]. For DTA, a latent, low-dimensional ?core? tensor and a set of\\nprojection matrices are learned by processing each member Xn ? RI1 ?????IQM of the sequence as\\n(m)\\nfollows. For each mode m, the tensor is flattened into a matrix Xn ? R( k6=m Ik )?Im and then\\n(m)T (m)\\nmultiplied by its transpose. The result Xn Xn is added to a matrix S (m) that has accumulated\\nthe flattenings of the previous n ? 1 tensors. The eigenvalue decomposition U ?U T of\\x01 the updated\\nS (m) is then computed and the mth projection matrix is given by the first rank S (m) columns of\\nU . After this procedure is carried out for each mode, the core tensor is updated via the multilinear\\ntransformation given by the Tucker decomposition. Like the LDS, DTA is a sequential model. An\\nadvantage of DTA over the LDS is that the tensor structure of the data is preserved. A disadvantage\\nis that there is no straightforward way to predict future terms of the tensor time series. Another\\ndisadvantage is that there is no mechanism that allows for arbitrary noise relationships among the\\ntensor elements. In other words, the noise in the system is assumed to be isotropic.\\nOther families of isotropic models have been devised that ?tensorize? the time dimension by concatenating the tensors in the time series to yield a single new tensor with an additional temporal\\nmode. These models include multilinear principal components analysis [11], the memory-efficient\\nTucker algorithm [12], and Bayesian tensor analysis [13]. For fitting to data, such models rely on\\nalternating optimization methods, such as alternating least squares, which are applied to each mode.\\nBPTF allows for prediction and more general noise modeling than DTA. BPTF is a multilinear extension of collaborative filtering models [14, 15, 16] that concatenates the members of the tensor time series (Xn ), Xn ? RI1 ?????IM , to yield a higher-order tensor R ?\\nRI1 ?????IM ?K , where K is the sequence length. Each element of R is independently distributed\\n(M )\\n(1)\\nas Ri1 ???iM k ? N (hui1 , . . . , uiM , Tk i, ??1 ), where h?, . . . , ?i denotes the tensor inner product\\nand ? is a global precision parameter. Bayesian methods are then used to compute the canonicalPR\\n(M )\\n(1)\\ndecomposition/parallel-factors (CP) decomposition of R: R = r=1 ur ?? ? ??ur ?Tr , where ? is\\n(m)\\nthe tensor outer product. Each ur is independently drawn from a normal distribution with expectation ?m and precision matrix ?m , while each Tr is recursively drawn from a normal distribution\\nwith expectation Tr?1 and precision matrix ?T . The parameters, in turn, have conjugate prior distributions whose posterior distributions are sampled via Markov-chain Monte Carlo for model fitting.\\nThough BPTF supports prediction and general noise models, the latent tensor structure is limited.\\nOther models with anisotropic noise include probabilistic tensor factorization (PTF) [17], tensor\\nprobabilistic independent component analysis (TPICA) [18], and generalized coupled tensor factorization (GCTF) [19]. As with BPTF, PTF and TPICA utilize the CP decomposition of tensors. PTF\\nis fit to tensor data by minimizing a heuristic loss function that is expressed as a sum of tensor inner\\nproducts. TPICA iteratively flattens the tensor of data, executes a matrix model called probabilistic\\nICA (PICA) as a subroutine, and decouples the factor matrices of the CP decomposition that are embedded in the ?mixing matrix? of PICA. GCTF relates a collection of tensors by a hidden layer of\\ndisconnected tensors via tensor inner products, drawing analogies to probabilistic graphical models.\\n\\n7\\n\\nConclusion\\n\\nWe have proposed a novel probabilistic model of tensor time series called the multilinear dynamical\\nsystem (MLDS), based on a tensor normal distribution. By putting tensors and multilinear operators\\nin bijective correspondence with vectors and matrices in a way that preserves tensor structure, the\\nMLDS is formulated so that it becomes an LDS when its components are vectorized and matricized.\\nIn matrix form, the transition and projection tensors can each be written as the Kronecker product of\\nM smaller matrices and thus yield an exponential reduction in model complexity compared to the\\nunfactorized transition and projection matrices of the LDS. As noted in Section 4.3, the MLDS generalizes the LDS, factor analysis, PPCA, the CP decomposition, and low-rank matrix factorization.\\nThe results of multiple experiments that assess prediction accuracy, convergence in likelihood, and\\nmodel complexity suggest that the MLDS achieves a better fit than the LDS on both synthetic and\\nreal datasets, given that the LDS has the same number of parameters as the MLDS.\\n\\n8\\n\\n\\x0cReferences\\n[1] Jan R. Magnus and Heinz Neudecker. Matrix Differential Calculus with Applications in Statistics and\\nEconometrics. Wiley, revised edition, 1999.\\n[2] Vin De Silva and Lek-Heng Lim. Tensor rank and the ill-posedness of the best low-rank approximation\\nproblem. SIAM Journal on Matrix Analysis and Applications, 30(3):1084?1127, 2008.\\n[3] Tamara G. Kolda. Tensor decompositions and applications. SIAM Review, 51(3):455?500, 2009.\\n[4] Peter J. Basser and Sinisa Pajevic. A normal distribution for tensor-valued random variables: applications\\nto diffusion tensor MRI. IEEE Transactions on Medical Imaging, 22(7):785?794, 2003.\\n[5] Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin. Maximum likelihood from incomplete data via\\nthe EM algorithm. Journal of the Royal Statistical Society. Series B (Methodological), 39(1):1?38, 1977.\\n[6] Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer, 1st edition, 2006.\\n[7] NOAA/Pacific Marine Environmental Laboratory. Tropical Atmosphere Ocean Project. http://www.\\npmel.noaa.gov/tao/data_deliv/deliv.html. Accessed: May 23, 2013.\\n[8] Zoubin Ghahramani and Geoffrey E. Hinton. Parameter estimation for linear dynamical systems. Technical Report CRG-TR-96-2, University of Toronto Department of Computer Science, 1996.\\n[9] Jimeng Sun, Dacheng Tao, and Christos Faloutsos. Beyond streams and graphs: dynamic tensor analysis.\\nIn Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data\\nMining, pages 374?383. ACM, 2006.\\n[10] Liang Xiong, Xi Chen, Tzu-Kuo Huang, Jeff Schneider, and Jaime G. Carbonell. Temporal collaborative\\nfiltering with Bayesian probabilistic tensor factorization. In Proceedings of SIAM Data Mining, 2010.\\n[11] Haipin Lu, Konstantinos N. Plataniotis, and Anastasios N. Venetsanopoulos. MPCA: Multilinear principal\\ncomponents analysis of tensor objects. IEEE Transactions on Neural Networks, 19(1), 2008.\\n[12] Tamara Kolda and Jimeng Sun. Scalable tensor decompositions for multi-aspect data mining. In Eighth\\nIEEE International Conference on Data Mining. IEEE, 2008.\\n[13] Dacheng Tao, Mingli Song, Xuelong Li, Jialie Shen, Jimeng Sun, Xindong Wu, Christos Faloutsos, and\\nStephen J. Maybank. Bayesian tensor approach for 3-D face modeling. IEEE Transactions on Circuits\\nand Systems for Video Technology, 18(10):1397?1410, 2008.\\n[14] Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender systems. Computer, 42(8):30?37, 2009.\\n[15] Ruslan Salakhutdinov and Andriy Mnih. Probabilistic matrix factorization. In Advances in Neural Information Processing Systems, volume 20, pages 1257?1264, 2008.\\n[16] Ruslan Salakhutdinov and Andriy Mnih. Bayesian probabilistic matrix factorization using Markov chain\\nMonte Carlo. In Proceedings of the 25th International Conference on Machine Learning. ACM, 2008.\\n[17] Cyril Goutte and Massih-Reza Amini. Probabilistic tensor factorization and model selection. In Tensors,\\nKernels, and Machine Learning (TKLM 2010), pages 1?4, 2010.\\n[18] Christian F. Beckmann and Stephen M. Smith. Tensorial extensions of independent component analysis\\nfor multisubject FMRI analysis. Neuroimage, 25(1):294?311, 2005.\\n[19] Y. Kenan Yilmaz, A. Taylan Cemgil, and Umut Simsekli. Generalized coupled tensor factorization. In\\nNeural Information Processing Systems. MIT Press, 2011.\\n\\n9\\n\\n\\x0c'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 177
        }
      ],
      "source": [
        "#in order to get the indices for all the rows with non-empty paper text info\n",
        "df['paper_text'][4550]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "L0y1ZbEdS_9J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6923010b-9efb-4766-8a8e-9e551c3676d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------Title-------\n",
            "Multilinear Dynamical Systems for Tensor Time Series\n",
            "\n",
            "--------Abstract-------\n",
            "Many scientific data occur as sequences of multidimensional arrays called tensors.  How can hidden, evolving trends in such data be extracted while preserving the tensor structure?  The model that is traditionally used is the linear dynamical system (LDS), which treats the observation at each time slice as a vector.  In this paper, we propose the multilinear dynamical system (MLDS) for modeling tensor time series and an expectation-maximization (EM) algorithm to estimate the parameters.  The MLDS models each time slice of the tensor time series as the multilinear projection of a corresponding member of a sequence of latent, low-dimensional tensors.  Compared to the LDS with an equal number of parameters, the MLDS achieves higher prediction accuracy and marginal likelihood for both simulated and real datasets.\n",
            "\n",
            "--------Keywords-------\n",
            "tensor 0.819\n",
            "vec 0.393\n",
            "time seri 0.121\n",
            "model 0.112\n",
            "seri 0.077\n",
            "latent 0.074\n",
            "matric 0.073\n",
            "likelihood 0.07\n",
            "decomposit 0.067\n",
            "matrix 0.063\n"
          ]
        }
      ],
      "source": [
        "idx = 4550\n",
        "keywords = get_keywords(idx,docs)\n",
        "print_keywords(idx,keywords,docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "XJUxqfoMQnIy"
      },
      "outputs": [],
      "source": [
        "#creating the pickle files\n",
        "import pickle\n",
        "pickle.dump(count_vector,open('count_vector.pkl','wb'))\n",
        "pickle.dump(feature_names,open('feature_names.pkl','wb'))\n",
        "pickle.dump(tfidf_transformer,open('tfidf_transformer.pkl','wb'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}